{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Layers import *\n",
    "from Solver import *\n",
    "from Classifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n",
      "320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>k_value</th>\n",
       "      <th>l_value</th>\n",
       "      <th>m_value</th>\n",
       "      <th>percentage_free_sulphur</th>\n",
       "      <th>n_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>8.100</td>\n",
       "      <td>4.0500</td>\n",
       "      <td>0.636</td>\n",
       "      <td>30.909091</td>\n",
       "      <td>0.6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>8.680</td>\n",
       "      <td>4.3400</td>\n",
       "      <td>0.778</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>0.8290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>8.560</td>\n",
       "      <td>4.2800</td>\n",
       "      <td>0.742</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.7440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "      <td>11.480</td>\n",
       "      <td>5.7400</td>\n",
       "      <td>0.655</td>\n",
       "      <td>35.294118</td>\n",
       "      <td>0.7195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>8.100</td>\n",
       "      <td>4.0500</td>\n",
       "      <td>0.636</td>\n",
       "      <td>30.909091</td>\n",
       "      <td>0.6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0</td>\n",
       "      <td>6.800</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>0.670</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.6610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.450</td>\n",
       "      <td>3.2250</td>\n",
       "      <td>0.822</td>\n",
       "      <td>13.076923</td>\n",
       "      <td>0.7110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.810</td>\n",
       "      <td>3.4050</td>\n",
       "      <td>0.826</td>\n",
       "      <td>13.793103</td>\n",
       "      <td>0.7540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.545</td>\n",
       "      <td>3.2725</td>\n",
       "      <td>0.785</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.6615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.310</td>\n",
       "      <td>3.1550</td>\n",
       "      <td>0.727</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>1.2075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  k_value  l_value  m_value  percentage_free_sulphur  \\\n",
       "0         9.4        0    8.100   4.0500    0.636                30.909091   \n",
       "1         9.8        0    8.680   4.3400    0.778                26.800000   \n",
       "2         9.8        0    8.560   4.2800    0.742                36.000000   \n",
       "3         9.8        1   11.480   5.7400    0.655                35.294118   \n",
       "4         9.4        0    8.100   4.0500    0.636                30.909091   \n",
       "...       ...      ...      ...      ...      ...                      ...   \n",
       "1594     10.5        0    6.800   3.4000    0.670                13.750000   \n",
       "1595     11.2        1    6.450   3.2250    0.822                13.076923   \n",
       "1596     11.0        1    6.810   3.4050    0.826                13.793103   \n",
       "1597     10.2        0    6.545   3.2725    0.785                13.750000   \n",
       "1598     11.0        1    6.310   3.1550    0.727                23.333333   \n",
       "\n",
       "      n_value  \n",
       "0      0.6080  \n",
       "1      0.8290  \n",
       "2      0.7440  \n",
       "3      0.7195  \n",
       "4      0.6080  \n",
       "...       ...  \n",
       "1594   0.6610  \n",
       "1595   0.7110  \n",
       "1596   0.7540  \n",
       "1597   0.6615  \n",
       "1598   1.2075  \n",
       "\n",
       "[1599 rows x 17 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first step would be to import the dataset\n",
    "X_full = pd.read_csv('./red_wine_dataset.csv')\n",
    "percentage = 0.8\n",
    "X_train = X_full.sample(frac=percentage, random_state=0)\n",
    "y_train = X_train.pop('quality')\n",
    "X_test = X_full.drop(X_train.index)\n",
    "y_test = X_test.pop('quality')\n",
    "print(len(X_train.index))\n",
    "print(len(X_test.index))\n",
    "X_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 16)\n",
      "(320, 16)\n",
      "float64\n",
      "(1279,)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train.to_numpy()\n",
    "x_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_train.dtype)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #classic way to plot the data\n",
    "# for i in range(x_train.shape[0]):\n",
    "#     if(y_train[i] == 1):\n",
    "#         plt.scatter(x_train[i,0], x_train[i,1], color='red')\n",
    "#     else:\n",
    "#         plt.scatter(x_train[i,0], x_train[i,1], color='blue')\n",
    "# plt.xlabel('fixed acidity')\n",
    "# plt.ylabel('volatile acidity')\n",
    "# plt.title('Red Wine Dataset')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#quicker way to plot the data\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=50, cmap='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.999927832269524e-16\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABoVUlEQVR4nO2ddZgcVdaH3xqfnpkISSB4cA8W3F0Xd19Y2F0WW2DR3Y9lcRZYHBZ3XdzdNcEleAIBAgmxcev+fX+c6rRVVXfPTDKZmfs+z31muuTeW9XVp+4994gnCYfD4XD0XUp6uwMOh8Ph6B5OkDscDkcfxwlyh8Ph6OM4Qe5wOBx9HCfIHQ6Ho49T1huNDh8+XKNGjeqNph0Oh6PP8t577/0maUT29l4R5KNGjWLcuHG90bTD4XD0WTzP+z5ou1OtOBwORx/HCXKHw+Ho4zhB7nA4HH0cJ8gdDoejj9MjgtzzvCGe5/3P87wvPM8b73neej1Rr6MHmQGcA6wKrA5cBNT3ao8cDkcP0VNWK5cBT0vaw/O8CiDWQ/U6eoKfgTGYMG/1t30JXAWMA4b1Ur8cDkeP0O0Rued5g4CNgRsBJLVLmtndeh09yHHAFFJCHKAF+Ak4rTc65HA4epKeUK0sCUwFbvY87wPP827wPK+mB+p19AQdwCNAPGTfnXO3Ow6Ho+fpCUFeBqwBXCNpdaAJOCX7IM/zjvA8b5zneeOmTp3aA806CqINSETsbwZcSHqHo0/TE4L8R+BHSe/4n/+HCfYMJF0naYykMSNG5HiYOuYUNcDCEftXBry51BeHwzFH6LYgl/QLMMnzvOX8TVsAn3e3XkcP4QFnE7z8HMMsWRwOR5+mp6xWjgbu9C1WvgN+30P1OnqCA4DpwOlkvrr/A/yuV3rkcDh6kB4R5JI+xAzcHPMqxwCHA+9iwnwdoKJXe+RwOHqIXol+6OglqoFNersTDoejp3Eu+g6Hw9HHcYLc4XA4+jhOkDscDkcfxwlyh8Ph6OM4Qe5wOBx9HCfIHQ6Ho4/jBLnD4XD0cZwgdzgcjj6OE+QOh8PRx3GC3OFwOPo4TpA7HA5HH8cJcofD4ejjOEHucDgcfRwnyB0Oh6OP4wS5w+Fw9HGcIHc4HI4+jhPkDofD0cdxgtzhcDj6OE6QOxwORx/HCXKHw+Ho4zhB7nA4HH0cJ8gdDoejj+MEucPhcPRxnCB3OByOPo4T5I5w4sCVwJJADJgPqAXqgB2A93uvaw6HI0VZb3fAMY8iYB/gSaDZ39aStv8p4GX/78ZztWcOhyMLNyJ3BPM2JqSbQ/bL33fEXOuRw+EIwQlyRzD3Ei7E0/nBLw6Ho9dwgtwRTCs26s5HCdA+h/vicDgicYLcEcwO2MJmPmqxxVCHw9FrOEHuCGZ7YCmgMuKYGPBv3FPkcPQy7ifoCKYUeAXYC6jyS4lfqoCFgeuBA3urgw6HI4kzP3SEMxi4DbgWmA6MABqBNmBBwOu9rjkcjhROkDvyE/MLRKtaHA5Hr+BUKw6Hw9HHcYLc4XA4+jhOkDv6H3Hgf8CWwJrAScCPvdojh2OO4nTkjv5FHNgFeAlo8rd9ii3YvoQJdoejn9FjI3LP80o9z/vA87zHe6pOh6No7iZTiIN5njYAe1OYt6rD0cfoSdXKscD4HqzP4Sieq8kU4un8go3OHY5+Ro8Ics/zFsGcum/oifocji4zLWJfWZ79DkcfpadG5JdiS0qJsAM8zzvC87xxnueNmzp1ag8163BksQHmlRpEG7DKXOyLwzGX6LYg9zxvR2CKpPeijpN0naQxksaMGDGiu806HMGcRLDTUjWwPzBs7nbH4Zgb9MSIfANgJ8/zJgL3AJt7nndHD9Tr6A5fA48BHzCwFviWBx4F5sdS0g3CBPtemP7c4eiHdNv8UNKpwKkAnudtCpwo6YDu1uvoItOA3YF3gQqgE1gEeARYrhf7NTfZAvgZeAuYhZkcjuzVHjkccxRnR96fELAVZpnRQSrH5lfAhsAECosx3h8oxa7Z4RgA9Khnp6SXJe3Yk3U6iuAtTGh3ZG0XJtTvmus9cjgccwHnot+fGIupUoJowuKLOxyOfocT5P2J+YDykH2lwAJzsS8Oh2Ou4QR5f2Jnwi35K4BD52JfHA7HXMMJ8v7EIOAmLAlE0inGA2qAE4CVe6lfDodjjuIEeRSfAfth+SmXAy4Cmnu1R/nZG3gHOAhYHYsE+CRwVi/2yeFwzFGc+WEYr2CZ5FtJqSv+D7gTeBPzFJxXWRkbmTscjgGBG5EHIWxE20ymzrkF+BLLHt8X6MQE+prAEtjs4uNe7ZHD4ZgDOEEexKeER8lroW8I8jgWj/Jo4H1gInAvsB7wVO91y+Fw9DxOkAfRRHgEveT+eZ3/AW+QqdNP+J8PJNze3OFw9DmcIA9iNOGCrgzYei72patcT/gLpx3T8zscjn6BE+RBxIC/+X+zqcJCpc7rzIzY52GpzxwOR7/ACfIwzgBOxoJM1WFWKitg+SCX7MV+FcrWBMflBhuRrzUX++JwOOYozvwwDA8zNzwJs1SpY94W4DOwRHsPYi+dXTFvzras42LAvli8bofD0S9wgjwfVcCqvd2JPHwPrI2pS5Kha9/FTA47/f3l2Ej898B/eqGPDodjjuEEeX/gMMxcMp62rQn4Fjgd2BOYjqmGBs/13jkcjjmM05H3dWYAr5EpxJO0ANcCywLr4oS4w9FP6X+CvBW4DdgJ2A3TGfcFm+lpwIXAdsDBmA14IcwiPHRtcn9BdGI3azfs5t2G3UyHwzGv079UK9OxkefPpGyonwNWAV7E9N3zIh8Dm2ALky3YQusDwCHAFf7nMBYm2nlptUI60AJsjrm0NvrbXgLOwdIOzVdIJQ6Ho5foXyPyYzBX9HRHmEYsk/x5vdGhAhBmYTKT1EKlsGu4BXg2z/nlhNu8x4B/FtKJc4EPSQlx/P8nAMcWUoHD4ehF+o8gb8Pc0rPzVYJpCK6Zu90BbIC7E2YOWIOZ/X2bdcz7wJSQ85uAywto5zTgj9iMY5Bf6oCrsYF2Xq4hWI3SAdyPmbuk8TqwMWanPgj4E+HX0K94EQtWUwkMxV5yM3q1Rw4H9CfVSn2e/TO728DPwI3AF8CKmKnIyPDDPwA2wmKbyN92Hxaw6j1gKX/bL0SrRn4uoGslwCWYhcobmEDfmCJUSfkU6Y3MVq88CexBavbQjkVYfBz4CBhWaJt9jfsxXVcyeE07tpL8OPZlD+qdbjkc9KcR+XwEqxeSLN2dyh/xKzgXS0V/NiaJnww/5WhsRK20bQnM1vuUtG0rk+u0k6SM/B6Ywkb1r/rH74R5dUYJ8Scxu/MaYBHgt6ibUwMM8duKwxHtKSGepAP4Dbg0T1/7LJ3An8nNKtIOTAb+O9d75HCk038EeSnmhdktXXEQ0zCdSAsp9UMr9qPek8ChfjOWpSeIBPBY2ufFMfVHkDt9BXB8RNdeARbDFkp3wiYIJxBsipjkKr/bY/1+/gQcfwa0hN24k7HH5EkYvynMbA84DnsZ3RHRbp/mfXLUS7NpwSx8HI7eo/8IcjBB/ntsNJqMkVKFqRz26mqld+fZf2/upihBGrT/bmADUrr0Omymfh+wfEgd47EMRj9imo9Z2PvlWuDUkHMasIXR7IHl7fvABadBvMpvvBa7cYcCJ2JSf0/onAVeWHZn+oaZZ5foINp0qN9euKOP0H905GCvpSsxwf0CdnXbYOtSXeZ7cnUJSZoxSZpFHeZF+UnIaZtlfR6E9fczTGbOh/U7LOgVmBVOkEqmGdOX74aZYqbzPOHf+Jmnw1d/hruexQTTlqTWAM4EWmDFz6E8RGiVY/lB+yVrkpkqKp1KYPe52BeHI5f+NSJPsiBwALAP3RTiACthI9QgajGJHcClBOf1jAHnRzR1CKYmyRbin2KJlUdiwbseJXzkHwc2xdZj03X0YdqBJL/Nh920A8hcyH3TKiqLw0UnQiwr0Lknm0n0hfC+XaIKy16drX4qwd7ax8z1Hjkc6fRPQd6j7EW462QlNvQNYHNM2K7on14OjMFG3msU2YVXgXUw88pfMfPufFY6bZjW55a0bRsTLsxrCL0UE1Y+h94M1/8BFvseKtqgvB02a4e3gUXz9KlPcxzmnbUQ9r2XY9Omd3GhJB29jScp/1E9zJgxYzRu3Li53m7X+QD70bZiErISG24/h6UTysMM7JWZE+tkGhaK8G5s6r4bppNeMHWIsBH4xC52fXlMn55kf8zwJpshwCRCJh9nY16eabbmAqYNh6qVoPblwvvzK3Y/RjHvetpGIux7Sy5oOBxzD8/z3pM0Jme7E+SF0oHZ7X2HmSJuR/eWGH7BdK/TSCm7K7DR71gsBi3wOWYqGJa2zSNTfZJNLalsQML07zNDjv0EM4fMoQlYH/iG1EppBSbM3sB0Qnn4DoshM9Y/VZg/zZlE29E7HI7ZhAny/rXYOUcpB3buwfpOw9wh0xcP27Hh6tGYowm2zhol6IZggjlMmC+e9v+zRDtGHYppCnKowXQnN2FOUS3ADsBfsWAveZiGqYamYxOP5HvrP35/rsxfhcPhCMfpyHuNewk2W0tgEteXdisRbjBRhnlZHkawmqKGTOejfGEKPoraWQ38BbOpHg9cREFCHOA6bFCffR3N2HthamHVOByOYJwg7zXC3DnBhtf+/ips8B6kjq3C/HUuI+WpWYJNHqowAb9/2vG9Nf96lHALzgoswKLD4egyTrXSa6wOhK0TLEaGpcgpmHrlHEzGd2ILoLeSitnyMqb9eJ6UMU225/0xWHjcqC7NCfKtCQaZaf4CPIEtTWzNvJ0v1eHoZZwgz6YNs+q4GTPS2AWL7tfjIbnPxwzGs90sY/DaDXCJZ5ES18Dc7k/CLOC+whYwR2Wd5mGB+daLaDJSoApWivJe7AaHYS+ZsAXbjTO7wT+Ai7GXV8LftgdmSukWRh2OHJxqJZ1mzDjjaCx92ljM8m55um7+F8oWwO3AAqTiCcwHp78Jm20GD2NWJLdiwvxOTA2xMrlCvFC+AGrC3Mk9mDCHLJj2wEb72SPvGBZvKt356W5sEbQVE/zJEDcPYhYuDocjByfI07kIM/dLHzm2YNYWf5gTDe6Gxal9A3gFPvkVzls112OzEzPdy+cElI9FO8ELUVaXdsJSk7vZQAjlmMrnbExFMh8WAeAZzJE0nbPInaTgb7sMF9bE4QjACfJ0/ktwfoU4NkKfOScaLcGcilaHM8rCzQjjdD85xobPw5BZBJrBVLbBUZd1s4EIKrFIjt9i5ojPARsGHPddRB3t2Et1XqMZi3R8F/BDL/fFMSBxOvJ0oka8ZViEwSFdrPsnTBC/TUqV0IbptP+EeX6HBdlK8nYX205SMh2e2BM2fQLaK6Cp1lzsS+Nw5j/gy8Gmn+7AQt3uS0odEscWH2/Boi3uDBxExppsjzAMC/EdRlj+hgS2TpCu5pdf5uRw5Q7s+yv12+rAJlq3EJ0U2+HoQZxnZzobYWnMghiC+e905cf5KhZytpNcq8MSUkJoENHJeo7D9MdgUQO+xVQVqxMdZXU2XwKrQUMZ3LE/vL4hfL00fL6SCXUPkF9RDRY36x2/XztgZoLJtJ4xLOTAO3QtxsosLJ66hwX4Sr4QzsPUK9kaoAosaFh26O8XMBPMD7CX7W7YwvAVmL69DYt3cx7wuy70M4o3MIuabFVQNbbAe0UPt+cY8IR5diKpWwX7Gb+EeYl8Bhyb75w111xT8yQvSoopt8cxSed2sc52SfMF1Fls8SRN9MvKkmokDfL/riTpu0I7tJWkSqm9TFpjnFTeEt5muaS1/TaC9pdK2qLI+5GQdLakar//g/z/L/T3t0naUlJtWju1klaUNCOrrof9c7P7VCKpLGt7TNLNRfY1H9so/N5VS6rv4fYcAx5gnAKeuG6PyD3PWxBYUNL7nufVYRkpd5H0edg58+yIHGyqfCSpEW47Zn99PgWOerN4GguguP09ps54ZGdIhNjQzTcN1n0bZgyFt9cF+TqBEmyx9SpsBP4TmWruEkw18x0FzBgagH3h/sFwwE3QHhX0vAAqsZDswws8/ibMKijA6pLrgf2wa3ue1Ih6N0yVk35tbZhj6bQi+jqErs+qghiJBQELYhA2E1u1h9pyOAgfkXdbeyhpsqT3/f8bsJF5gb7b8yAHYC7jDwL3YPraC+iaEAf7oZ9wDtxwGNQPChfiw6fAozvCQ7vAS5vCrwvARq/bnbwGy/zzGLbgmr1WmcBUFY8U0qE64HG45obuC3EwlUehwlRYyr0wq5T/8/8vwVQWN2MLiHuQKXwfxiLHFiPEwe7T2CLPiWJExL72PPsdjh6kR5eBPM8bhWlsczJWep53hOd54zzPGzd16rwcXOMnqPwDbDEEtquBITsAH3a9ujWmw8lnQ20zrP8GVAaY/y37JYxfEdZ9Byo6obIDRvwGr24GP06EI7AXyVhSkQyzaaA4ITUhyJ2yCzRgGYl+K+DYNmw2Eca35E+T9w4WdqCrppg9uSR0NME5YkuwwJYL9WBbDkcEPSbIPc+rxRzAj5OU8zOTdJ2kMZLGjBgxrw5VfsbeQ7diQ9xm4CksoeabXatylecg7g8n/3xtQKo0wSM7wdDpUJotZTqBXYHVgOVh11NhUJB9JKbiWKCIfoVZf3SF67AFxSDTwJnYot8hmHqqDMv7ueVzcPWf4KojYfMXAJlQzPdE/ovwuC2FkLtM1HUOxdL2pcdwj2GWN7f3YDsORz6CFOfFFmzi+wxwfCHHzzuLnS9J2kRSnaRFZO+ZUgV3e5UutnGnFK9L1fP6+tLwKVLdLKl2lrTOm1JTVXCbCaSWyszPrRXS4t8Fd3G4bGFwhKSjJf0a0qWnJS0ccpndKYenN9Iivf2JNKhTiiVsf6Wk0oR02llSfa0Ux0p9rfTiZtLRrflv5/A8ffD8UpK1PSbpmvzVF01c0uOSdpUt0l4qaeYcaMfhUPhiZ08IcQ8zCru00HPmDUF+u4JNVMJKpaRfutDOZP/ctLo6SqVntpJu21/6bvHodj8cLe30kDRzkGYL80kL5wqq7FIuaUHlCvNrCrjsWkkVRdyadGGphKSLpLah0rDfgo+rbpK+G5W5sblaajk9/+1cOk8ftpb0pqS9/HtQKmkpSffkr9rhmNcJE+Q9oVrZADgQ2NzzvA/9sn0P1DsHacNMU4JW3cIowbw9imUk8EcylKllcdj6OTjwTlji+/BTO0rh/TXgmW1h+ydtmwcs/BOsPzN6AbYD01ufk7atEQvAFXXZdZieOl+i5iBawVZl/w+e2gjaQ8xD4qVw3eGZ26pboO1quJzo2/xngqMlgoXyfQZzsroXU8E0YImN9i70IhyOvkdPWK28LpvMjpa0ml+e7InOzTleo3gzlBF03RjnP5i5xjDMzKNAa5H2Srj4BGirgg9XhXfXsu0e8Ft5/oW7DsyEL8lz5I8e2EDXddCDBJwBNMOkRaEjxHG4vRK+zY6xCwyaAafHYVvCFz3/gi1jpEdyrAKGkquXLiVc6Dsc/YgBGmslZMEwlBhmmtFVG8QS4G+YEfMvmMlFwK1vrYTmKqivhaYYHHIzfOYn0Wyphjc2sP8FTCkwc3FbyP9zgmPrme36udyXUBYS4aqqGVb9MHf75JHQWGqWKWGmlJVY7PVrsfC3a2KenV8Ay3a96w5HX2aAxlpZn3DdQSU2+v4VE7YLYsGxd+tek53A4yXw0VCzLjl8CJRmmXmcfB7Ey2D6MHh0J3ObT6ISmD7UhPhnK8KsAgJzl2BRBpNsTNdUJoW0swpwaoLZQ+ktXoBh06GpBpTV19IE/OHGzG2NMbj8GDj8OlhwMnyxMhavPUA9U47Z+x/Qw9fhcPRRBqggnw+bo19DpsK4FLPL+wgbfXdgQr0LI/HfMAvGxYBJmBCtx1QXMeAPM3PPmbAUPLZTeJ2VbTB1OGzwRn57a4DqOJw5BXsZYXbNB2GxzYtZHsimDHsX/oLp1P+MrZJUDIVf1oMRr5op5QtbwGYvwcwh0FoFFQkoqYSHr4Lh9dDizyraKuD+3WH/O2DJiVDdDC11mF3fK8Ay3eisw9H/GaCqFYALsWSYgzGFawWwCZZGfj5M6To/RQvxbzChvQjmnj0MW4SbTMqZpxloC9CTLzGB0EzLZZ1wxwGwwFSoH5LaXoVNIsr9rpbLBP7oj+CF9WHlhTAJ/jUwHq69BO67FAaFeRYVgIDNsbABh2IBwSr8ru9xHdQPNuG81HcWlOsvV5gzVB3wsQebnwzTPobTLoCNX4H5ZsARN8LoT2HRH2DsWlDT4N+0rULuyc9YzIKLCE+Z53AMEIJMWeZ0mTfMD5O0yyJOTet+Vb9KGqb8poFIuukQC1yVvnHsmlKsMfh4Lx68fVFJUyRN8C9l4prS5AUyD0ogJTwpUSWpUopXSk9uJ1VGBMwiLpHwS8gxnqQqv1wg6UdZsKgFf5IuPFF6Zgtp/slSeWtmPRf492tIUL1+mzs9KL2/msx+8HeSlpO0rqRbJf3Tb7RaZmMYk/kAfNzFL87h6Bswp+zIu1LmLUHeg/yfTL4UchcWmCz9tGCmw097qXTQTVJZe64QL2kPrmfb9A48HN5gIuDzrg9ktZWQSuLSn6+Q/nVqtBDPLjFJ96tw+/P78x0TlzZ/zu5JxpuxUtFvylpJDxb1tTkcfYUwQT6AVStzgMco3CDm15Ew+mO46ET4dkmYuDg8sBv8bw/ozPpa5EEixCY7I355hF94tobIw9q66MRkIzDsN4vOeNXR8J8TAk6KoBm4EjMNLITj8+xf7ku48wAoj5OpWmkjVP0EmNXMbphpi8MxMHCJJXqSyJAsIlAwlpHKQ7n6OBjzvoWxfWIHaAmKyITFKtniBVjyO5g4ClbcHM4rh6qDKDrIR8KzDEF4UFtvL5hYC1S0QEeBJo5JRgqe8SyFWz4VfA2ZuVGz2e4JeGLHrlt8shxmk+hw9B/Cwtj2Y6uVFmwtN5/zjTCJUkW3b8ehWKDEIIsQT1DTCI11UNUBlFsQqYnArTMthO3qH9ix8VIoScCBt8PDu6b104MVPodntoHBs2wBtLMMGurgyIfhxpOA24sTftOGMfuEtcdazPRYC8w/BX5alKIqmx6Hj8ssW8/WROTfFKzxIby2WnD9NQ1w6M3dEOJgi7tpFPo4OBx9kH6oWnkV8xKpw4Z9m2LmhNkIuBGzDxyKmbrtjzntdJEDgOWB6vRZTgJiTXD7AXD1kXDMZXDmvyxk67HYKP6PV5t6pbEWaprNoqS2yaxUVvDzc8Sa4ahL7fx31zIhHmuxYxecDBduB8stCc9tmesa31Zh7v7ZNFfDlX+BrZ+Bg26FURPshQNw0+/zX6+XZQPZXmpmiKdg1p1hVLXCdfuZw1O2e2p1E6zxPuzycP72oztnf17C1D3Jx2Fz8udGdTj6GkGK8zld5txi50vKzf2VXAD7NOvYc5UbPapM0mLqVo6uJklnNUuL/iANniFt8Zz02gZZ7ZTasZvLFjJr/UiIVc3ScZekFibby6QbDrVTSjqkymaptl6qmynVNEh37Juqs75W2u8O+7jLA9K7Y6QZg6UvlpX+dKV000G+9Upa+WEhacp80qw6qb5GaoxZQK9knTs8rEzLlbT/K1qkpb+Qtnss+Fs+XzmxwoSk8jbrS3LDhSdIq35g92rUt9K/j7cIj5GPUCFmQRtIzyn4cSiRtLOkd7r+NTscvQEDw2pllZAmPZkJW5J6Bf/CkQn3y7rZj4SkhULqR9Jo6U9SoFVIrFG65LjUhvdXlcraci1Zkse+vXZqw3knBTd31GUmpLN3ZFuy1NdKD+wi3beb9Nt8JlD3u00qS5oPpvW3ssUsb95aO/g6BgVdelz6esngexL3Iu5Xehmh4DdEeimV9I20fJ7DYpLO7MLX63D0EmGCvB+pVmYSvrgl4Fn7twO4/xvY9V7Y5SG4a98sVUQzmZGmCuQH4FQs4NNRHnx2OcHpY2LAOXBDgkAlcHMNnHeqdTkBTFrEDusMsFppqYZzT/PPq4afFgloT3D6uaayySa9+f8cCwv8Cr+/Bf52Mfz7BHhzPdj+KVj33dwT2qrgt+Fw8gW26JpNUAafmg5YYmLADqBE+YOA/bgizFiZ6KAxKwOfwG9LRejofZqxNH4f5znO4ZjXCZLuc7rMmRH5LJlzSFiz1VKjpDUk1XakNtfWSyt9kor3LSRtUFzTT8pGd8nmS605Xf20vyOZLr5O0vV2TpSNdkVrqj/vrGVqh7BjF/nB/mmqkoZNzd1fWy+1ZaeUzyodJdK5J9vHFT+Vpg2VGv0ZS9yTGmqka44I7nNli7ToxMK++ZUSUvui4QckyqXW6pTqJ3203hiT1nlbmhqVWWKQZqvQflNhNu2lko4q4rt2OHoR+v+IfBAwOmRfCbCDRZL9DGhMs05prDM38pMu9DfEKCoaUxOWHLiZVBzt1cbBxX+GkdfBb2dhqeMewrI6/8GO8SKGnyUJqGqxEeoCvwa78yeZf4qNxg+6DaYFpLJvjgWP5tMpS8AxV8Dab8ODu8GQmVDjx7ItkS28HnBnKiZ6xrmdMGmx6PqTTGqE8qEEx9KNgXcLVL4J3k3g7Wzml62V8PS2Fl/mnXVgSlSawDbAvwfDsIXnfMSBHwvrvsMxzxIk3ed0mXM68jeVu4DpyUZqX4Xobf1S3SR1VklaVrZiWSC3KXPt7aK/+ouG/sbWmMxvf7ykSZKekvSBtNbbwSPcsnZp3delA+6QmvzR6dpvSyWducdWNUt73CstPiH6jl9/WPACYmeJ9MsIa6OzRHpkRxt9h1X09NYBI9qO6LaRtNhEaZunpHXfCNhZK5u+XJR5Xz8N+CqRdMQ1wfp+lUraVHpe0lmSnpD0WkgdGd+7pAsL+qYdjl6HgbHYKUljJW0m+2GXycwTvrT1x6j1tLJ2qfF4STOKa+5PaXVs9UyIIPQk1cgW6Qbb/98vIsUagoV5siz+nXT3XtJny0vDf02LwxL3BWiBLvSDZ0ifrWCWKcmN9TXSB6tKIyZLS35tKeXeH22WLmEVfbxy5qbsl0uZLCRKertPbSM1V1m9LZXS5PmlEy+Qrj3cv1fzK9BK6Hn/VqXXP/Jnc9t/Yz2rc/aOGql9pLTCD5nHxyTdIGnjiHtTK1PDOBzdYpIsBtD+smBCU+ZIKwNIkCdJ+CWNURG9WiD38II4Iq2OR3ewZMKF3IY4UmOVWYUQEhBrdklIdErlLWmf8wS0yi4VrdL+t1sfH95R2vPeNEuYuDRohnT+iVkCMq20l1qgLxJmRROoL1emyd8b60otWTOBTk/6dbgF0xo2Vfp8TUlvyyJ+/Vtm/hmT2leX9nwodeo6b9isqbLFTDZ3flD6327S44dKK04Mvxelkhokra3gF/lg2fKKw9Fl7pE99ElrqmrZwO2FHm8pTJAPLBf9m4GjyPW8jAHnAcd0oc6bMLW3gA9Hw6oFeJv8uBDceBj8sBh0lMPDO0PDkDwnhbj4JymJw3ZPwbZPQWs13Ls3jFur8HqqWqCiDW49GLZ9BqqyLEOaYrDpS2ahMmEUjF03t46hwJfAeOD6++HaQ4KtZRpr4ISL4PojYKkJ8NVH4F0LvEHGl9MagzPPgBsOsbWM1qC8bcnnN+Le7AM8SrDHbTWwHeagBXAI9hzkXT2agq19fAOsiHlCzZfnnASWb+8xLO7wXsC60X13zONMBpYiOD9iLRa0vyZgX9cIc9HvxyPyABKS/i57cdb6pUrS8coajXfIwvPdqrzDtZlKvYhvOsSP1pdnJH7PXjZCRlJFc3Ej66AyaKapRuprbUNniemRb98vPPxtWKmtl57dQmort2uZVSeNX1ba8FULR5uvvgqZfnrKqrl26unl3j399hqkVx+UOv2+T1hcemsds2MXUmuVtNMjth5Q1H2JS9s+If3+Bmn05Py68vQyJN/X/riswuTsJSZ7mF6KOKdRNi3wr3O2um1nSZ1RjTnmac5TuF9DrWwRredg4KlWIpgq6U5Jt0v6JXvnWcr1HNwlur67ZbOp0Z+kzPaiSnNV1qZuCvI79stVYQipISb94b9F1pcw3fdab0v73C4Nm1J8/zxJT/05XJB3lEpX/EWzBfnxF5sAX+ctE9iDZ9jfA26TmoZLl51eXPtrjpW+Xsraj2N/313bVwkVWMd6YV/2VIW/FWplepwg/qjgH3xMOQu9jj5Eum41u5TKBH3P4QS5JMt88IHCXfBvUHi398089FtJH0lq9T9/KukwSSfeLzXHbCGvqcoW+LIXQNvLeu5u1jQEC/FkGb9sgXXFpcOvtUXYHxeSrjhSKg2JgV5IWfjHcEHeGJNWf8//mJAuON68REuyLGCqms3a5YKTwhNuZJdR36VmJuklgfTumvaxrN3CFtTNiq4rHvSM/EfhXsG1km4JOKdN0VOCRYIacvQFmq+RPlxHmriYcr/XOkkP9WhzA1yQfysbYlXJbBCrZV4g7VnH+dP5wOJJapPelbSCX0Wd7Pe5hczV+3xJC0sa1CjtdY/0x2ukNd41IfTgztL9u0unnyUdfWkX7lpCWvETP9uOv22NcdKXS0erMBqqpWP/I/3r79KOjwabMXpxae+7Ml8It+9napawvhQySr/kmNSoWP7fhhrLHpR+3Grv+RY8AXVUNUlLfhkidAP6cMVfpNYQx7AE0qgJ0nNbSDMG5VcTBapXjspz0UE+/78qOuNIeVBDjnmZuExNW5OwZ7O6SRr9gVl2CXNiiy+gXBnTPQawIJ8hi8+RrS6plnRA1rF5uv7V+ykVZ7HF67TAWMn/izo/Ia39ljR9kLThKxY8a4lvg0eeQcKrtdz+zqqTvl/UXi7pQvCoy3K9P4tWyYSUmnrprxdJ34ySntzWRthB9ybs/ArfUmfFT6Qh0+3lUtFqfwfNzBXG45eNvhcv/15q9GdII36N+L4SISPya2S67aCTamV6tmzaZW/9sMaWDGqoOGbKXBWcBc7c4W8KmGTF7Zkcv6xl/9rv865ZwkUwgAX5xQqf1lZK+iHt2DyBmw6daWqvLl11N/TgJZ02Yh3xq7nKH/5f6fpDc3N+FlISmAngt6NSQjXohXDihYU5+xRSCnpxhd2ftO1lbfZC2/gladt3pY4O6Q9ZX9snK0Zf+4ejU58vPDFEZZOQtn1VwcxSuGfZMKV0bdmcpGCVTEzSpZKuk3S2LN5DEYufMyXtLRvw1/l/91e3Ang68jBT4ROskg5piW/sN1sj6a2ebXoAC/LNIrpSp8wR1NopNcWvI6QfFkmL+1FrqsxeuWNpJdZo1jGF2qtHlaZqaaNXzEIle9+Ho6XqAvXSBZVuLuhmlx3TvrbvlYqrcstBEYusJdKraSGF45504C02La5oMbVVqf9DbCtX+LT4DZkBeq3szV4nE+IfhBwvv66dlUoYXSmTBtsoZXdc4te1hDIHGCHEJa2u3DXUSlme6h4ZDX4gc7ILnJ4MTF5QrrNaUCmTqVt7kDBB3o9irYQxKGKfR4aNZ/198MhOMPpDWOwHyxu5yCS4+WCYcJdlrultmmvglPOZbXv82zC44TD4z3EwNte8lFmD4JaD4ZK/wmsbZkYYjLXABSfDO2vnnrfqx3D4DZbVaDZidgWrvw/H/QcOvw6GTy2g4x6ZjXfTf+E54M9YjtTFgLMxf4Ajr7KMSdlNCTj+Enj8dxabBiyOzG2HwPtrwLmnwzl/h8e3h++Whngl3F0KFwPPkJUmdH3Mfvg64CwsQcnPwGoRHS4HHgbeBs7B0kM9DryG2SA3+Y00YKE0tyfvPXoBS4SUHQyyDUue8Vr06dFcgqVTWh1Yy///n92psP9QQ3Ta2CTl9KQJeTQFvFd6vHR/RJ6QucAWohB8ROGK7RpJzalDD1PIyDEuLfeFdOHpNnrrlbuWVmKN0nejpCuPNMuOmgYbUdY0mL13fa2pXa4+whYL0/XKq34gTUmLINhZYrFdvh2VmVRCSJ1ID+9kdS7yvYUMqGqUXtzULE9aK+zvzDrpD9cWMPIuwCPV6/QXZAuwf6+StLVssPipbHK1laQVZklP7mNmnh2l0ldLStv7CTCGTpOmDLPrDqv4hS2kuiYbHFfI/i4u6bt8z1qx/EU2bAt7NsdFn35S+CXIk3RGIX34TdL0zE3Tb5B+XiDkHp1bSKX9m05ZZIl8z2elpJ96tmn6j2rlXpkbd6VsirqBpE8ijo9L2k6ZenLP/3xH5qGRYU8T0vcLSyt/nCXMe1hlUEipbJEe3CVYv1vZIi073t8X0LfyNlswFdLUYdLuvqt+SYetvv/jn9LMWnPI+WjlzJNvPNiyEiVVMS2V0jH/sbaqmiSvI8ISJM99GjpNunNfq7O5ysL4/t8ZwVY26aVMmevYg2Tv7okR5yz5jYUPUKVy1kUmLyrVBNzXEtmaZI8uXm0U0ck6SXdFn36Gwt8D5bKQH6E8IwuMU+EfvLr0xbtmdVXRagOEIdOlc07NSvpR2YXr7IckI1SHjQdiks7p+Wb7iSC/Q8ELl3WSvok4r0PSfyWtLAuqsp1Mz5lFZK8T0ttr2Qj0/JOkpb422+clv5aNHgsxyStQ6G/wmnTfHtK4NSzV20qfZO5f9QPzWgwdteZpp7xN+nglu4ZsW3EvLg2ZZgK0ukl6ZqvUzlfXTy2wJpC2fjp3hlLWFiJ8sxYtSztMWFQ12f8H3yS1ZOnqG2O2qNuVp+w1hU/EvHZp96ck3SfpRUlHS1pU0iLSv56UqkLua4nM/yNrANt10lLe5XZS0nGKXLX8TOEm7VWK+Ek8m3tiU7UfRTPr2mMNZtWUsfHXLl5vP+MDSbvLRMqCsjW0+WXv5yfnTJP9QJDHZXcsqMpSSYd0oU7JvHpulPRQHgGYkD4PMG37fmHpP0dL+91uI5jZQiugrgV+ihZwJGxE3BhLjYLaS+3zXndLy30m/fFqS5e22MRufAMJ6Y9Xmiom37Erfmr/tFZId+ydWmR9Z63w88tbTDhXtNox6aP0WEOweirWIF3159zKmqukRb8v/hqXlznVBd7vuHTFn6SOQTI7sjR2zVNvuewxLGAtMj+nRTfWWSXVLy49MUVqCTj92/HSBTdJ+9yfOTurkXRiVLsr5bZ1/WHh32dVszQ5/bfnbBx7i34gyL9SuP0uMlvxYpghe3XG/HoH5fFkjAd7byWwkLCz6mxUc9TlWUIj7fA/XmV21cl9R1wtrfyR6YW9uLTe61ZHUAc6SkyoNVdamxu81o1vICGt9HFhx5a1S/veKa37prTdo9aPtnJzMMr2xEwWLy4tE7eYJTu8YiqX5L5BM8PbWnhS7saGGun3NxZ/jSWK0GMm7EUzfIr05qbK0EWfqOhEU8jGDduHPlhFsHn+C2krl+480FRGT/inNTVJ72wvNVdbGIZZtfayP+QOs2K5TxEqoFkK1McE2fcnS90s6a59/A9De+LCHV2kHwjyCQqfRyKb2xTDVspRiq/3esTDPDN3MTCoNMYCfhQJG0FPWkja5EUbkR5wq/TmOtJmL5hQKemUbjkw3DY826TuwV1CRlCFqG8S0irvF/htJVICu6be16d3Wmzw0BdfQto07VaXJkfk8Ty26YncBbZZtRZzpdinLGItM6PU1ktT0oavXytc75xeKtQDttrbFtbJlkrfl0A2nnltv2Dv1aYq6bN387TZGHyBv3skvAuDZkr37e5/eLi7F+3oBmGCvA+ZHy4OLBSyrwLYr4i6vsVss9ozN59/KlQHxDqtaoYzzoSyeP6qa5rhH2dlbfQszGxtI7y4hZn7HXk1bP0cvLQpxMsgUWrHlXcG15sd6XSXh+GQmyHWBCX+OTWNFGzWt/5bUFuIOaUHCT81XlOtpY1LlMIvC1q/w85ZOO3jUP8xq2iHqtbwpgbVQ2mWXVd5Jzy5ffg5dfXwlyvhqW3gvj1guyfBS1gO5kKIl8KNq6Q+N1NACFssW123rVEPpiD7tLJOqGyzVIJXToW174PKjtzjqluh6V95KqsB1rEUerccDNs/ATs9Akt/nWVqmkZHOWzxJZbAfOf8/XXMfYKk+5wuXV/sfEG5i51lkkaquAWYhxXqnXff7tLQmdKgdhuJVDdJZ5zhj4hLZGqYLRTpBTp9SOamDV81FUH6aLOlUjrz75nHHXZ9YW736eX91aTjLrHFwjv2s+w7eXXfCbNAWWyCwkfw8Yh9BZQb/Nv8pKQj09o9+KbgkLRVzdLfzs/c2BCTTjs7t+/Jfi020XS3DWnPRH2tJc+4s4gwCLt8nXo0jpLvvZuIiDUjC8vT7eiz7ZLWUnQcFkyll7zm/V+OdgabOTh/s9M+kpb5OvM5qam3dYogq6Odunudjp6Cvq9aSfKOpC1lD/9gmRnB5C7UESEw23czq4cXPpLqt/bbGirpGFl84TAXbb98tVTqY0mHmfkFHdcYk8a8m9oUazSP0s6sl0RUUKzskkC68RCzpqn0bcuzM/qUtku37R/huZmQFp1g/y/zZbRACytl/m1Kxiir9AXr3ndZsK/0OmvrLS/pzQdajIqWSumTlaQ97suttzLtJfDqhsHqrsaY9McCY8WUtUvHfJ56NHaTtMpHFlb3oZ2CF2Zr1IORZ5sl/UOhroKNMemgm1Obzngw+nlorcrf5AGSygMEdmXcjHeS440qma26Y56hHwnyniCh0Lxv3y8n7f9zypN6c1nEQ0m2KJYnQ0FjLHPBc5unzGEm6NiOEjMvTN+8zJfS58vZCH5WnQmq2WECCiyz6tLCxGaXhDTiFzMxTBfuo76V7tnTFlRbKyxP6Nkn2+Lr/L908ZtOmAv8d4ub3fZCP9rC2Wn/ku7dwyxx9rtdun836ahLC6tzuB/oauTP4WnphL0ICqmvukn6bJfUo3HlDGnGkNSo9+aDrc+DZtqotTJhxiY9HAzJOE1KpH3fHSWWzDvZ1xpJ746NdmZqyrNW1KboCcCwOXFdjp7CCfIcPpSNgpILqCXSpKWlYY25gbFikl6RzDYtQqXS6Ulj1/AtU/wR6EE3RWemf2arYAG41TP2AmgP+NEmsv5mlynDwx1pSjrMdjtjW6e02vuZYWzTXx5/uqq4pAzJcu4pmWqPuGeJkx/d0RZ+hS3aLf9Z6rqj1Dnlkg69zezgV/nInIbCDm6M5bdCqm6SLjpeqYXySVLbarmj/JZK6dktpQd2laZHeVu2ynwd9pPNFF9T4RL/c5k/RNrzFccWv+/YV1rrY+mvktQhdQ4OvqjOEilxRnQz0xTt+FZaYHeLoU2W1nI/WZCzlzSHXoT9n74vyOMyjciLsuhjPcI0SRfKbMkOlQ77NdxiYUXJ3N4iLu2dMdIT20jHXmLJjJE57wQFpRI2ojzjH8HVXfWn3NCy2SVBltedL8D2vSP8tDDPy5oG6e69g0+atLA032+ZZoT5yuDpJgDzHThzkLTzQxIJmyVURIRAKJH0w96WtLluRrippvz7stTXIbsT0qhvUrGjtZ2kB5U3+mWiWtLVac/P15Kel1lU/SILdpWdym1vFRZwavvw9hNIndWyANiSdJvfl/TrLZUSC8pc7iOIS0qL0JBTli+gq8UwVdIychnueog5KsiBbbG0u98Ap+Q7vmhB/ozMCaNWpnOtktn79nRAtqEK73WlpMl57H7HLxfsNv/OmODpcEONtOCPChyFTon6taWVjlJThcQ9E0zbPx5xeDxa373DY+HtfDdK2v1+PyRtAYugB9+UORoPK7PqpC2fkZb5wh+Z56l75mvSxOWkPe+xLEZhqfVm1abyogaVullpH75QYfGJa2VG2pMkrSObzQ2WPZDzKXgUUCNzOIsiEXJudolJSobXfUzmqezJHs4DJP2cpx2fixSsIYzJImD0JLsp2C4/JunKHm5rADDHBDlmiPUtsCRmB/gREBEUukhB/qHCH7q/R5wnyTwo1pFNWZeUpemKyNgxOKLXVZJ+DAsRkFYe3dH0qXWzLGPInfuYwE6qKuprTGUyZbi0ftJuPU14lXRKR1wTrQfNLls9La36nnTbAeEp0by4tMKnqeQWQWXrp/K31Vpuuu2qZt/KwRfslc2ZVhBHXFvYNfwyQvrvYf6IvwBLk58k6Sb7QlrLpNf8sAHZaqYpw22xN1QmNsiG+LfIwjcU+sjvKmkhFReYfqWQBy5JQoUZvnuS9sg6t1NF6ykSMjVNMoZ5Mo55T8fDmqnwvMRIWrqH2xsAhAnyMEPgYlgb+EbSdwCe592DGZt+3gN1W3jSINPjZuBS4DSgOujES4HT/QPBjH5PB54GniTQWHhb4H6CQ1SOBBbaG7gDs0FvgvZSOOwmeHobs5H+94mw333w80i4/UA49BYo60g1ddPB8MaGMGkxeHHzNDvspJG44O59zbY32546itsPMhv1qlazDT/gTmivMJtv4maL3VEJK4yH75YKrqOmEfa+L39blR1w774wcXF4fUM7b/AsmLgE/LQw/J9vQ//zglAScQ1xgBKYPhRmzGd2zSqNbtsTlO2JTf62g8rnYL23LfTsrCFQ2gmrfQQrjYfhv8Hi38NXywXUE4etnsPCx1YAf8t/3bN5KHxXayV8sBqsPTbr+/vF/gh4EXgCKAP2xCLE4gEbAa/kaVvAxKxtpcB07Ln8ClgeOAAYkjrkR+AkLLTtEsAFWJTak/3+lAJbZ56SSRt23W8AI4AD/YryMA27zuwwu0mm5K/CUSBB0r2YAuwB3JD2+UDgyoDjjgDGAeMWW2yxwl9BIyNaH6SQWP4zFL40Xy3pf8FtffGTNGasReLLPm327LhT0gPSx2v6+ubkaNpfqFt2fLid75bPRtzJhI3QoxZG00tniakV4kgTFjNLl2Z/+PPFstKfrzK3+l3/l/KmXP09yxeanR+zvNXMFbOtQJorzSa+tSLaaqbTk6YPlt4+KLX5yCujPWETaeV3D0dfbnWTmSyu9HFmnb/NJ63wWSqfZ2m7HXvU5VbvQzuFjPLj0gvpxtEPFXbP85W4J511uvXpt/T8rxtITZLWV66ueA//kdK7ipzt/biQNHYdadpxWQ/ts35FyXNjfiMv2e4bAqpb4lvp8nEyL898TJCtDSU7XiH7bV2Y/9Tm6EvSGukHd8qm35/IJbEIhzmoWtkzQJBfEXVOUaqV5SJqqpY9ZzncpegciaWS/iRbTpekHyVtIrVVWVLe5ipLPpyuplgivf4ffRvsIH1uwlQjQe3uc6dCIxZWNUtX/zHXhjxd8HWWmBqhrdzyX+5yvwXQaqw2XXNDjfTiJplCt6kiU1c8dnXpwhMszkhFq6kf9rtD+mFh6b3V/PrLpI9WkTZ/3lQ9v0UsHiSQPlxZmrCUNOnV1K797jA9ddg56Z8PvSFY4HpxC6PaGLNF0exF4989HKwHr2mQ7t3TUuKVB1jbVDVLlx4jaUNJV6mw4NIFlKYqe4mUt1qi69mC9SlzigoaW8RkGj9J0puSxihj0XPKMGmTl6zPg2ZaZMb9ZS8GzVBk/tBZDZnrpyt/LH24ii0SzxwkxWOyxaaoVcdVFaz2ifn9zcOxCs9w91DyoNtkdo911m+NlC0+O7KZk4J8PeCZtM+nAqdGnVOUIL9U4W/10WEn3az8WZIrZfZQLbL45lmjx+YqMzlLPyXpPDp2oiIX5Zb+KnjHi5tGRJhrCrcaEWYBcuwlFmGxqtksMoK8QBtj0osbpz4nsIXBpCneyJ/NxnpmnfTjgtKMOhtJ/jZUemI7aYtnczPWhzk0JdubVSsbqY1PrdnVzbJ9QeckkCaPMDvpjlLp8W2CdfsXnpBbR3IUP3VYpnNQdll9bLAHabKM+k55rVSKLc1VFtoY2QtyyiKSLrLxQtTIdFFlMUHSqlLLEOn3N/jJstOfFVmoIF0VUXFMOvfy1HO64E++8A44Tkdnd8Dnk4j6PUl7hpyXRptsWaFaqYx2VbL0pJKkB0LaqJZZBDnSmZOCvAz4DlOaJRc7A+JkpkpRgrxFtl6Z/l1XyNQqH4WdNEF53Z5nP4wHKlToN8ak0R+mNr3mV39elqdkdhk8I3hHAumQm9IiIMoEbNLzcrf7w130m6oz45Jfd1h4gK3GmI0OkyPfnxa0H/Js+/GECewzzsgcNcf9cw+8JfP67tzHhG5QW1OG+5nrS6WOPTMF8o0H5zozxcnc1lkiPbyjtNbbFv529j2cFu7w01YmvT86OpLi0N+iv6PKlgKejzwleQ1xL9cRbFCb9IEfuPzXPI9jubJokzrXlx7fzsI4XHGkZXDaIk01F5P00b+j+3fkFal7cP5JmX4CGaVKZoqbzWOKtgBYXKlZbR7Gy6xUblBWNI2lI+ofU1jdA4g5JsitbrbHVlq+BU7Pd3zR5oetkq6XtLbMnvt4mQVYJDt3/9KaqzJ/nMln/aOJ0UJitfdsxPj1krnCNqm73fAVG1XvfWfKtruqSfp6qdzIdk3VZg2T3kZQSN1kmVmXm01+2lDp7FPNkWbVD6R/Hx/ucdpULc2fjJ0el9Z5PXz0f/i1qX2TFk69LIZMDxbE9bUpAfj9opY+rnaWtVPeqtlrDVs8Y2qusGv8brFoq5S134z+jpb5svvPhzAh/v6q0npvZO6qSqSel3ZFTxCX8J+tr2XP+o+b27NRW2/PRnWTzS5u+L2ZaSKTvVe8ocjQzhcdn7oHH4wO6DvS6+tJ724ixZ9RLl8oOuJoqSx89CeyEddLis7WlU2Dos0uS+Q8hzKZo4K82DLnPTsjXPCLKfW1NoJG0grp9U/OjV+SLNVNttiVjHMyeIZ07slpI1BPuvi4VNKFihaJuHTKOTb1bYjZKLXTH+W1VEqPbS/VzrT6axqkmw7OdQRKLw019hIJ25/AhGzYAmanJ+1+n13LSedJ/9vNAn+9t7r1pzEm/TxSOvQ66eCbU+dNG5oKeXvQLbnCv5NMr9SVPgk3OVzp43Adu7AX1d53BatXYg324lvtveDvqKbB4tEU9BxUynQfEceMWyPrlBZpr6wMVCcqWCZW+c9WpUwmr/G5tOZYiwGT82w1Stccbv/XSrq5WZFxf5oqU+ahr22Que+kc/1FcP/FWR6XLlYA6yq/jXt2yrwaFabjblN08PfqAuoYWAwwQf6y8uvICyjNVWbBMlS5M88vdpaIp9KWVbTYD7iqKVc4xRpNUCc3fL9I5v4gXXCnJ00bIm3zhDRjsP24a2aZi3vSOiWsfL9ItJXJ9CHSExGxsJurpFc2sgXQeKmUKJdOGWejzMUmSkt8baPojV5J9TtBKm42MquZbM/L9pJUv8atESysZpeEvSyCdjbUSKf/y14U675pgrmkQ6psM5nyj7tN9/7rMFvULWs3FVYyF+WRVxYQu6ZEJmVvlTnhhDxPrRXSv05PCcyyNnuB1A/3H5oOSbdJ8fWkH5aXrv+jtOzXJr8qZSqS9OWZIy/PWi9ISNs9YaEcvlhWenNtexaq5Kece0Om/qhVyhQGU6eddL60yA9Wx1pv2cKpMJVa2GzlhqznXJMlLas8CW1DyuvZlQWwo4IXU8skHVzA+QOLMEHu2b65y5gxYzRu3Lg52MKNwLFAU9dOF9AWgxsugLqjLGx0BtOBKyB+Hpxzotk/1zXAD4vCf/8ErQGG7VUt8MtIGFwP8ZJUbPORP8Jp55vd8bZPw5ITUuc0V8Eb68NWL1qfHtgVtn0WakOuq7ME2qrgm6Vg1U/snHgJlPk2zfESaK2Cfe6BRX+AC0+C2oD46wJUkmYHHgM2homPw4OToPUW2Ox5WPcNM4FOeFAi+HRFWON9s1lf6mv4eFWItaTqbayBR3aGacPgx4Xh3yeRG2g9jS2et+MrW1PX0BSDj0fDQ7vAhadYX99cH17e1Nra+Vw4vAom/Aw7PGA27u+sBZu8Znb2uzwMS38b3J6A99aCNzeGIWtBaSP8BqxcCZsdASUtmce3VsLGr8BnK0OzH1e8rN368dp2MPpyzHfhdWb7M6gUOqrgf0/D+A3Nnjv9K9jlQfMxqB9iny85Dg6/IfWdJ7/DN6+BrQ7yT2oGHgQmAKPgsythw6ehuRraq1L9WnICvLUejJxs31EQg4GZ2RsTwJbAS8HnhLIyZrwexQRgDNBIKj9AJTAUeB9YsMg2+zee570naUzOjiDpPqfLnB+RP69o88OwMkbmtbeVZtvh5nCVbDhUKxtSlWn2tDI04qAs9koykfFPI1PbSztsylzdaKPFg2/K9IhMj7fy0kbhNuoJpC+Xkr5ZIrWtqcpUCD8sbEGq7t/dgmMhadB06dfhuYuYzVV2Xk4bNZLu9u/Bl7IcqQtLWk76ammL4ljVnDnSW+lja0NID+xiI826WaYOKW+xrEvZFjLppbbesjbduY8lix63uvSXyywC4g+LBJ/03GNSTcBI/7EdbPQc1ljcv/bmKumcU2whtdbva+0saclvctclLjxeqgrypI1Lq34oi8sSNntaSNosYFRc1ZSybFr9vXDLH1XLXCenybdFTLHY5OCYOlVN0j4vK3LtAIXwr4hrCXsmy8Iqy+Jn2cLXYjKV6GmyIC2ObBhYqpW48ga4yilrF1Dvywo2laqUtLhN88Oqr5tp6orGmHTiheHHxRrNdjpoZ0tluEqgvdRsyDOOr5B2fcA+lrf5+vhO6ZEdTP2y3GemO21Os5+/7jBz9w9sZ+Pg27LBW8Fxu0mYGuDTFTLVBRu+Kn2wqr2k2spNzZK9WDj6A9v33z9II35NCaa6WdLTW4V/j4eH5J6sarZ45/nCBvwyf7CJaEmHmZUm1xeOujw8ABmSqlulltUi2iqVjh0bvGuBn01FdflREU5VyRj5FTI9zXaSvrO1xihBXUgkgEB+VPTCZ0D5chkLBRCUONrRJQaQIK+XdJnMbaxMqSe3RKaMDFrlH6rCMoNvHXBu2g9r9wfDY50MmWYj3Yd3ypO3UqaXj1rMDCpNVWailr4tgXTr/pmBrl5fz7affK5mOyctPMlGf0lzvtI2M0Fc901ph0dtNJtAal9UltRjLdmsZYzdkwWaleHoNGim9NeL7CXx5jrSITek9OHrvx48ymyMWXIJZC+c+3eXLv5r7v0s7bB46oG27eXSsj+E36alv5R+zuP8c9ZpuWF+k6W23l6W2zwRbaOOpLq41BJhfy+kt+8MX8qJNablySyklEgaJj0/JfowT5YUO2z/korgAZkwT87YIgR7U7X0twvskC0UYXzSIrNHdN6chTBABPlkSYso3ImhWjaFu14mjMbIVCWFPkRRo/zB0q8LmUDMEARx+1Heu7upMbZ4LlzYJ0tFa3Ss7ezSUCNd+WdL3fbAriZ031lLuukQ6fQzU4cO+9USPN+9t7TEVxFVJjJjedc0WKCs1hDrhcW+T31ceJItUqZHJVzr7dT+t9cKv46XNzaVy0avWHCxUOepZunMf6Q2TFxMun1/6YGnpN/fFxw7fYfH7GUR5jmbLLv+L3x3dZPd559HWpKMqK+lVlJHHkHe+YBNcsLk4QkXRKhWgkql1HJq9CHDZSn4gr5zLyG9/ZZsgTaMKTLzlqMlXSfpTKsgfQbXUmmj8WSkzRoFOIF+IrNDT55XKun3Wcd0SHpaFtjsg4g+DRwGiCDfXflNpWKywOZdYe2IequkzjobiZx1mrTo96YX3+K5TNOv5krp4mPDEz8gE/RRcUrSS1u5tP/tmj3iLumQFv7BhGBtfWbY2pJO0/eW5nFoCio1DfaSCNq58cupj09sl9v3ve4xNUR5W7Rqo7NEOudk02W/slEqpntQWflrqa3Swh5UtUi17X5auVZfhZT+1TQXHsPmhAuD3fqRqcce+Z2psR7ZMbyamKT/kyy4d8ABsx2iPjMLvH/L+p/8TgbNlF7YzGZZca+47FBaOnrieIMkJaTnO2z07SXsu1npU2nshrK1peEqzOIkye3SF6ubddWPC0lnn5bprOXJstnNZpLCo0du5x/zht+PpNt+TDb4mlJEv/ofA0CQN6swE6kSSYd1sY17FKyaKZP9KkIezgSme170e/vR1DTYKDVoCl/ZYqZ7hd7Or5YO2BwgpNccayPeTt8t/pktLfZGMd/cZi8E73h4p9RCZtCC4uvr2/6y9ugXVEdpKob56+tHe27WNpmJc9BotrLF1C+19Raa98ulw4Vh9vavlg7R98tC7SYdvForAmYMCbvGk97wJ3krK+f6Wiukaw+3RdTtZlliICnz0X166+CkHAmktorw52zSwtLeTwSbZnuSzp0iG/VW+RuWkbSUgk+oVcHxzSUbmId9taWS/pl+8M4hBybLJwrWOZXLhPnAZQAI8qmKdi5IL9t3sY2EpM0C6itToA92a4Vltl/mi1yb6YqW1Kg5qV+unWVR/oqJgPjGuuYFWN0k/f5GyyB/z16mSkguxq31Tu4UPY7ZYae7/ecri060l9H8v0j735qySJk+yJxwFvk+XBXwz/+zPj6/WbDlTdwzl/S797L/28vMOzSwL3HplLMjPDvjFsVy54ekbZ+Unts8elSbvq++RjrrVBvFl/sBuar9l9Sb66aOa6q2sAfpVQ2ZJk1MWtPUKiMVz/urSts+kRlZ00vYgPPztDqW+DbEagiprVR6YHcFCrmWCun5Tc1jd4lvU7vKZUG2/j5L+mVxqaPQ30iVpDNyfwJhfKRwFVFMWeE08j3fmys8pkFM0vuF96ufMQAEeVyFZVmplnR+F9sYq4JX7mcMlpb/3IR1mHVDrF469Rzp0OstU/ojvys8oURS+LSWmRXFlGEmhGYLpFoL+lXeaqZ7QXXEPenx7bM2h6hcvHjWIm3CPr+3mr9A2Gyfpw8J7/MnK0kXnmj9TV/MjXu27YITpK+WSm2/Y9+Q9YSE9OfLTdUReosSqfu+4SvRuubJI2wk/vmy0nW/t4TQx18g/e08U938+/issLSYs1b6/Vj6y9xrT5B6aSXNPrNLiSyLTvLzzg/lCU2wuGYP3+Np7aQ/D40x88xNP/WEi8OzKYWWbVQUByh3eSome5FkkM9Zb5mIfTWyoHgDkwEgyGeqsGh2g9V1G9VdCmwDc+2vaDWLkaj0ar97pLD60oVDodubK83NPWo02l6m2W7asXpp/ddCrDZCQvYOmyot93lq28nnRqd4i2OhdTtKUgIozDZemKnhip8oJ/zvIt9Hx1rJLu+MCc6dmsBUHclN/zlGas+XuxPplQ3Ma7KsXVp8gtnqB93nBPZSD9O7o0xDkHXeCg9NECelM8+nN2+qNlVQctPdexX3nAmZCqQI4pIul0U0KPP/XqYAW4I987R7oMLtJAdJerK4fvUjwgR5QJqc/syimHfa8C6ePxZz/8tDRxncsw+0V8LKn3axrZCmwhwhg7ZXt8GgxkjnyQw2eRWe3wp2fMK8KQfNgrp6KOkMb3j3/8GzW5uX48TFoakGrv0TtFRBfS10ltp1NFdBQy38NhzwvU09v9qop3D9t2CtcbkH/TISOsso6PsA2O5peHdt80JNp7USrjgm9fmevaEsT50esPEblumpo8KyIy36U/At8jCP2yhasEeyFHhnHcuYFNZuiVL3LQoBe9+b9rnQhyCd9vyHpFMCHA38AHT4f48h4Pu9EigPqWRPzCu7KmR/ObBVcf0aAPQjQV6H/RLCqAG+B1bvRhsFvgAaayHh39oVxkNHuQnFv1wJtx0A55wGS30DNQ3mLj+btP63l0DcK1hOdYkE8PwWzJYMrdVQ3Qr37wVfLQu3HAKP7gSD6oPPv+4IuOQEWOxHqGyHxX+A80+BhSbDwj9ZyrkdH4fVxsH+d8KWz0FzDGJBuftCKG+DrZ6Fb5aEsWvCbv+z7Z3lfqq8LAHlxYPrmT7M+jJ2TOram6rhzP+Ddd+E19aH19eHv5+VK+yTCPhpZPB30hn17JVYKsAwlvkK/nIS3HIAHHMt7H8HzBpkL79kX0XhL2SAmhZY4JfU53fXsustim+KPL5Q5sfS9a2Uti2Zcu8+YE0sN12MlIiqBGqxlHM9kaFyLjMFS7F3AHAW8HMP1x80TJ/TpXcWOwt1F47iRoXbqKdNBeOeBWtCZld+8XE2vU6qHNrKfVfwk9Om+2Uy21zfVr3dry97Gl2UKVpESS52rvKRbapokU4+J/jYDV+xf724Wb9s8Jq0+rjcoFjJ0hgLCFeQkCqbiksq3V6Sil+efi/eWjv4lJoGaeOXUhEYs0tFizRpQVPrPLGtWQ69smFmuN2o+xsnFdM7+zsJi50uzIkorNrjL7L7mHwOmmuk+iHSCQ9Kz+wrTR8m/bKEFC8yaNWsOmn3+1Ob6maZvX1bIetIybKUUkyT9Iqkj9XzoWXD/DjGSTpU5nz2D/mZt/seyWx8yUekUqZO60ISJPq/jjyf+eF8PdBGp6QdlLlYU+3XfZIyFkIvPMEsOUo6g3WzGcKgRGZW9WZE/9POidIpF1ISSGPXzEyaUdJpCSKCBNmzW0q7PChNXsCEfzLtWliyiY4Sy12ZvcuLB5vV5etr0LZTz849PNZo9t1hC6Rl7abTv2NvE56T5y+uP2F9iWM68qAXW2PMUual9yO53hAZS8WTmbRc6z97CxXez07PnJZGfy0toNQYY8SvtoBc8GBgT5mh++Ey6TNYJpGWlglZR17qFeG9q6wkG/kZAIJcspxSQSOOCtnDl9w3WNJFXWwjLukRWfjNjWQWMNPMW27qAplZY466zMzf8poTLitzdNgkz3HJuv36Z9VKf/+nOQANnSbt+Ij07hr5z58yzLIU1dT7QasS0rknRZ/THmAkHCYQ4p50wd+Cv4bEJuECsZjHaMqw3M219dL45cy1vabJj1UesEgbazSb+p6a3QizFpk+2Ebm9bWWtKOp2vK35hyekLZ62l4kefsQk3SvzBA7YNSfnsS6pcIWuBP4I+8TpYmyvLe1vv/CQpOCv8ucUiVzyjlEwZZadcrN7pKQ5ctdVRYHYLT/eQAnh7hJ4daW1SpaDA0QQf6zTDWR/uDFFL4Cfkj3movLpk0byLQ6QXHCZwwOT8mW0cf/ylLFFHAL62ukL5a0LDfpMT88PxzAbfvnr+O91c2F/7HtpdfXiT62WIFXX5vy9qxusuQTF/1VuusMRacOK6K0l+Y65Mz/i6+6KTH1xOAZ4VVs+GrhbaWrd/Jd9363W1KNPe8Nj+z4lyuKdL1fShbhcE1lSIWmSnth/PFqadvHpWMuSRPSFZrtwZyQ9Ppl0k1/sNlVXpv6mCxr8i8Kt+eukPQ3ZXKMcqVWjb99gHKmog3djiquugEiyCUzQ7xE5va3kaSIBApCRc9tksyS/a7Sf49fLt0NtUdM5p5cwLGN1eaQExbcacEfi+tHmOpHFC7EkqWjRHp1A4mEqW5+my+VKagnR8Bxz+yt13nLv32N0u37SdPmkz5c3TIIRQW2qmoqoq0C+96J9M+/m5PVkt8ocDYweEa4w09oKZWpDltlcUeWlhprpKv+aLOx+aZKx/87Tf9dKdMrp4+EA1K9ZZeEJ5sVJpMeP6HADES/zC99OFqasVFa/VFp4ar9/QOQ+xQeUbtGFq6mCAaQIM8mItpdAlns4y6wn3JV8qu9F74AmLcMkmWnzXNcU7V05ZHSkl+HH1bTID26feFtz4qI3d5WXpwAfmFTi3Ve2mGqg+7o8wuxl54xWFp2vHTLARbTpbLFXPsrWqLDzA6bmmqj0L4U2t+WChPWXy9jC8Pph+1zV/T9Dizlyg1kdZNMb14hG/KVKJUv7liZ0E9n4zxtLC/praxzXlOGFJq8gLT106l7XNVmJt9NknS2wh3yyvz9A5A2hYugwbK0pUUQJsj7kflhCImOPAc0Zn6sr4eXzoUJy8GkJeCVo+GnH7KOwRKyLDjRN4fz7dE+XAMOv84yszTUQFtFEeaDAoYAV4Xs96C9HF7YHI6/xGzUwyhJQFNtoQ3DB6tZX4O61FlWuNlbS6XZoNcPhW2egeqWwg1cBXSUQn1dyvb8/JMsI5AIv4+DZsH4FeDaI81O/fEd4dOV4bmtYcTU4BMrWuGQW+z/jtJwc8N0CrkHSfvuqnYzsVz6a3hmazjzH/DRaPh+MTjmMijrLKCydHYm1+Tu98CPfqnHjNG/B2YAl2Lmeun8CTPBDWIosAxmw70WcAdm87ge4JsstlXA+m/Ci5tZFqr6wdBaAfcDO4FlKQq7rk4y0yANICqAF4EFMAvpCv/vfMBzmEVlTxDyrpijZa6OyL/cNdob8uf3UsfOnCFNXCrTlKytXJo1SPr6s9RxX0la5yMbWZ11mnKm0INmml74hAulv90i813OF+OiUtIPfgMzZBH5d5Zl7b1S0qXSvY+mwstu9HK4t2BtvTQj36gvTXE3bYhl3GlJ62NX1CA/LJKyGPnLFdEmeUHlkxVtxHrcJdLp//SDV8Wll/KMJicuJv39zFy988cr+cG60u5TdZO5088YbIvQ//2DdPcexV9roSVBZqCwpEdrUfX8Vd2nU+Zyn67DLpWN6IOSJ+8l6SmZirJEumvfcA/lmKQP3lW4eUatpBd64Br6MB2SHpVFAH5ANlLvAgxY1coek2wBLPvHk8BiJt+TduxLfws2R4t70sfryeaQE6WGJstwE0fa+UFFhoSdv0n5gwTFVFCi2W/STllokr+YlqU+iDVKh1+Tp72A0lGg63dUaa5KRQ7c9sniVQhxz0LXLjZRWnRCKoztMZdGLw4+uU34S+O7xaVNXpSWGy+t/JEtutbX2jOx08P23e12b9evea6UIeoZy48OWTLpdWRWXAcpfDEza4XuoJvDu1cp6dKEzIQ2+/dT6W8fwJYrPcjAFeQ7yxw/fhuaaar15jrS8BZ7SyaZEqFPb62Qpo2ULdykxYieOUg67Lrwqz3oQUXnDy2Vjbqjgvn7XKDMgf3SX0rLf2Y6y5oGW0g78vIi9dJe8QuaUeVPV9siY0mnxaYupi+dnr1IG2NmmXPRX03XPnRatAnnyxuGC/K2stwUeMICXCGLDVNo7PceLUU4RqlMUmPko9E1HlDBuW2Pujw8hv7sRbt6WYSsZE7bKtliUv0c6PvAZOAK8keUGhAv84XZ7yZHejUyY4Ak9REPddKBJGhfY0z6y+XBpz50m6KjvW3lN/6CpJWUcrw4TjnC/fSQKkb8YtmBTjwvop2IUsyCX75jm6ulTV+wF8tKH0u/jrARcL5zgxydmiul2w6wj5u9YPUkIzymeyjOGBTunCTsJZ7+ua3c8mEedXm4TfvTW9usYqVPpANulT5apWv3NrQk4+I/LNPVDY84NmnRtJJMUH6Q+VzoNVlAtxVlyVVy0vEEMN0/tsAgcG+vHZ7ZqkpZ+R5myaxUCkmf6CiGgSvI47LfQFB4zduzjh27dfgIcsqw6NHljMG5MccHSaYPiYqtfKmkK0L2j1SGMH9GEWrIWdKT+Uwtu1nyRVJMlgTSW+tI//yHdMGJ0k8j8x8fVm9buTTqG/s4eLp01Z+l19fNFdxBCS1ErqqsrdQSMAyfEh6l8b7dpQNvTamJSjv8dH3F6tLzjfSrZd46M2VWHUEWT6XKnIaV+M/Nnf5DcaH/OSmQPf/z5annRnGZSeFVsinoBNmzVaRX6yE35druxxTh1DJT9iO7Ruba7+guA1eQSyYLr5UNaBaQmZYHZbL69J1wN+uXN8zdni1INnrFpp+lnZZuqzNZ8a7K/ZGWyEZhvyl6mn1iqn9xSSu3WHjc9EPKW6UVPy0+YXMxJYH00ibFq2C6q7JJIP3pSmmNdy2Rxay6LFVIlSxeKrn5OLNVRsmFx0uOsbgvQX3rLDE1zaw6mwHs8Fia0GoswpHHkzoPlhL5Xn6VsjyyzbKEJ0kHNs+/trBno1rSpwofJFTJYpN8I2mUUqqOOtmLoQvqpASWKGX1uP2ONpYNLgK5we9jrf+3Wpa4e06oiAYOA1uQF8PYp6WfFzad7Kw6s1jJZzUxu4yU9FxApS1KpdgaLHuo15L9yP6bp86hmfVMW0ba/nHTQw+eYX+3e8JPfDAHBXlLpfThysUL5k5MDTKrtmu6+ATSVX8y9/cwganHlH9BOavOQu3jG2PS0n6i6rpZlrh69v4QYdhRKv3jHGlwp7TW+9KXy+Vpq1wm0MtlcXu2lnSypD0U/ZIfpHDb7SqZicRieeoIKlF689WVn9cVHFyuStK+BZzvCCNMkPd/O/JiGbMNjJwEP78FP70ElS/DpmMLOHEk8BOwZcC+KuAmLHbls8DnwLvAUsDkPPW2pP3/IMw3GZ7YESYsAc9sA98tCU/uAMNmAAsW0M8u0F5u5tirfFpcKFWAhsGw60Ow5Quw3Jep8L7FMOw3KA+zUa7G7JSXJjzGdRYeUOH7F+Sz8y/vMNtvMJv6GUOTO6D+aItnns2+d8FFf4VZpTB2ddjgNbuHoXQAbf7f6cDrfidLMXvuMOoJt91uBcZhduVRdWRTCvwCjAjYNwh4voA6ziPzuU3v00PAb0X0x1EITpAH4Xmw7CqwwppQuTqwH+HOFGBxk28l/+0cCqwNjErbtmuec1ZI+/9TZjswjfwV1nkXFkzGnBYW9LhAYRaEQv4v6bR44115Wirb4I0NYeza8M2y8MNixZ3fWWoyrbYp5IBWYDzwArA55ghTYLzq5EspSphXdMBaY2GXh2CP+2HD1/wdcYsr/+tIc4xJ8slK9qJtSYv9PW0YTA9JFhFIM+bUsxbRz10UtdgAoojY7wCMxp7nX4Brsfj9o4GLsZdCIdfxMeE3tRL4tsg+OfLhBHlBXA9cBiyHPeTzYe5ZtcA2WNahrbtY92gyBXs2V6T9vwioOkLwdGKjui7ikao7feRdJstMUzQx+OkPsOQMqGqGkjj83xnQXkRiAA/4eHSw5ylgwmpBYBjwNHAJRSUeSHpjhl1eAhjzHtxyMFxzJKzyWerEpwVrvAdXHQVTRphX6qXHWXakdFQC//wnNMYK7xcVmDtgjOKnQWXYPdmc8Ew7QcSAs/3/S4A/Au8DHwHHU7i4WChiXxtzbOY4gHGCvCA84DDgC6AJmIZNaxsw4bF2N+v/CFgya1sp9gLZwD4+C6xxBJQ2QqwZDroFJo/sZrsBdCUjWAZ1wGBMgBwES/8HHvgMShOQKIXqdhNshdIcg6nDIzLstJA5q7mLokehL26cCguQjQeUxWFwA9Sku5kLVGqZh068GBaYAoPr4aY/EPizuu6PcPY/oC2GqSgGEZ3RCmw0/gaZmXSC8LD7PRhTNa0DvArsFnFOmX/soLRyFbB9nrYK4XiCZxKlwBpAkbMyR36CFOdzuszTi529yqeSzpV536VlTblPuWtHZe3SyJ+lqcNU+K2f044vtZLelsX2/S3V/2Nf8sMJJMz0r5g6G2PS7QcHhwievYCWnox32eLqb6iRdnhE2u3+VGJjUYDNfIl0ckd0LpPsUiPpjUaZz8ArsmzwUcGqZ6Zd1y4KX7RcUdKXkp6W9LUyeU728CStW8r9um+WeSq/IOllddlnPJCEbHE/3SyyRtKCkr7vwXYGHjirlbnB15Iel/RRz1XZKWmEgu9kZYv093+F7MwupTI312JSfRVZOh8zg4UnlRkdeK1PfZnbXLwXZaJC+ma3PNd1blpjB6rgF1ZLpYXcTXosLjZRemAX6YeFpG/yxYavlKb+ZGZ46bd0yHRp8R+kqiwvyGpZcqkMT/U2WRKG7JdUTGYfns73MouWbGEekwniKCZL+pfMAegkmbXUnCYhS0xxhCzT0A3ywyTOoyRkg5AnNC+nlHOCfI4yVWZUW61UOqyVlDs66gIfKtoxdMlvsjZ4CjZDHCrpNhXskl1UKZM+uN/CddbJrOKqZI6L7ZJ2flsibmFlu+QO/3eFD31rJF2fdsM+UXhe1bSSqJP+9Xd7GQa+HzoKsP9utFwmh0iKxaWLj5daq6T2wdJVx0mLTpJKEnZfzvHvRQ4NMl+BITIhvZwsI1AQEyTt47ddJmlTmfBxdI+3JS0i+6ENlj28eyrT7XvewAnyOUZCNqrKjm5YIvsFd/Nh+EDRsneJb9M+lMmcj8bIhFmd7OFcXCbgoqLcRwjpyMiNQ6XPFCw7qyUdKem598wj0IsXmGYsuxytcMeXalm0yHQelwnGpPNL8jrq/I5eZTOdKLP7ks5cB6OMe7JLVpv/F3ITYnLCdl7mBwWPlKpkdvzzFmGC3C12doUPgROBw4H7voD2H8i1FklgZmT3dK+tlQm3KKxohT3vT9vwByxQ+ljgLeAGbDF2gl/RZgH9TFKS1VAFMBy4G7N7D1sFPRvOx4wRsmnBzOfXXB0Ofg/mmw5eV6xfvsPM36pJLRCW+59vw+K4p7MDZor5P+BOzPLiHizO9hTgSKtmvYgmN3gdSoP6WoNZZVybtq0Vs5YJirndAvwroiFH73IFELSQ3go8Dkyau93pIk6QF4OAozBDkv9gcvKwpWD5cfDLAgEnNAKvdK/NMr+tbMu10k4Y1GBJJgAzhdww7YDRwF5+Z5NCeDjw14DKKoBqkJey3BBpx11F+NvkMnhDEPdPWvEzWPctqG2w3ZXAZx5cuTE8MAkaBxdw0dkMBY4E3gYOBTbFEiV8BOwRck45ZhK6J2YLvTuWASHNmuIycm8F/raLGzGHrcGYRdF6mLPXRcBnmGlgkgkRfZff78+BNzFrpyhasZfTrQzYZAxzlZcJFuRgv4v3515XukPQMH1Olz6rWvmfgo0MytqkLZ4N2iFbXOoBHpK0TNx0t+Vt0i4PWkIFIdMRjJCFAshHsywedZZ6oi1AdVGQPrta2nGi5c78ZkmzApkxyGLWXPA3qSZuqpfZXKaCdNgZ5b7i71ehvCtLnl3ql/U1O2dx4fys6ABU5bJrHiybsh+r4LDFf1amvseTWX845hxRKRHrJL3ae10LAKcj7wHWVfhVVTVLPy2YtbFa0uc924em8VL7KKX0vXWyhZrPIk9LsZtyAnhFxfrOK8xj0lPPSw0BesbGmHTd37IsNRIyQZbML1kjE3RhpnUxzZVFp1blprksinVVeKybmHLTp/8z4vjsbPWOniMqTs/8Sot8N08QJsi7pVrxPO/fnud94Xnex57nPeR53pAemSbMq0Spyyrb4edF/Q8eNj8/mUwX+yg6gUeAUzF98E/Bh8WWh/KPgCOAjTC9+KfAigW0MQF4kpw4GJVBCm6feIijzGwGwdb3QlWAE05NMxx2JXgNaRs9zP38O+BKTG3zA3AMmU4kJdg9vIvZeSPnJJXkprksilsxXX26J2UZwWsLzZhebkbatvMi6r60Ox1zRLIDOSo3yv3P95HfaWveoLs68ueAlSWNBr7CpFD/JUomt9XBqDWBVbFkuU8BZxRY8U/AssCB2Mrh6VgQqCsCjn0dWBRbbHsGuM7//GoB7bxNoK570qK525K0VkF9WIbYGPBvKHkxPKFwSQUWeyObhYBDgIOxgGP/wQIq7Yjdw99jQZ92Du/bPMWymOfvSZj34oaYHj0q5khS/9pJ8GpxkmQwLUfP42EL4ncCWwGrYesvHwOb9F63iqRbglzmwpf8Bb8NLNL9Ls3DnELw4lgl8LsSGH41ZtLyELBxERXvgo1KkyPXNmzR6xQsSmKSRmwEUY+FCsD/24AJwPSRbxB1wZvPPSU4DkhTNVx1JFQkv+ISbAGoCnt53AgcgLl3h9GZZ386WwGPYffwBgqfzcwrzA+cCbwHvEa0K3qc1H0p5GdYTJwWR3F42IDhWeAD4HJyQ2bM2/Sk1cqh2DA0EM/zjvA8b5zneeOmTp3ag83ORbYAzsLkWIzUDGwMJtO6xHjMCiIesK8FU7MkuTfkODBzx3ymjkEhdoG31oObD7VgTy1VFs2vuRqe2g7eXB+qk2qTI7CoeN8B3wP7pG0PEzQjMNPHgcgfCY9eOAhY0/+/hGjBsTDFBb9yDDTyhonzPO95bO6bzemSHvGPOR0bet0ZVo+k6zA9AGPGjOmKMfG8wfHA/tiguxFTU69NN4JNfYeNcoPiNwv4Mu3zt6RG4tk0Ad/kaasKuBk4CBvx+3GqHzwQVnoPLvkr/O5xKI3DS5vCj4vC2LX8c4di6o8ggXIoZjL3ESmTuTJsqnIHPRCJq4+yH3ALNqtK3pdS7L7cSeY46iHMTDI7drjn73M4wskryCWFDOMMz/MOxub1W/irqv2fBTA1Wo+wJOF2rB4WOjfJUtgIL0iY12B69Xzs7rd5HqaDnh+WOxa+9eDwJeG/x0BZG+z5ADy5Eyw0DdNjX034qLACC+V7M/BfYBYWQvVkYJmIvozHhNlyEcf0Zcqx6fqtwDXY4uYm2H1ZPuvY5DLTH7CIh2BRDK+j76mYHHMbrzuy1/O8bTGXtk0kFawvGTNmjMaNG9fldvsfa2G6uWy1SQwTkMkwuY3YImGQLrwOWzQN0YPPU1wI/IPUC6wCuAA4rrc65HD0CTzPe0/SmOzt3dWRX4lJjuc8z/vQ87xr853gCOJhbGEsKYQrsdHveWTGOq/FzAfrSOlea/zPj9M3hPhl2Ig0fRbSjnmc/rdXeuRw9HW6NSLvKm5EHkQn8ARm/DMcW0hcOOTYRmzh82tMnbI3fUOIg80ygtYDwK4hnwu7wzFwCRuRF5ETyzFnKcNMoAqxm67FMhb1NRoJF+JgKqNO3GPpcBSHC5rlmIsUIqDdI+lwFIv71TjmIlVEZ2FfAPdIOhzF4341jrnMrV3c53A4wnCC3DGX2RGzrU6P77I48AKwTa/0yOHo67hVJUcvsBUWW8bhcPQEbkTucDgcfRwnyB0Oh6OP4wS5w+Fw9HGcIHc4HI4+jhPkDofD0cfplVgrnudNxTIT9CTDgd96uM65het77+D63ju4vnedxSWNyN7YK4J8TuB53rigYDJ9Adf33sH1vXdwfe95nGrF4XA4+jhOkDscDkcfpz8J8ut6uwPdwPW9d3B97x1c33uYfqMjdzgcjoFKfxqROxwOx4DECXKHw+Ho4/QrQe553r89z/vC87yPPc97yPO8Ib3dp0LxPG9Pz/M+8zwv4XnePGfeFITnedt6nvel53nfeJ53Sm/3p1A8z7vJ87wpnud92tt9KRbP8xb1PO8lz/PG+8/Lsb3dp0LxPK/K87x3Pc/7yO/7mb3dp2LwPK/U87wPPM97vLf7kk2/EuTAc8DKkkYDXwGn9nJ/iuFTYDfg1d7uSCF4nlcKXAVsB6wI7Ot53oq926uCuQXYtrc70UU6gRMkrQCsC/ylD933NmBzSasCqwHbep63bu92qSiOBcb3dieC6FeCXNKzkjr9j28Di/Rmf4pB0nhJX/Z2P4pgbeAbSd9JagfuobDM0b2OpFeB6b3dj64gabKk9/3/GzDBsnDv9qowZDT6H8v90iesLTzPWwTYAbiht/sSRL8S5FkcCjzV253oxywMTEr7/CN9RKD0FzzPGwWsDrzTy10pGF898SEwBXhOUl/p+6XASUCil/sRSJ/LEOR53vPAyIBdp0t6xD/mdGwKeufc7Fs+Cul7H8IL2NYnRlf9Ac/zaoEHgOMk1fd2fwpFUhxYzV+/esjzvJUlzdNrFZ7n7QhMkfSe53mb9nJ3AulzglzSllH7Pc87GEsMuYXmMSP5fH3vY/xIZuLNRYCfe6kvAwrP88oxIX6npAd7uz9dQdJMz/NextYq5mlBDmwA7OR53vZAFTDI87w7JB3Qy/2aTb9SrXiety1wMrCTpObe7k8/ZyywjOd5S3ieVwHsAzzay33q93ie5wE3AuMlXdLb/SkGz/NGJC3JPM+rBrYEvujVThWApFMlLSJpFPacvzgvCXHoZ4IcuBKoA57zPO9Dz/Ou7e0OFYrnebt6nvcjsB7whOd5z/R2n6LwF5WPAp7BFtzuk/RZ7/aqMDzPuxt4C1jO87wfPc87rLf7VAQbAAcCm/vP+If+SLEvsCDwkud5H2MDgeckzXOmfH0R56LvcDgcfZz+NiJ3OByOAYcT5A6Hw9HHcYLc4XA4+jhOkDscDkcfxwlyh8Ph6OM4Qe5wOBx9HCfIHQ6Ho4/z/8MCCt8sVfnEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Normalizing the data (mean=0, std=1)\n",
    "x_train = x_train - x_train.mean(axis = 0)\n",
    "x_train = x_train / x_train.std(axis = 0)\n",
    "#quicker way to plot the data\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=50, cmap='spring')\n",
    "print(x_train[:,3].mean())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old loss =  [9.53929371]\n",
      "[[ 0.74103969]\n",
      " [ 1.92384508]\n",
      " [-0.93967353]\n",
      " [ 1.13056768]\n",
      " [ 1.09259298]\n",
      " [-2.28836489]\n",
      " [-0.60025259]\n",
      " [ 3.0633428 ]\n",
      " [-0.96473569]\n",
      " [-2.9897273 ]\n",
      " [-3.51712174]\n",
      " [ 0.95631136]\n",
      " [ 0.95631136]\n",
      " [-2.35572104]\n",
      " [ 1.41999691]\n",
      " [ 1.0724019 ]]\n",
      "new loss =  [4.73489546]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "w = np.random.randn(x_train.shape[1], 1)\n",
    "b = np.zeros((1, 1))\n",
    "op, cache_sigmoid = sigmoid_forward(x_train[15:20], w, b )\n",
    "# print(op)\n",
    "loss,cache_logistic = cross_entropy_loss(op, y_train[15:20])\n",
    "print(\"old loss = \", loss)\n",
    "grads = cross_entropy_loss_backward(cache_logistic)\n",
    "# print(grads.shape,'\\n', grads)\n",
    "dw, db = sigmoid_backward(grads, cache_sigmoid)\n",
    "print(dw)\n",
    "# print(db.shape)\n",
    "b = b - 0.1 * db\n",
    "w = w - 0.1 * dw\n",
    "op_new,cache_sigmoid = sigmoid_forward(x_train[15:20], w, b)\n",
    "loss, cache_logistic = cross_entropy_loss(op_new, y_train[15:20].reshape(-1,1))\n",
    "print(\"new loss = \",loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplelogistic = Logisitic_Classifier(input_dim=x_train.shape[1], reg = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0 Batch =  0 Loss =  [10.38493373] Gradient_max =  5.32391708644967 learning rate =  0.0001\n",
      "Epoch =  1 Batch =  0 Loss =  [10.22336544] Gradient_max =  5.096499845642786 learning rate =  0.0001\n",
      "Epoch =  2 Batch =  0 Loss =  [10.08531354] Gradient_max =  4.885536238779455 learning rate =  0.0001\n",
      "Epoch =  3 Batch =  0 Loss =  [9.96744675] Gradient_max =  4.6900565438605115 learning rate =  0.0001\n",
      "Epoch =  4 Batch =  0 Loss =  [9.86688144] Gradient_max =  4.509029558172069 learning rate =  0.0001\n",
      "Epoch =  5 Batch =  0 Loss =  [9.78113431] Gradient_max =  4.341409005281927 learning rate =  0.0001\n",
      "Epoch =  6 Batch =  0 Loss =  [9.70807478] Gradient_max =  4.186166687603289 learning rate =  0.0001\n",
      "Epoch =  7 Batch =  0 Loss =  [9.64587944] Gradient_max =  4.042314823600674 learning rate =  0.0001\n",
      "Epoch =  8 Batch =  0 Loss =  [9.59299021] Gradient_max =  3.908919977142324 learning rate =  0.0001\n",
      "Epoch =  9 Batch =  0 Loss =  [9.54807672] Gradient_max =  3.785110715629328 learning rate =  0.0001\n",
      "Epoch =  10 Batch =  0 Loss =  [9.51000317] Gradient_max =  3.670080767147276 learning rate =  0.0001\n",
      "Epoch =  11 Batch =  0 Loss =  [9.4777995] Gradient_max =  3.5630890727638347 learning rate =  0.0001\n",
      "Epoch =  12 Batch =  0 Loss =  [9.45063666] Gradient_max =  3.4634577937914437 learning rate =  0.0001\n",
      "Epoch =  13 Batch =  0 Loss =  [9.42780535] Gradient_max =  3.370569053366858 learning rate =  0.0001\n",
      "Epoch =  14 Batch =  0 Loss =  [9.40869802] Gradient_max =  3.283860969247454 learning rate =  0.0001\n",
      "Epoch =  15 Batch =  0 Loss =  [9.39279357] Gradient_max =  3.202823364611104 learning rate =  0.0001\n",
      "Epoch =  16 Batch =  0 Loss =  [9.37964445] Gradient_max =  3.12699341720141 learning rate =  0.0001\n",
      "Epoch =  17 Batch =  0 Loss =  [9.36886569] Gradient_max =  3.0559514153708234 learning rate =  0.0001\n",
      "Epoch =  18 Batch =  0 Loss =  [9.36012567] Gradient_max =  2.989316724343615 learning rate =  0.0001\n",
      "Epoch =  19 Batch =  0 Loss =  [9.35313838] Gradient_max =  2.9267440206192985 learning rate =  0.0001\n",
      "Epoch =  20 Batch =  0 Loss =  [9.34765673] Gradient_max =  2.867919821538462 learning rate =  0.0001\n",
      "Epoch =  21 Batch =  0 Loss =  [9.34346705] Gradient_max =  2.8125593165583385 learning rate =  0.0001\n",
      "Epoch =  22 Batch =  0 Loss =  [9.34038431] Gradient_max =  2.760403493686365 learning rate =  0.0001\n",
      "Epoch =  23 Batch =  0 Loss =  [9.3382481] Gradient_max =  2.737214358834442 learning rate =  0.0001\n",
      "Epoch =  24 Batch =  0 Loss =  [9.33691925] Gradient_max =  2.7775254971996706 learning rate =  0.0001\n",
      "Epoch =  25 Batch =  0 Loss =  [9.33627688] Gradient_max =  2.8153639526629033 learning rate =  0.0001\n",
      "Epoch =  26 Batch =  0 Loss =  [9.33621594] Gradient_max =  2.8509112223571864 learning rate =  0.0001\n",
      "Epoch =  27 Batch =  0 Loss =  [9.33664506] Gradient_max =  2.8843330211170524 learning rate =  0.0001\n",
      "Epoch =  28 Batch =  0 Loss =  [9.33748477] Gradient_max =  2.9157808038535906 learning rate =  0.0001\n",
      "Epoch =  29 Batch =  0 Loss =  [9.33866591] Gradient_max =  2.9453931390252457 learning rate =  0.0001\n",
      "Epoch =  30 Batch =  0 Loss =  [9.34012829] Gradient_max =  2.97329694547301 learning rate =  0.0001\n",
      "Epoch =  31 Batch =  0 Loss =  [9.34181953] Gradient_max =  2.999608604793231 learning rate =  0.0001\n",
      "Epoch =  32 Batch =  0 Loss =  [9.34369407] Gradient_max =  3.0244349609597925 learning rate =  0.0001\n",
      "Epoch =  33 Batch =  0 Loss =  [9.3457123] Gradient_max =  3.0478742182279563 learning rate =  0.0001\n",
      "Epoch =  34 Batch =  0 Loss =  [9.34783981] Gradient_max =  3.07001674755705 learning rate =  0.0001\n",
      "Epoch =  35 Batch =  0 Loss =  [9.35004674] Gradient_max =  3.0909458109473955 learning rate =  0.0001\n",
      "Epoch =  36 Batch =  0 Loss =  [9.35230723] Gradient_max =  3.1107382122435885 learning rate =  0.0001\n",
      "Epoch =  37 Batch =  0 Loss =  [9.35459891] Gradient_max =  3.1294648821406668 learning rate =  0.0001\n",
      "Epoch =  38 Batch =  0 Loss =  [9.35690246] Gradient_max =  3.147191404358964 learning rate =  0.0001\n",
      "Epoch =  39 Batch =  0 Loss =  [9.35920128] Gradient_max =  3.1639784892372536 learning rate =  0.0001\n",
      "Epoch =  40 Batch =  0 Loss =  [9.36148111] Gradient_max =  3.1798824003362105 learning rate =  0.0001\n",
      "Epoch =  41 Batch =  0 Loss =  [9.36372977] Gradient_max =  3.1949553390457903 learning rate =  0.0001\n",
      "Epoch =  42 Batch =  0 Loss =  [9.36593693] Gradient_max =  3.2092457916494945 learning rate =  0.0001\n",
      "Epoch =  43 Batch =  0 Loss =  [9.36809384] Gradient_max =  3.222798842812275 learning rate =  0.0001\n",
      "Epoch =  44 Batch =  0 Loss =  [9.37019316] Gradient_max =  3.2356564590235606 learning rate =  0.0001\n",
      "Epoch =  45 Batch =  0 Loss =  [9.37222881] Gradient_max =  3.247857745138224 learning rate =  0.0001\n",
      "Epoch =  46 Batch =  0 Loss =  [9.37419578] Gradient_max =  3.2594391768122057 learning rate =  0.0001\n",
      "Epoch =  47 Batch =  0 Loss =  [9.37609001] Gradient_max =  3.2704348113217017 learning rate =  0.0001\n",
      "Epoch =  48 Batch =  0 Loss =  [9.37790831] Gradient_max =  3.280876478981474 learning rate =  0.0001\n",
      "Epoch =  49 Batch =  0 Loss =  [9.3796482] Gradient_max =  3.290793957135275 learning rate =  0.0001\n",
      "Epoch =  50 Batch =  0 Loss =  [9.38130785] Gradient_max =  3.300215128476312 learning rate =  0.0001\n",
      "Epoch =  51 Batch =  0 Loss =  [9.382886] Gradient_max =  3.309166125264948 learning rate =  0.0001\n",
      "Epoch =  52 Batch =  0 Loss =  [9.38438187] Gradient_max =  3.3176714608417055 learning rate =  0.0001\n",
      "Epoch =  53 Batch =  0 Loss =  [9.38579512] Gradient_max =  3.3257541496837857 learning rate =  0.0001\n",
      "Epoch =  54 Batch =  0 Loss =  [9.38712578] Gradient_max =  3.3334358171203293 learning rate =  0.0001\n",
      "Epoch =  55 Batch =  0 Loss =  [9.3883742] Gradient_max =  3.3407367997036412 learning rate =  0.0001\n",
      "Epoch =  56 Batch =  0 Loss =  [9.38954102] Gradient_max =  3.347676237128968 learning rate =  0.0001\n",
      "Epoch =  57 Batch =  0 Loss =  [9.3906271] Gradient_max =  3.354272156502304 learning rate =  0.0001\n",
      "Epoch =  58 Batch =  0 Loss =  [9.39163351] Gradient_max =  3.3605415496731745 learning rate =  0.0001\n",
      "Epoch =  59 Batch =  0 Loss =  [9.39256152] Gradient_max =  3.3665004442756845 learning rate =  0.0001\n",
      "Epoch =  60 Batch =  0 Loss =  [9.39341251] Gradient_max =  3.3721639690558423 learning rate =  0.0001\n",
      "Epoch =  61 Batch =  0 Loss =  [9.39418802] Gradient_max =  3.3775464140047733 learning rate =  0.0001\n",
      "Epoch =  62 Batch =  0 Loss =  [9.39488966] Gradient_max =  3.3826612857655425 learning rate =  0.0001\n",
      "Epoch =  63 Batch =  0 Loss =  [9.39551915] Gradient_max =  3.3875213587349386 learning rate =  0.0001\n",
      "Epoch =  64 Batch =  0 Loss =  [9.39607826] Gradient_max =  3.3921387222401624 learning rate =  0.0001\n",
      "Epoch =  65 Batch =  0 Loss =  [9.39656883] Gradient_max =  3.396524824133399 learning rate =  0.0001\n",
      "Epoch =  66 Batch =  0 Loss =  [9.39699271] Gradient_max =  3.400690511114087 learning rate =  0.0001\n",
      "Epoch =  67 Batch =  0 Loss =  [9.3973518] Gradient_max =  3.4046460660591293 learning rate =  0.0001\n",
      "Epoch =  68 Batch =  0 Loss =  [9.39764802] Gradient_max =  3.408401242614699 learning rate =  0.0001\n",
      "Epoch =  69 Batch =  0 Loss =  [9.3978833] Gradient_max =  3.4119652972794183 learning rate =  0.0001\n",
      "Epoch =  70 Batch =  0 Loss =  [9.39805955] Gradient_max =  3.415347019187373 learning rate =  0.0001\n",
      "Epoch =  71 Batch =  0 Loss =  [9.39817869] Gradient_max =  3.4185547577801394 learning rate =  0.0001\n",
      "Epoch =  72 Batch =  0 Loss =  [9.39824264] Gradient_max =  3.4215964485396864 learning rate =  0.0001\n",
      "Epoch =  73 Batch =  0 Loss =  [9.39825328] Gradient_max =  3.4244796369385067 learning rate =  0.0001\n",
      "Epoch =  74 Batch =  0 Loss =  [9.3982125] Gradient_max =  3.4272115007492188 learning rate =  0.0001\n",
      "Epoch =  75 Batch =  0 Loss =  [9.39812212] Gradient_max =  3.429798870843259 learning rate =  0.0001\n",
      "Epoch =  76 Batch =  0 Loss =  [9.39798399] Gradient_max =  3.4322482505968495 learning rate =  0.0001\n",
      "Epoch =  77 Batch =  0 Loss =  [9.39779988] Gradient_max =  3.434565834012077 learning rate =  0.0001\n",
      "Epoch =  78 Batch =  0 Loss =  [9.39757155] Gradient_max =  3.436757522651594 learning rate =  0.0001\n",
      "Epoch =  79 Batch =  0 Loss =  [9.39730072] Gradient_max =  3.4388289414769706 learning rate =  0.0001\n",
      "Epoch =  80 Batch =  0 Loss =  [9.39698909] Gradient_max =  3.4407854536731013 learning rate =  0.0001\n",
      "Epoch =  81 Batch =  0 Loss =  [9.3966383] Gradient_max =  3.4426321745340807 learning rate =  0.0001\n",
      "Epoch =  82 Batch =  0 Loss =  [9.39624997] Gradient_max =  3.444373984479655 learning rate =  0.0001\n",
      "Epoch =  83 Batch =  0 Loss =  [9.39582566] Gradient_max =  3.4460155412656825 learning rate =  0.0001\n",
      "Epoch =  84 Batch =  0 Loss =  [9.3953669] Gradient_max =  3.4475612914467604 learning rate =  0.0001\n",
      "Epoch =  85 Batch =  0 Loss =  [9.3948752] Gradient_max =  3.449015481144449 learning rate =  0.0001\n",
      "Epoch =  86 Batch =  0 Loss =  [9.39435201] Gradient_max =  3.4503821661702863 learning rate =  0.0001\n",
      "Epoch =  87 Batch =  0 Loss =  [9.39379873] Gradient_max =  3.4516652215487196 learning rate =  0.0001\n",
      "Epoch =  88 Batch =  0 Loss =  [9.39321674] Gradient_max =  3.4528683504816184 learning rate =  0.0001\n",
      "Epoch =  89 Batch =  0 Loss =  [9.39260738] Gradient_max =  3.453995092792692 learning rate =  0.0001\n",
      "Epoch =  90 Batch =  0 Loss =  [9.39197193] Gradient_max =  3.455048832887167 learning rate =  0.0001\n",
      "Epoch =  91 Batch =  0 Loss =  [9.39131166] Gradient_max =  3.4560328072593527 learning rate =  0.0001\n",
      "Epoch =  92 Batch =  0 Loss =  [9.39062778] Gradient_max =  3.456950111578202 learning rate =  0.0001\n",
      "Epoch =  93 Batch =  0 Loss =  [9.38992146] Gradient_max =  3.457803707378706 learning rate =  0.0001\n",
      "Epoch =  94 Batch =  0 Loss =  [9.38919386] Gradient_max =  3.458596428384876 learning rate =  0.0001\n",
      "Epoch =  95 Batch =  0 Loss =  [9.38844607] Gradient_max =  3.4593309864880974 learning rate =  0.0001\n",
      "Epoch =  96 Batch =  0 Loss =  [9.38767916] Gradient_max =  3.4600099774029474 learning rate =  0.0001\n",
      "Epoch =  97 Batch =  0 Loss =  [9.38689416] Gradient_max =  3.4606358860208792 learning rate =  0.0001\n",
      "Epoch =  98 Batch =  0 Loss =  [9.38609208] Gradient_max =  3.461211091480739 learning rate =  0.0001\n",
      "Epoch =  99 Batch =  0 Loss =  [9.38527388] Gradient_max =  3.461737871973696 learning rate =  0.0001\n",
      "Epoch =  100 Batch =  0 Loss =  [9.38444049] Gradient_max =  3.462218409298918 learning rate =  0.0001\n",
      "Epoch =  101 Batch =  0 Loss =  [9.38359281] Gradient_max =  3.4626547931851555 learning rate =  0.0001\n",
      "Epoch =  102 Batch =  0 Loss =  [9.3827317] Gradient_max =  3.463049025392327 learning rate =  0.0001\n",
      "Epoch =  103 Batch =  0 Loss =  [9.38185801] Gradient_max =  3.4634030236062596 learning rate =  0.0001\n",
      "Epoch =  104 Batch =  0 Loss =  [9.38097254] Gradient_max =  3.4637186251387764 learning rate =  0.0001\n",
      "Epoch =  105 Batch =  0 Loss =  [9.38007606] Gradient_max =  3.463997590444513 learning rate =  0.0001\n",
      "Epoch =  106 Batch =  0 Loss =  [9.37916933] Gradient_max =  3.4642416064650967 learning rate =  0.0001\n",
      "Epoch =  107 Batch =  0 Loss =  [9.37825308] Gradient_max =  3.464452289810542 learning rate =  0.0001\n",
      "Epoch =  108 Batch =  0 Loss =  [9.37732798] Gradient_max =  3.464631189787145 learning rate =  0.0001\n",
      "Epoch =  109 Batch =  0 Loss =  [9.37639472] Gradient_max =  3.4647797912804905 learning rate =  0.0001\n",
      "Epoch =  110 Batch =  0 Loss =  [9.37545394] Gradient_max =  3.464899517501595 learning rate =  0.0001\n",
      "Epoch =  111 Batch =  0 Loss =  [9.37450625] Gradient_max =  3.4649917326038056 learning rate =  0.0001\n",
      "Epoch =  112 Batch =  0 Loss =  [9.37355226] Gradient_max =  3.4650577441774377 learning rate =  0.0001\n",
      "Epoch =  113 Batch =  0 Loss =  [9.37259253] Gradient_max =  3.465098805628791 learning rate =  0.0001\n",
      "Epoch =  114 Batch =  0 Loss =  [9.37162762] Gradient_max =  3.4651161184497346 learning rate =  0.0001\n",
      "Epoch =  115 Batch =  0 Loss =  [9.37065805] Gradient_max =  3.4651108343836587 learning rate =  0.0001\n",
      "Epoch =  116 Batch =  0 Loss =  [9.36968433] Gradient_max =  3.465084057493237 learning rate =  0.0001\n",
      "Epoch =  117 Batch =  0 Loss =  [9.36870695] Gradient_max =  3.465036846135107 learning rate =  0.0001\n",
      "Epoch =  118 Batch =  0 Loss =  [9.36772637] Gradient_max =  3.4649702148462582 learning rate =  0.0001\n",
      "Epoch =  119 Batch =  0 Loss =  [9.36674305] Gradient_max =  3.464885136146685 learning rate =  0.0001\n",
      "Epoch =  120 Batch =  0 Loss =  [9.36575741] Gradient_max =  3.4647825422624656 learning rate =  0.0001\n",
      "Epoch =  121 Batch =  0 Loss =  [9.36476987] Gradient_max =  3.464663326773322 learning rate =  0.0001\n",
      "Epoch =  122 Batch =  0 Loss =  [9.36378083] Gradient_max =  3.4645283461883922 learning rate =  0.0001\n",
      "Epoch =  123 Batch =  0 Loss =  [9.36279065] Gradient_max =  3.4643784214537585 learning rate =  0.0001\n",
      "Epoch =  124 Batch =  0 Loss =  [9.36179971] Gradient_max =  3.464214339395049 learning rate =  0.0001\n",
      "Epoch =  125 Batch =  0 Loss =  [9.36080835] Gradient_max =  3.464036854098286 learning rate =  0.0001\n",
      "Epoch =  126 Batch =  0 Loss =  [9.35981691] Gradient_max =  3.4638466882319165 learning rate =  0.0001\n",
      "Epoch =  127 Batch =  0 Loss =  [9.35882569] Gradient_max =  3.4636445343128597 learning rate =  0.0001\n",
      "Epoch =  128 Batch =  0 Loss =  [9.35783501] Gradient_max =  3.463431055919181 learning rate =  0.0001\n",
      "Epoch =  129 Batch =  0 Loss =  [9.35684516] Gradient_max =  3.4632068888519303 learning rate =  0.0001\n",
      "Epoch =  130 Batch =  0 Loss =  [9.35585641] Gradient_max =  3.4629726422484763 learning rate =  0.0001\n",
      "Epoch =  131 Batch =  0 Loss =  [9.35486903] Gradient_max =  3.4627288996495906 learning rate =  0.0001\n",
      "Epoch =  132 Batch =  0 Loss =  [9.35388328] Gradient_max =  3.462476220022406 learning rate =  0.0001\n",
      "Epoch =  133 Batch =  0 Loss =  [9.35289938] Gradient_max =  3.4622151387412434 learning rate =  0.0001\n",
      "Epoch =  134 Batch =  0 Loss =  [9.35191758] Gradient_max =  3.461946168528199 learning rate =  0.0001\n",
      "Epoch =  135 Batch =  0 Loss =  [9.35093808] Gradient_max =  3.461669800355325 learning rate =  0.0001\n",
      "Epoch =  136 Batch =  0 Loss =  [9.34996111] Gradient_max =  3.4613865043100804 learning rate =  0.0001\n",
      "Epoch =  137 Batch =  0 Loss =  [9.34898686] Gradient_max =  3.4610967304257003 learning rate =  0.0001\n",
      "Epoch =  138 Batch =  0 Loss =  [9.34801552] Gradient_max =  3.4608009094779897 learning rate =  0.0001\n",
      "Epoch =  139 Batch =  0 Loss =  [9.34704727] Gradient_max =  3.4604994537500637 learning rate =  0.0001\n",
      "Epoch =  140 Batch =  0 Loss =  [9.34608227] Gradient_max =  3.460192757766346 learning rate =  0.0001\n",
      "Epoch =  141 Batch =  0 Loss =  [9.3451207] Gradient_max =  3.45988119899722 learning rate =  0.0001\n",
      "Epoch =  142 Batch =  0 Loss =  [9.3441627] Gradient_max =  3.4595651385355564 learning rate =  0.0001\n",
      "Epoch =  143 Batch =  0 Loss =  [9.34320842] Gradient_max =  3.4592449217462953 learning rate =  0.0001\n",
      "Epoch =  144 Batch =  0 Loss =  [9.34225801] Gradient_max =  3.4589208788902757 learning rate =  0.0001\n",
      "Epoch =  145 Batch =  0 Loss =  [9.34131158] Gradient_max =  3.4585933257233417 learning rate =  0.0001\n",
      "Epoch =  146 Batch =  0 Loss =  [9.34036926] Gradient_max =  3.4582625640717977 learning rate =  0.0001\n",
      "Epoch =  147 Batch =  0 Loss =  [9.33943118] Gradient_max =  3.457928882385177 learning rate =  0.0001\n",
      "Epoch =  148 Batch =  0 Loss =  [9.33849744] Gradient_max =  3.457592556267272 learning rate =  0.0001\n",
      "Epoch =  149 Batch =  0 Loss =  [9.33756814] Gradient_max =  3.4572538489863325 learning rate =  0.0001\n",
      "Epoch =  150 Batch =  0 Loss =  [9.33664338] Gradient_max =  3.4569130119652476 learning rate =  0.0001\n",
      "Epoch =  151 Batch =  0 Loss =  [9.33572325] Gradient_max =  3.4565702852525995 learning rate =  0.0001\n",
      "Epoch =  152 Batch =  0 Loss =  [9.33480785] Gradient_max =  3.456225897975287 learning rate =  0.0001\n",
      "Epoch =  153 Batch =  0 Loss =  [9.33389724] Gradient_max =  3.45588006877352 learning rate =  0.0001\n",
      "Epoch =  154 Batch =  0 Loss =  [9.33299151] Gradient_max =  3.455533006218884 learning rate =  0.0001\n",
      "Epoch =  155 Batch =  0 Loss =  [9.33209073] Gradient_max =  3.4551849092161495 learning rate =  0.0001\n",
      "Epoch =  156 Batch =  0 Loss =  [9.33119496] Gradient_max =  3.454835967389463 learning rate =  0.0001\n",
      "Epoch =  157 Batch =  0 Loss =  [9.33030426] Gradient_max =  3.454486361453581 learning rate =  0.0001\n",
      "Epoch =  158 Batch =  0 Loss =  [9.3294187] Gradient_max =  3.4541362635707094 learning rate =  0.0001\n",
      "Epoch =  159 Batch =  0 Loss =  [9.32853831] Gradient_max =  3.4537858376935158 learning rate =  0.0001\n",
      "Epoch =  160 Batch =  0 Loss =  [9.32766316] Gradient_max =  3.453435239894903 learning rate =  0.0001\n",
      "Epoch =  161 Batch =  0 Loss =  [9.32679328] Gradient_max =  3.453084618685002 learning rate =  0.0001\n",
      "Epoch =  162 Batch =  0 Loss =  [9.32592872] Gradient_max =  3.4527341153159554 learning rate =  0.0001\n",
      "Epoch =  163 Batch =  0 Loss =  [9.32506952] Gradient_max =  3.452383864074891 learning rate =  0.0001\n",
      "Epoch =  164 Batch =  0 Loss =  [9.3242157] Gradient_max =  3.4520339925656263 learning rate =  0.0001\n",
      "Epoch =  165 Batch =  0 Loss =  [9.3233673] Gradient_max =  3.451684621979497 learning rate =  0.0001\n",
      "Epoch =  166 Batch =  0 Loss =  [9.32252434] Gradient_max =  3.4513358673557284 learning rate =  0.0001\n",
      "Epoch =  167 Batch =  0 Loss =  [9.32168685] Gradient_max =  3.450987837831786 learning rate =  0.0001\n",
      "Epoch =  168 Batch =  0 Loss =  [9.32085486] Gradient_max =  3.450640636884058 learning rate =  0.0001\n",
      "Epoch =  169 Batch =  0 Loss =  [9.32002837] Gradient_max =  3.4502943625592772 learning rate =  0.0001\n",
      "Epoch =  170 Batch =  0 Loss =  [9.31920741] Gradient_max =  3.449949107697007 learning rate =  0.0001\n",
      "Epoch =  171 Batch =  0 Loss =  [9.31839198] Gradient_max =  3.4496049601435663 learning rate =  0.0001\n",
      "Epoch =  172 Batch =  0 Loss =  [9.31758211] Gradient_max =  3.449262002957688 learning rate =  0.0001\n",
      "Epoch =  173 Batch =  0 Loss =  [9.31677779] Gradient_max =  3.4489203146082676 learning rate =  0.0001\n",
      "Epoch =  174 Batch =  0 Loss =  [9.31597904] Gradient_max =  3.448579969164463 learning rate =  0.0001\n",
      "Epoch =  175 Batch =  0 Loss =  [9.31518586] Gradient_max =  3.4482410364784717 learning rate =  0.0001\n",
      "Epoch =  176 Batch =  0 Loss =  [9.31439825] Gradient_max =  3.4479035823612434 learning rate =  0.0001\n",
      "Epoch =  177 Batch =  0 Loss =  [9.31361621] Gradient_max =  3.447567668751418 learning rate =  0.0001\n",
      "Epoch =  178 Batch =  0 Loss =  [9.31283974] Gradient_max =  3.4472333538777176 learning rate =  0.0001\n",
      "Epoch =  179 Batch =  0 Loss =  [9.31206884] Gradient_max =  3.446900692415071 learning rate =  0.0001\n",
      "Epoch =  180 Batch =  0 Loss =  [9.3113035] Gradient_max =  3.4465697356346983 learning rate =  0.0001\n",
      "Epoch =  181 Batch =  0 Loss =  [9.31054371] Gradient_max =  3.4462405315483684 learning rate =  0.0001\n",
      "Epoch =  182 Batch =  0 Loss =  [9.30978947] Gradient_max =  3.4459131250470816 learning rate =  0.0001\n",
      "Epoch =  183 Batch =  0 Loss =  [9.30904077] Gradient_max =  3.445587558034366 learning rate =  0.0001\n",
      "Epoch =  184 Batch =  0 Loss =  [9.30829759] Gradient_max =  3.445263869554393 learning rate =  0.0001\n",
      "Epoch =  185 Batch =  0 Loss =  [9.30755993] Gradient_max =  3.4449420959151245 learning rate =  0.0001\n",
      "Epoch =  186 Batch =  0 Loss =  [9.30682777] Gradient_max =  3.444622270806661 learning rate =  0.0001\n",
      "Epoch =  187 Batch =  0 Loss =  [9.30610109] Gradient_max =  3.4443044254149755 learning rate =  0.0001\n",
      "Epoch =  188 Batch =  0 Loss =  [9.30537988] Gradient_max =  3.443988588531231 learning rate =  0.0001\n",
      "Epoch =  189 Batch =  0 Loss =  [9.30466413] Gradient_max =  3.4436747866568056 learning rate =  0.0001\n",
      "Epoch =  190 Batch =  0 Loss =  [9.30395381] Gradient_max =  3.443363044104228 learning rate =  0.0001\n",
      "Epoch =  191 Batch =  0 Loss =  [9.3032489] Gradient_max =  3.4430533830941603 learning rate =  0.0001\n",
      "Epoch =  192 Batch =  0 Loss =  [9.30254939] Gradient_max =  3.4427458238485644 learning rate =  0.0001\n",
      "Epoch =  193 Batch =  0 Loss =  [9.30185526] Gradient_max =  3.4424403846802365 learning rate =  0.0001\n",
      "Epoch =  194 Batch =  0 Loss =  [9.30116648] Gradient_max =  3.4421370820787884 learning rate =  0.0001\n",
      "Epoch =  195 Batch =  0 Loss =  [9.30048302] Gradient_max =  3.4418359307932764 learning rate =  0.0001\n",
      "Epoch =  196 Batch =  0 Loss =  [9.29980488] Gradient_max =  3.4415369439115424 learning rate =  0.0001\n",
      "Epoch =  197 Batch =  0 Loss =  [9.29913202] Gradient_max =  3.4412401329364353 learning rate =  0.0001\n",
      "Epoch =  198 Batch =  0 Loss =  [9.29846442] Gradient_max =  3.4409455078590097 learning rate =  0.0001\n",
      "Epoch =  199 Batch =  0 Loss =  [9.29780205] Gradient_max =  3.4406530772288297 learning rate =  0.0001\n",
      "Epoch =  200 Batch =  0 Loss =  [9.29714489] Gradient_max =  3.440362848221461 learning rate =  0.0001\n",
      "Epoch =  201 Batch =  0 Loss =  [9.29649291] Gradient_max =  3.440074826703291 learning rate =  0.0001\n",
      "Epoch =  202 Batch =  0 Loss =  [9.29584609] Gradient_max =  3.4397890172937653 learning rate =  0.0001\n",
      "Epoch =  203 Batch =  0 Loss =  [9.29520439] Gradient_max =  3.4395054234251177 learning rate =  0.0001\n",
      "Epoch =  204 Batch =  0 Loss =  [9.2945678] Gradient_max =  3.4392240473997284 learning rate =  0.0001\n",
      "Epoch =  205 Batch =  0 Loss =  [9.29393628] Gradient_max =  3.4389448904451654 learning rate =  0.0001\n",
      "Epoch =  206 Batch =  0 Loss =  [9.2933098] Gradient_max =  3.4386679527670125 learning rate =  0.0001\n",
      "Epoch =  207 Batch =  0 Loss =  [9.29268833] Gradient_max =  3.4383932335995704 learning rate =  0.0001\n",
      "Epoch =  208 Batch =  0 Loss =  [9.29207185] Gradient_max =  3.4381207312545055 learning rate =  0.0001\n",
      "Epoch =  209 Batch =  0 Loss =  [9.29146033] Gradient_max =  3.437850443167519 learning rate =  0.0001\n",
      "Epoch =  210 Batch =  0 Loss =  [9.29085373] Gradient_max =  3.437582365943135 learning rate =  0.0001\n",
      "Epoch =  211 Batch =  0 Loss =  [9.29025203] Gradient_max =  3.437316495397643 learning rate =  0.0001\n",
      "Epoch =  212 Batch =  0 Loss =  [9.2896552] Gradient_max =  3.437052826600295 learning rate =  0.0001\n",
      "Epoch =  213 Batch =  0 Loss =  [9.28906319] Gradient_max =  3.436791353912824 learning rate =  0.0001\n",
      "Epoch =  214 Batch =  0 Loss =  [9.288476] Gradient_max =  3.4365320710273144 learning rate =  0.0001\n",
      "Epoch =  215 Batch =  0 Loss =  [9.28789358] Gradient_max =  3.4362749710025366 learning rate =  0.0001\n",
      "Epoch =  216 Batch =  0 Loss =  [9.2873159] Gradient_max =  3.4360200462987476 learning rate =  0.0001\n",
      "Epoch =  217 Batch =  0 Loss =  [9.28674293] Gradient_max =  3.435767288811088 learning rate =  0.0001\n",
      "Epoch =  218 Batch =  0 Loss =  [9.28617464] Gradient_max =  3.435516689901546 learning rate =  0.0001\n",
      "Epoch =  219 Batch =  0 Loss =  [9.285611] Gradient_max =  3.435268240429617 learning rate =  0.0001\n",
      "Epoch =  220 Batch =  0 Loss =  [9.28505197] Gradient_max =  3.435021930781668 learning rate =  0.0001\n",
      "Epoch =  221 Batch =  0 Loss =  [9.28449753] Gradient_max =  3.4347777508990522 learning rate =  0.0001\n",
      "Epoch =  222 Batch =  0 Loss =  [9.28394765] Gradient_max =  3.434535690305063 learning rate =  0.0001\n",
      "Epoch =  223 Batch =  0 Loss =  [9.28340228] Gradient_max =  3.4342957381307238 learning rate =  0.0001\n",
      "Epoch =  224 Batch =  0 Loss =  [9.28286141] Gradient_max =  3.4340578831394923 learning rate =  0.0001\n",
      "Epoch =  225 Batch =  0 Loss =  [9.28232499] Gradient_max =  3.4338221137508986 learning rate =  0.0001\n",
      "Epoch =  226 Batch =  0 Loss =  [9.281793] Gradient_max =  3.4335884180631817 learning rate =  0.0001\n",
      "Epoch =  227 Batch =  0 Loss =  [9.28126541] Gradient_max =  3.433356783874935 learning rate =  0.0001\n",
      "Epoch =  228 Batch =  0 Loss =  [9.28074218] Gradient_max =  3.433127198705832 learning rate =  0.0001\n",
      "Epoch =  229 Batch =  0 Loss =  [9.28022328] Gradient_max =  3.4328996498164313 learning rate =  0.0001\n",
      "Epoch =  230 Batch =  0 Loss =  [9.27970867] Gradient_max =  3.4326741242271357 learning rate =  0.0001\n",
      "Epoch =  231 Batch =  0 Loss =  [9.27919834] Gradient_max =  3.4324506087363034 learning rate =  0.0001\n",
      "Epoch =  232 Batch =  0 Loss =  [9.27869224] Gradient_max =  3.432229089937565 learning rate =  0.0001\n",
      "Epoch =  233 Batch =  0 Loss =  [9.27819035] Gradient_max =  3.4320095542363815 learning rate =  0.0001\n",
      "Epoch =  234 Batch =  0 Loss =  [9.27769262] Gradient_max =  3.4317919878658416 learning rate =  0.0001\n",
      "Epoch =  235 Batch =  0 Loss =  [9.27719904] Gradient_max =  3.431576376901779 learning rate =  0.0001\n",
      "Epoch =  236 Batch =  0 Loss =  [9.27670957] Gradient_max =  3.431362707277175 learning rate =  0.0001\n",
      "Epoch =  237 Batch =  0 Loss =  [9.27622418] Gradient_max =  3.4311509647959477 learning rate =  0.0001\n",
      "Epoch =  238 Batch =  0 Loss =  [9.27574283] Gradient_max =  3.4309411351460612 learning rate =  0.0001\n",
      "Epoch =  239 Batch =  0 Loss =  [9.2752655] Gradient_max =  3.4307332039120855 learning rate =  0.0001\n",
      "Epoch =  240 Batch =  0 Loss =  [9.27479216] Gradient_max =  3.4305271565871376 learning rate =  0.0001\n",
      "Epoch =  241 Batch =  0 Loss =  [9.27432277] Gradient_max =  3.430322978584279 learning rate =  0.0001\n",
      "Epoch =  242 Batch =  0 Loss =  [9.2738573] Gradient_max =  3.4301206552473795 learning rate =  0.0001\n",
      "Epoch =  243 Batch =  0 Loss =  [9.27339573] Gradient_max =  3.429920171861472 learning rate =  0.0001\n",
      "Epoch =  244 Batch =  0 Loss =  [9.27293801] Gradient_max =  3.4297215136626065 learning rate =  0.0001\n",
      "Epoch =  245 Batch =  0 Loss =  [9.27248413] Gradient_max =  3.4295246658472265 learning rate =  0.0001\n",
      "Epoch =  246 Batch =  0 Loss =  [9.27203405] Gradient_max =  3.4293296135811127 learning rate =  0.0001\n",
      "Epoch =  247 Batch =  0 Loss =  [9.27158775] Gradient_max =  3.429136342007868 learning rate =  0.0001\n",
      "Epoch =  248 Batch =  0 Loss =  [9.27114518] Gradient_max =  3.428944836256999 learning rate =  0.0001\n",
      "Epoch =  249 Batch =  0 Loss =  [9.27070633] Gradient_max =  3.4287550814515875 learning rate =  0.0001\n",
      "Epoch =  250 Batch =  0 Loss =  [9.27027115] Gradient_max =  3.428567062715589 learning rate =  0.0001\n",
      "Epoch =  251 Batch =  0 Loss =  [9.26983963] Gradient_max =  3.428380765180745 learning rate =  0.0001\n",
      "Epoch =  252 Batch =  0 Loss =  [9.26941173] Gradient_max =  3.4281961739931543 learning rate =  0.0001\n",
      "Epoch =  253 Batch =  0 Loss =  [9.26898743] Gradient_max =  3.4280132743195106 learning rate =  0.0001\n",
      "Epoch =  254 Batch =  0 Loss =  [9.26856669] Gradient_max =  3.4278320513529867 learning rate =  0.0001\n",
      "Epoch =  255 Batch =  0 Loss =  [9.26814948] Gradient_max =  3.4276524903188417 learning rate =  0.0001\n",
      "Epoch =  256 Batch =  0 Loss =  [9.26773579] Gradient_max =  3.4274745764797006 learning rate =  0.0001\n",
      "Epoch =  257 Batch =  0 Loss =  [9.26732557] Gradient_max =  3.4272982951405737 learning rate =  0.0001\n",
      "Epoch =  258 Batch =  0 Loss =  [9.2669188] Gradient_max =  3.4271236316535676 learning rate =  0.0001\n",
      "Epoch =  259 Batch =  0 Loss =  [9.26651545] Gradient_max =  3.426950571422361 learning rate =  0.0001\n",
      "Epoch =  260 Batch =  0 Loss =  [9.2661155] Gradient_max =  3.426779099906427 learning rate =  0.0001\n",
      "Epoch =  261 Batch =  0 Loss =  [9.26571891] Gradient_max =  3.4266092026249813 learning rate =  0.0001\n",
      "Epoch =  262 Batch =  0 Loss =  [9.26532566] Gradient_max =  3.426440865160745 learning rate =  0.0001\n",
      "Epoch =  263 Batch =  0 Loss =  [9.26493572] Gradient_max =  3.426274073163459 learning rate =  0.0001\n",
      "Epoch =  264 Batch =  0 Loss =  [9.26454907] Gradient_max =  3.4261088123531778 learning rate =  0.0001\n",
      "Epoch =  265 Batch =  0 Loss =  [9.26416567] Gradient_max =  3.4259450685233843 learning rate =  0.0001\n",
      "Epoch =  266 Batch =  0 Loss =  [9.2637855] Gradient_max =  3.4257828275438933 learning rate =  0.0001\n",
      "Epoch =  267 Batch =  0 Loss =  [9.26340853] Gradient_max =  3.4256220753635755 learning rate =  0.0001\n",
      "Epoch =  268 Batch =  0 Loss =  [9.26303474] Gradient_max =  3.425462798012906 learning rate =  0.0001\n",
      "Epoch =  269 Batch =  0 Loss =  [9.26266409] Gradient_max =  3.4253049816063195 learning rate =  0.0001\n",
      "Epoch =  270 Batch =  0 Loss =  [9.26229657] Gradient_max =  3.425148612344444 learning rate =  0.0001\n",
      "Epoch =  271 Batch =  0 Loss =  [9.26193215] Gradient_max =  3.4249936765161406 learning rate =  0.0001\n",
      "Epoch =  272 Batch =  0 Loss =  [9.2615708] Gradient_max =  3.4248401605004144 learning rate =  0.0001\n",
      "Epoch =  273 Batch =  0 Loss =  [9.26121249] Gradient_max =  3.4246880507681756 learning rate =  0.0001\n",
      "Epoch =  274 Batch =  0 Loss =  [9.2608572] Gradient_max =  3.424537333883871 learning rate =  0.0001\n",
      "Epoch =  275 Batch =  0 Loss =  [9.26050491] Gradient_max =  3.424387996506971 learning rate =  0.0001\n",
      "Epoch =  276 Batch =  0 Loss =  [9.26015559] Gradient_max =  3.4242400253933525 learning rate =  0.0001\n",
      "Epoch =  277 Batch =  0 Loss =  [9.25980921] Gradient_max =  3.424093407396551 learning rate =  0.0001\n",
      "Epoch =  278 Batch =  0 Loss =  [9.25946575] Gradient_max =  3.423948129468899 learning rate =  0.0001\n",
      "Epoch =  279 Batch =  0 Loss =  [9.25912519] Gradient_max =  3.423804178662563 learning rate =  0.0001\n",
      "Epoch =  280 Batch =  0 Loss =  [9.25878749] Gradient_max =  3.4236615421304686 learning rate =  0.0001\n",
      "Epoch =  281 Batch =  0 Loss =  [9.25845265] Gradient_max =  3.423520207127141 learning rate =  0.0001\n",
      "Epoch =  282 Batch =  0 Loss =  [9.25812062] Gradient_max =  3.4233801610094323 learning rate =  0.0001\n",
      "Epoch =  283 Batch =  0 Loss =  [9.25779139] Gradient_max =  3.4232413912371826 learning rate =  0.0001\n",
      "Epoch =  284 Batch =  0 Loss =  [9.25746494] Gradient_max =  3.423103885373774 learning rate =  0.0001\n",
      "Epoch =  285 Batch =  0 Loss =  [9.25714124] Gradient_max =  3.4229676310866255 learning rate =  0.0001\n",
      "Epoch =  286 Batch =  0 Loss =  [9.25682027] Gradient_max =  3.422832616147581 learning rate =  0.0001\n",
      "Epoch =  287 Batch =  0 Loss =  [9.256502] Gradient_max =  3.422698828433265 learning rate =  0.0001\n",
      "Epoch =  288 Batch =  0 Loss =  [9.25618641] Gradient_max =  3.422566255925327 learning rate =  0.0001\n",
      "Epoch =  289 Batch =  0 Loss =  [9.25587349] Gradient_max =  3.4224348867106493 learning rate =  0.0001\n",
      "Epoch =  290 Batch =  0 Loss =  [9.2555632] Gradient_max =  3.4223047089814753 learning rate =  0.0001\n",
      "Epoch =  291 Batch =  0 Loss =  [9.25525552] Gradient_max =  3.4221757110354893 learning rate =  0.0001\n",
      "Epoch =  292 Batch =  0 Loss =  [9.25495044] Gradient_max =  3.422047881275823 learning rate =  0.0001\n",
      "Epoch =  293 Batch =  0 Loss =  [9.25464792] Gradient_max =  3.4219212082110313 learning rate =  0.0001\n",
      "Epoch =  294 Batch =  0 Loss =  [9.25434795] Gradient_max =  3.4217956804550003 learning rate =  0.0001\n",
      "Epoch =  295 Batch =  0 Loss =  [9.25405051] Gradient_max =  3.4216712867268027 learning rate =  0.0001\n",
      "Epoch =  296 Batch =  0 Loss =  [9.25375557] Gradient_max =  3.421548015850529 learning rate =  0.0001\n",
      "Epoch =  297 Batch =  0 Loss =  [9.25346312] Gradient_max =  3.4214258567550555 learning rate =  0.0001\n",
      "Epoch =  298 Batch =  0 Loss =  [9.25317313] Gradient_max =  3.421304798473771 learning rate =  0.0001\n",
      "Epoch =  299 Batch =  0 Loss =  [9.25288558] Gradient_max =  3.421184830144287 learning rate =  0.0001\n",
      "Epoch =  300 Batch =  0 Loss =  [9.25260045] Gradient_max =  3.4210659410080786 learning rate =  0.0001\n",
      "Epoch =  301 Batch =  0 Loss =  [9.25231772] Gradient_max =  3.4209481204101233 learning rate =  0.0001\n",
      "Epoch =  302 Batch =  0 Loss =  [9.25203736] Gradient_max =  3.4208313577984786 learning rate =  0.0001\n",
      "Epoch =  303 Batch =  0 Loss =  [9.25175937] Gradient_max =  3.4207156427238528 learning rate =  0.0001\n",
      "Epoch =  304 Batch =  0 Loss =  [9.25148371] Gradient_max =  3.4206009648391236 learning rate =  0.0001\n",
      "Epoch =  305 Batch =  0 Loss =  [9.25121038] Gradient_max =  3.420487313898858 learning rate =  0.0001\n",
      "Epoch =  306 Batch =  0 Loss =  [9.25093934] Gradient_max =  3.420374679758776 learning rate =  0.0001\n",
      "Epoch =  307 Batch =  0 Loss =  [9.25067058] Gradient_max =  3.4202630523752124 learning rate =  0.0001\n",
      "Epoch =  308 Batch =  0 Loss =  [9.25040408] Gradient_max =  3.420152421804554 learning rate =  0.0001\n",
      "Epoch =  309 Batch =  0 Loss =  [9.25013982] Gradient_max =  3.4200427782026335 learning rate =  0.0001\n",
      "Epoch =  310 Batch =  0 Loss =  [9.24987778] Gradient_max =  3.4199341118241433 learning rate =  0.0001\n",
      "Epoch =  311 Batch =  0 Loss =  [9.24961794] Gradient_max =  3.419826413021994 learning rate =  0.0001\n",
      "Epoch =  312 Batch =  0 Loss =  [9.24936028] Gradient_max =  3.419719672246674 learning rate =  0.0001\n",
      "Epoch =  313 Batch =  0 Loss =  [9.24910479] Gradient_max =  3.4196138800455955 learning rate =  0.0001\n",
      "Epoch =  314 Batch =  0 Loss =  [9.24885145] Gradient_max =  3.4195090270624147 learning rate =  0.0001\n",
      "Epoch =  315 Batch =  0 Loss =  [9.24860023] Gradient_max =  3.419405104036349 learning rate =  0.0001\n",
      "Epoch =  316 Batch =  0 Loss =  [9.24835112] Gradient_max =  3.419302101801481 learning rate =  0.0001\n",
      "Epoch =  317 Batch =  0 Loss =  [9.2481041] Gradient_max =  3.419200011286031 learning rate =  0.0001\n",
      "Epoch =  318 Batch =  0 Loss =  [9.24785916] Gradient_max =  3.4190988235116593 learning rate =  0.0001\n",
      "Epoch =  319 Batch =  0 Loss =  [9.24761626] Gradient_max =  3.418998529592719 learning rate =  0.0001\n",
      "Epoch =  320 Batch =  0 Loss =  [9.24737541] Gradient_max =  3.4188991207355293 learning rate =  0.0001\n",
      "Epoch =  321 Batch =  0 Loss =  [9.24713658] Gradient_max =  3.418800588237611 learning rate =  0.0001\n",
      "Epoch =  322 Batch =  0 Loss =  [9.24689975] Gradient_max =  3.4187029234869524 learning rate =  0.0001\n",
      "Epoch =  323 Batch =  0 Loss =  [9.2466649] Gradient_max =  3.4186061179612413 learning rate =  0.0001\n",
      "Epoch =  324 Batch =  0 Loss =  [9.24643202] Gradient_max =  3.418510163227093 learning rate =  0.0001\n",
      "Epoch =  325 Batch =  0 Loss =  [9.2462011] Gradient_max =  3.418415050939287 learning rate =  0.0001\n",
      "Epoch =  326 Batch =  0 Loss =  [9.24597211] Gradient_max =  3.4183207728400014 learning rate =  0.0001\n",
      "Epoch =  327 Batch =  0 Loss =  [9.24574504] Gradient_max =  3.4182273207580143 learning rate =  0.0001\n",
      "Epoch =  328 Batch =  0 Loss =  [9.24551986] Gradient_max =  3.418134686607941 learning rate =  0.0001\n",
      "Epoch =  329 Batch =  0 Loss =  [9.24529658] Gradient_max =  3.4180428623894485 learning rate =  0.0001\n",
      "Epoch =  330 Batch =  0 Loss =  [9.24507516] Gradient_max =  3.4179518401864564 learning rate =  0.0001\n",
      "Epoch =  331 Batch =  0 Loss =  [9.2448556] Gradient_max =  3.417861612166373 learning rate =  0.0001\n",
      "Epoch =  332 Batch =  0 Loss =  [9.24463787] Gradient_max =  3.417772170579298 learning rate =  0.0001\n",
      "Epoch =  333 Batch =  0 Loss =  [9.24442197] Gradient_max =  3.417683507757227 learning rate =  0.0001\n",
      "Epoch =  334 Batch =  0 Loss =  [9.24420787] Gradient_max =  3.4175956161132817 learning rate =  0.0001\n",
      "Epoch =  335 Batch =  0 Loss =  [9.24399556] Gradient_max =  3.417508488140909 learning rate =  0.0001\n",
      "Epoch =  336 Batch =  0 Loss =  [9.24378502] Gradient_max =  3.4174221164131047 learning rate =  0.0001\n",
      "Epoch =  337 Batch =  0 Loss =  [9.24357625] Gradient_max =  3.4173364935816313 learning rate =  0.0001\n",
      "Epoch =  338 Batch =  0 Loss =  [9.24336922] Gradient_max =  3.4172516123762233 learning rate =  0.0001\n",
      "Epoch =  339 Batch =  0 Loss =  [9.24316392] Gradient_max =  3.4171674656038173 learning rate =  0.0001\n",
      "Epoch =  340 Batch =  0 Loss =  [9.24296033] Gradient_max =  3.4170840461477767 learning rate =  0.0001\n",
      "Epoch =  341 Batch =  0 Loss =  [9.24275844] Gradient_max =  3.4170013469671017 learning rate =  0.0001\n",
      "Epoch =  342 Batch =  0 Loss =  [9.24255824] Gradient_max =  3.4169193610956747 learning rate =  0.0001\n",
      "Epoch =  343 Batch =  0 Loss =  [9.24235971] Gradient_max =  3.4168380816414774 learning rate =  0.0001\n",
      "Epoch =  344 Batch =  0 Loss =  [9.24216283] Gradient_max =  3.4167575017858254 learning rate =  0.0001\n",
      "Epoch =  345 Batch =  0 Loss =  [9.2419676] Gradient_max =  3.4166776147826217 learning rate =  0.0001\n",
      "Epoch =  346 Batch =  0 Loss =  [9.241774] Gradient_max =  3.4165984139575745 learning rate =  0.0001\n",
      "Epoch =  347 Batch =  0 Loss =  [9.24158201] Gradient_max =  3.4165198927074614 learning rate =  0.0001\n",
      "Epoch =  348 Batch =  0 Loss =  [9.24139162] Gradient_max =  3.4164420444993677 learning rate =  0.0001\n",
      "Epoch =  349 Batch =  0 Loss =  [9.24120282] Gradient_max =  3.4163648628699503 learning rate =  0.0001\n",
      "Epoch =  350 Batch =  0 Loss =  [9.24101559] Gradient_max =  3.4162883414246874 learning rate =  0.0001\n",
      "Epoch =  351 Batch =  0 Loss =  [9.24082992] Gradient_max =  3.4162124738371435 learning rate =  0.0001\n",
      "Epoch =  352 Batch =  0 Loss =  [9.24064579] Gradient_max =  3.416137253848247 learning rate =  0.0001\n",
      "Epoch =  353 Batch =  0 Loss =  [9.2404632] Gradient_max =  3.4160626752655485 learning rate =  0.0001\n",
      "Epoch =  354 Batch =  0 Loss =  [9.24028213] Gradient_max =  3.415988731962512 learning rate =  0.0001\n",
      "Epoch =  355 Batch =  0 Loss =  [9.24010256] Gradient_max =  3.4159154178777933 learning rate =  0.0001\n",
      "Epoch =  356 Batch =  0 Loss =  [9.23992449] Gradient_max =  3.415842727014527 learning rate =  0.0001\n",
      "Epoch =  357 Batch =  0 Loss =  [9.2397479] Gradient_max =  3.41577065343963 learning rate =  0.0001\n",
      "Epoch =  358 Batch =  0 Loss =  [9.23957278] Gradient_max =  3.4156991912830916 learning rate =  0.0001\n",
      "Epoch =  359 Batch =  0 Loss =  [9.23939911] Gradient_max =  3.415628334737279 learning rate =  0.0001\n",
      "Epoch =  360 Batch =  0 Loss =  [9.23922689] Gradient_max =  3.415558078056258 learning rate =  0.0001\n",
      "Epoch =  361 Batch =  0 Loss =  [9.23905609] Gradient_max =  3.415488415555107 learning rate =  0.0001\n",
      "Epoch =  362 Batch =  0 Loss =  [9.23888672] Gradient_max =  3.415419341609236 learning rate =  0.0001\n",
      "Epoch =  363 Batch =  0 Loss =  [9.23871875] Gradient_max =  3.415350850653714 learning rate =  0.0001\n",
      "Epoch =  364 Batch =  0 Loss =  [9.23855218] Gradient_max =  3.415282937182613 learning rate =  0.0001\n",
      "Epoch =  365 Batch =  0 Loss =  [9.23838698] Gradient_max =  3.4152155957483457 learning rate =  0.0001\n",
      "Epoch =  366 Batch =  0 Loss =  [9.23822316] Gradient_max =  3.4151488209610052 learning rate =  0.0001\n",
      "Epoch =  367 Batch =  0 Loss =  [9.2380607] Gradient_max =  3.415082607487726 learning rate =  0.0001\n",
      "Epoch =  368 Batch =  0 Loss =  [9.23789958] Gradient_max =  3.415016950052041 learning rate =  0.0001\n",
      "Epoch =  369 Batch =  0 Loss =  [9.2377398] Gradient_max =  3.4149518434332466 learning rate =  0.0001\n",
      "Epoch =  370 Batch =  0 Loss =  [9.23758135] Gradient_max =  3.4148872824657728 learning rate =  0.0001\n",
      "Epoch =  371 Batch =  0 Loss =  [9.23742421] Gradient_max =  3.4148232620385586 learning rate =  0.0001\n",
      "Epoch =  372 Batch =  0 Loss =  [9.23726836] Gradient_max =  3.4147597770944462 learning rate =  0.0001\n",
      "Epoch =  373 Batch =  0 Loss =  [9.23711381] Gradient_max =  3.4146968226295566 learning rate =  0.0001\n",
      "Epoch =  374 Batch =  0 Loss =  [9.23696054] Gradient_max =  3.414634393692696 learning rate =  0.0001\n",
      "Epoch =  375 Batch =  0 Loss =  [9.23680854] Gradient_max =  3.414572485384751 learning rate =  0.0001\n",
      "Epoch =  376 Batch =  0 Loss =  [9.23665779] Gradient_max =  3.414511092858098 learning rate =  0.0001\n",
      "Epoch =  377 Batch =  0 Loss =  [9.2365083] Gradient_max =  3.4144502113160247 learning rate =  0.0001\n",
      "Epoch =  378 Batch =  0 Loss =  [9.23636003] Gradient_max =  3.4143898360121416 learning rate =  0.0001\n",
      "Epoch =  379 Batch =  0 Loss =  [9.236213] Gradient_max =  3.4143299622498082 learning rate =  0.0001\n",
      "Epoch =  380 Batch =  0 Loss =  [9.23606717] Gradient_max =  3.414270585381577 learning rate =  0.0001\n",
      "Epoch =  381 Batch =  0 Loss =  [9.23592256] Gradient_max =  3.4142117008086155 learning rate =  0.0001\n",
      "Epoch =  382 Batch =  0 Loss =  [9.23577914] Gradient_max =  3.414153303980167 learning rate =  0.0001\n",
      "Epoch =  383 Batch =  0 Loss =  [9.2356369] Gradient_max =  3.4140953903929905 learning rate =  0.0001\n",
      "Epoch =  384 Batch =  0 Loss =  [9.23549583] Gradient_max =  3.41403795559082 learning rate =  0.0001\n",
      "Epoch =  385 Batch =  0 Loss =  [9.23535593] Gradient_max =  3.413980995163828 learning rate =  0.0001\n",
      "Epoch =  386 Batch =  0 Loss =  [9.23521719] Gradient_max =  3.413924504748094 learning rate =  0.0001\n",
      "Epoch =  387 Batch =  0 Loss =  [9.23507959] Gradient_max =  3.4138684800250823 learning rate =  0.0001\n",
      "Epoch =  388 Batch =  0 Loss =  [9.23494312] Gradient_max =  3.4138129167211138 learning rate =  0.0001\n",
      "Epoch =  389 Batch =  0 Loss =  [9.23480778] Gradient_max =  3.4137578106068607 learning rate =  0.0001\n",
      "Epoch =  390 Batch =  0 Loss =  [9.23467355] Gradient_max =  3.4137031574968284 learning rate =  0.0001\n",
      "Epoch =  391 Batch =  0 Loss =  [9.23454043] Gradient_max =  3.4136489532488725 learning rate =  0.0001\n",
      "Epoch =  392 Batch =  0 Loss =  [9.23440841] Gradient_max =  3.413595193763674 learning rate =  0.0001\n",
      "Epoch =  393 Batch =  0 Loss =  [9.23427747] Gradient_max =  3.4135418749842805 learning rate =  0.0001\n",
      "Epoch =  394 Batch =  0 Loss =  [9.23414761] Gradient_max =  3.413488992895586 learning rate =  0.0001\n",
      "Epoch =  395 Batch =  0 Loss =  [9.23401882] Gradient_max =  3.41343654352388 learning rate =  0.0001\n",
      "Epoch =  396 Batch =  0 Loss =  [9.23389109] Gradient_max =  3.413384522936366 learning rate =  0.0001\n",
      "Epoch =  397 Batch =  0 Loss =  [9.23376441] Gradient_max =  3.4133329272406785 learning rate =  0.0001\n",
      "Epoch =  398 Batch =  0 Loss =  [9.23363877] Gradient_max =  3.4132817525844406 learning rate =  0.0001\n",
      "Epoch =  399 Batch =  0 Loss =  [9.23351416] Gradient_max =  3.413230995154789 learning rate =  0.0001\n",
      "Epoch =  400 Batch =  0 Loss =  [9.23339058] Gradient_max =  3.4131806511779335 learning rate =  0.0001\n",
      "Epoch =  401 Batch =  0 Loss =  [9.23326801] Gradient_max =  3.4131307169187037 learning rate =  0.0001\n",
      "Epoch =  402 Batch =  0 Loss =  [9.23314645] Gradient_max =  3.4130811886801085 learning rate =  0.0001\n",
      "Epoch =  403 Batch =  0 Loss =  [9.23302588] Gradient_max =  3.4130320628028983 learning rate =  0.0001\n",
      "Epoch =  404 Batch =  0 Loss =  [9.23290631] Gradient_max =  3.4129833356651367 learning rate =  0.0001\n",
      "Epoch =  405 Batch =  0 Loss =  [9.23278772] Gradient_max =  3.4129350036817696 learning rate =  0.0001\n",
      "Epoch =  406 Batch =  0 Loss =  [9.2326701] Gradient_max =  3.4128870633042094 learning rate =  0.0001\n",
      "Epoch =  407 Batch =  0 Loss =  [9.23255344] Gradient_max =  3.412839511019917 learning rate =  0.0001\n",
      "Epoch =  408 Batch =  0 Loss =  [9.23243774] Gradient_max =  3.412792343351986 learning rate =  0.0001\n",
      "Epoch =  409 Batch =  0 Loss =  [9.23232299] Gradient_max =  3.412745556858739 learning rate =  0.0001\n",
      "Epoch =  410 Batch =  0 Loss =  [9.23220918] Gradient_max =  3.41269914813333 learning rate =  0.0001\n",
      "Epoch =  411 Batch =  0 Loss =  [9.2320963] Gradient_max =  3.4126531138033465 learning rate =  0.0001\n",
      "Epoch =  412 Batch =  0 Loss =  [9.23198434] Gradient_max =  3.412607450530409 learning rate =  0.0001\n",
      "Epoch =  413 Batch =  0 Loss =  [9.2318733] Gradient_max =  3.4125621550097933 learning rate =  0.0001\n",
      "Epoch =  414 Batch =  0 Loss =  [9.23176317] Gradient_max =  3.41251722397005 learning rate =  0.0001\n",
      "Epoch =  415 Batch =  0 Loss =  [9.23165394] Gradient_max =  3.4124726541726162 learning rate =  0.0001\n",
      "Epoch =  416 Batch =  0 Loss =  [9.2315456] Gradient_max =  3.412428442411449 learning rate =  0.0001\n",
      "Epoch =  417 Batch =  0 Loss =  [9.23143815] Gradient_max =  3.412384585512661 learning rate =  0.0001\n",
      "Epoch =  418 Batch =  0 Loss =  [9.23133158] Gradient_max =  3.412341080334143 learning rate =  0.0001\n",
      "Epoch =  419 Batch =  0 Loss =  [9.23122588] Gradient_max =  3.4122979237652187 learning rate =  0.0001\n",
      "Epoch =  420 Batch =  0 Loss =  [9.23112104] Gradient_max =  3.4122551127262786 learning rate =  0.0001\n",
      "Epoch =  421 Batch =  0 Loss =  [9.23101705] Gradient_max =  3.4122126441684326 learning rate =  0.0001\n",
      "Epoch =  422 Batch =  0 Loss =  [9.23091392] Gradient_max =  3.4121705150731674 learning rate =  0.0001\n",
      "Epoch =  423 Batch =  0 Loss =  [9.23081162] Gradient_max =  3.4121287224519956 learning rate =  0.0001\n",
      "Epoch =  424 Batch =  0 Loss =  [9.23071017] Gradient_max =  3.4120872633461246 learning rate =  0.0001\n",
      "Epoch =  425 Batch =  0 Loss =  [9.23060953] Gradient_max =  3.412046134826121 learning rate =  0.0001\n",
      "Epoch =  426 Batch =  0 Loss =  [9.23050972] Gradient_max =  3.412005333991572 learning rate =  0.0001\n",
      "Epoch =  427 Batch =  0 Loss =  [9.23041072] Gradient_max =  3.4119648579707738 learning rate =  0.0001\n",
      "Epoch =  428 Batch =  0 Loss =  [9.23031253] Gradient_max =  3.4119247039203975 learning rate =  0.0001\n",
      "Epoch =  429 Batch =  0 Loss =  [9.23021514] Gradient_max =  3.411884869025175 learning rate =  0.0001\n",
      "Epoch =  430 Batch =  0 Loss =  [9.23011854] Gradient_max =  3.4118453504975874 learning rate =  0.0001\n",
      "Epoch =  431 Batch =  0 Loss =  [9.23002273] Gradient_max =  3.4118061455775517 learning rate =  0.0001\n",
      "Epoch =  432 Batch =  0 Loss =  [9.2299277] Gradient_max =  3.411767251532109 learning rate =  0.0001\n",
      "Epoch =  433 Batch =  0 Loss =  [9.22983344] Gradient_max =  3.4117286656551364 learning rate =  0.0001\n",
      "Epoch =  434 Batch =  0 Loss =  [9.22973994] Gradient_max =  3.4116903852670335 learning rate =  0.0001\n",
      "Epoch =  435 Batch =  0 Loss =  [9.22964721] Gradient_max =  3.411652407714431 learning rate =  0.0001\n",
      "Epoch =  436 Batch =  0 Loss =  [9.22955523] Gradient_max =  3.4116147303699007 learning rate =  0.0001\n",
      "Epoch =  437 Batch =  0 Loss =  [9.22946399] Gradient_max =  3.411577350631668 learning rate =  0.0001\n",
      "Epoch =  438 Batch =  0 Loss =  [9.2293735] Gradient_max =  3.4115402659233194 learning rate =  0.0001\n",
      "Epoch =  439 Batch =  0 Loss =  [9.22928374] Gradient_max =  3.4115034736935397 learning rate =  0.0001\n",
      "Epoch =  440 Batch =  0 Loss =  [9.22919471] Gradient_max =  3.411466971415811 learning rate =  0.0001\n",
      "Epoch =  441 Batch =  0 Loss =  [9.22910641] Gradient_max =  3.4114307565881528 learning rate =  0.0001\n",
      "Epoch =  442 Batch =  0 Loss =  [9.22901882] Gradient_max =  3.411394826732846 learning rate =  0.0001\n",
      "Epoch =  443 Batch =  0 Loss =  [9.22893193] Gradient_max =  3.4113591793961717 learning rate =  0.0001\n",
      "Epoch =  444 Batch =  0 Loss =  [9.22884576] Gradient_max =  3.411323812148136 learning rate =  0.0001\n",
      "Epoch =  445 Batch =  0 Loss =  [9.22876028] Gradient_max =  3.4112887225822157 learning rate =  0.0001\n",
      "Epoch =  446 Batch =  0 Loss =  [9.22867549] Gradient_max =  3.411253908315102 learning rate =  0.0001\n",
      "Epoch =  447 Batch =  0 Loss =  [9.22859139] Gradient_max =  3.4112193669864386 learning rate =  0.0001\n",
      "Epoch =  448 Batch =  0 Loss =  [9.22850797] Gradient_max =  3.4111850962585772 learning rate =  0.0001\n",
      "Epoch =  449 Batch =  0 Loss =  [9.22842523] Gradient_max =  3.4111510938163243 learning rate =  0.0001\n",
      "Epoch =  450 Batch =  0 Loss =  [9.22834316] Gradient_max =  3.411117357366703 learning rate =  0.0001\n",
      "Epoch =  451 Batch =  0 Loss =  [9.22826175] Gradient_max =  3.411083884638697 learning rate =  0.0001\n",
      "Epoch =  452 Batch =  0 Loss =  [9.22818099] Gradient_max =  3.4110506733830195 learning rate =  0.0001\n",
      "Epoch =  453 Batch =  0 Loss =  [9.22810089] Gradient_max =  3.411017721371886 learning rate =  0.0001\n",
      "Epoch =  454 Batch =  0 Loss =  [9.22802144] Gradient_max =  3.4109850263987544 learning rate =  0.0001\n",
      "Epoch =  455 Batch =  0 Loss =  [9.22794263] Gradient_max =  3.4109525862781225 learning rate =  0.0001\n",
      "Epoch =  456 Batch =  0 Loss =  [9.22786445] Gradient_max =  3.4109203988452834 learning rate =  0.0001\n",
      "Epoch =  457 Batch =  0 Loss =  [9.22778691] Gradient_max =  3.410888461956102 learning rate =  0.0001\n",
      "Epoch =  458 Batch =  0 Loss =  [9.22770999] Gradient_max =  3.4108567734868 learning rate =  0.0001\n",
      "Epoch =  459 Batch =  0 Loss =  [9.22763369] Gradient_max =  3.4108253313337262 learning rate =  0.0001\n",
      "Epoch =  460 Batch =  0 Loss =  [9.22755801] Gradient_max =  3.410794133413145 learning rate =  0.0001\n",
      "Epoch =  461 Batch =  0 Loss =  [9.22748294] Gradient_max =  3.410763177661022 learning rate =  0.0001\n",
      "Epoch =  462 Batch =  0 Loss =  [9.22740847] Gradient_max =  3.410732462032803 learning rate =  0.0001\n",
      "Epoch =  463 Batch =  0 Loss =  [9.2273346] Gradient_max =  3.410701984503226 learning rate =  0.0001\n",
      "Epoch =  464 Batch =  0 Loss =  [9.22726133] Gradient_max =  3.41067174306609 learning rate =  0.0001\n",
      "Epoch =  465 Batch =  0 Loss =  [9.22718864] Gradient_max =  3.410641735734064 learning rate =  0.0001\n",
      "Epoch =  466 Batch =  0 Loss =  [9.22711654] Gradient_max =  3.4106119605384846 learning rate =  0.0001\n",
      "Epoch =  467 Batch =  0 Loss =  [9.22704502] Gradient_max =  3.4105824155291526 learning rate =  0.0001\n",
      "Epoch =  468 Batch =  0 Loss =  [9.22697408] Gradient_max =  3.410553098774137 learning rate =  0.0001\n",
      "Epoch =  469 Batch =  0 Loss =  [9.22690371] Gradient_max =  3.4105240083595825 learning rate =  0.0001\n",
      "Epoch =  470 Batch =  0 Loss =  [9.2268339] Gradient_max =  3.4104951423895162 learning rate =  0.0001\n",
      "Epoch =  471 Batch =  0 Loss =  [9.22676465] Gradient_max =  3.410466498985657 learning rate =  0.0001\n",
      "Epoch =  472 Batch =  0 Loss =  [9.22669596] Gradient_max =  3.4104380762872255 learning rate =  0.0001\n",
      "Epoch =  473 Batch =  0 Loss =  [9.22662782] Gradient_max =  3.410409872450766 learning rate =  0.0001\n",
      "Epoch =  474 Batch =  0 Loss =  [9.22656023] Gradient_max =  3.410381885649956 learning rate =  0.0001\n",
      "Epoch =  475 Batch =  0 Loss =  [9.22649318] Gradient_max =  3.4103541140754285 learning rate =  0.0001\n",
      "Epoch =  476 Batch =  0 Loss =  [9.22642666] Gradient_max =  3.410326555934591 learning rate =  0.0001\n",
      "Epoch =  477 Batch =  0 Loss =  [9.22636068] Gradient_max =  3.4102992094514524 learning rate =  0.0001\n",
      "Epoch =  478 Batch =  0 Loss =  [9.22629523] Gradient_max =  3.4102720728664435 learning rate =  0.0001\n",
      "Epoch =  479 Batch =  0 Loss =  [9.22623031] Gradient_max =  3.41024514443625 learning rate =  0.0001\n",
      "Epoch =  480 Batch =  0 Loss =  [9.2261659] Gradient_max =  3.4102184224336374 learning rate =  0.0001\n",
      "Epoch =  481 Batch =  0 Loss =  [9.22610201] Gradient_max =  3.410191905147288 learning rate =  0.0001\n",
      "Epoch =  482 Batch =  0 Loss =  [9.22603863] Gradient_max =  3.4101655908816237 learning rate =  0.0001\n",
      "Epoch =  483 Batch =  0 Loss =  [9.22597576] Gradient_max =  3.410139477956658 learning rate =  0.0001\n",
      "Epoch =  484 Batch =  0 Loss =  [9.22591339] Gradient_max =  3.410113564707816 learning rate =  0.0001\n",
      "Epoch =  485 Batch =  0 Loss =  [9.22585151] Gradient_max =  3.410087849485789 learning rate =  0.0001\n",
      "Epoch =  486 Batch =  0 Loss =  [9.22579014] Gradient_max =  3.410062330656368 learning rate =  0.0001\n",
      "Epoch =  487 Batch =  0 Loss =  [9.22572925] Gradient_max =  3.410037006600282 learning rate =  0.0001\n",
      "Epoch =  488 Batch =  0 Loss =  [9.22566885] Gradient_max =  3.410011875713056 learning rate =  0.0001\n",
      "Epoch =  489 Batch =  0 Loss =  [9.22560893] Gradient_max =  3.4099869364048483 learning rate =  0.0001\n",
      "Epoch =  490 Batch =  0 Loss =  [9.22554949] Gradient_max =  3.4099621871003034 learning rate =  0.0001\n",
      "Epoch =  491 Batch =  0 Loss =  [9.22549053] Gradient_max =  3.409937626238397 learning rate =  0.0001\n",
      "Epoch =  492 Batch =  0 Loss =  [9.22543203] Gradient_max =  3.409913252272297 learning rate =  0.0001\n",
      "Epoch =  493 Batch =  0 Loss =  [9.225374] Gradient_max =  3.409889063669211 learning rate =  0.0001\n",
      "Epoch =  494 Batch =  0 Loss =  [9.22531644] Gradient_max =  3.4098650589102437 learning rate =  0.0001\n",
      "Epoch =  495 Batch =  0 Loss =  [9.22525933] Gradient_max =  3.4098412364902555 learning rate =  0.0001\n",
      "Epoch =  496 Batch =  0 Loss =  [9.22520268] Gradient_max =  3.409817594917719 learning rate =  0.0001\n",
      "Epoch =  497 Batch =  0 Loss =  [9.22514648] Gradient_max =  3.40979413271458 learning rate =  0.0001\n",
      "Epoch =  498 Batch =  0 Loss =  [9.22509072] Gradient_max =  3.4097708484161267 learning rate =  0.0001\n",
      "Epoch =  499 Batch =  0 Loss =  [9.22503541] Gradient_max =  3.4097477405708396 learning rate =  0.0001\n",
      "Epoch =  500 Batch =  0 Loss =  [9.22498054] Gradient_max =  3.4097248077402735 learning rate =  0.0001\n",
      "Epoch =  501 Batch =  0 Loss =  [9.22492611] Gradient_max =  3.4097020484989087 learning rate =  0.0001\n",
      "Epoch =  502 Batch =  0 Loss =  [9.22487211] Gradient_max =  3.409679461434034 learning rate =  0.0001\n",
      "Epoch =  503 Batch =  0 Loss =  [9.22481853] Gradient_max =  3.409657045145608 learning rate =  0.0001\n",
      "Epoch =  504 Batch =  0 Loss =  [9.22476539] Gradient_max =  3.409634798246133 learning rate =  0.0001\n",
      "Epoch =  505 Batch =  0 Loss =  [9.22471266] Gradient_max =  3.409612719360525 learning rate =  0.0001\n",
      "Epoch =  506 Batch =  0 Loss =  [9.22466035] Gradient_max =  3.409590807125994 learning rate =  0.0001\n",
      "Epoch =  507 Batch =  0 Loss =  [9.22460846] Gradient_max =  3.409569060191921 learning rate =  0.0001\n",
      "Epoch =  508 Batch =  0 Loss =  [9.22455698] Gradient_max =  3.409547477219725 learning rate =  0.0001\n",
      "Epoch =  509 Batch =  0 Loss =  [9.22450591] Gradient_max =  3.4095260568827563 learning rate =  0.0001\n",
      "Epoch =  510 Batch =  0 Loss =  [9.22445524] Gradient_max =  3.4095047978661612 learning rate =  0.0001\n",
      "Epoch =  511 Batch =  0 Loss =  [9.22440498] Gradient_max =  3.4094836988667767 learning rate =  0.0001\n",
      "Epoch =  512 Batch =  0 Loss =  [9.22435511] Gradient_max =  3.409462758593007 learning rate =  0.0001\n",
      "Epoch =  513 Batch =  0 Loss =  [9.22430564] Gradient_max =  3.409441975764711 learning rate =  0.0001\n",
      "Epoch =  514 Batch =  0 Loss =  [9.22425656] Gradient_max =  3.4094213491130856 learning rate =  0.0001\n",
      "Epoch =  515 Batch =  0 Loss =  [9.22420786] Gradient_max =  3.4094008773805475 learning rate =  0.0001\n",
      "Epoch =  516 Batch =  0 Loss =  [9.22415956] Gradient_max =  3.4093805593206388 learning rate =  0.0001\n",
      "Epoch =  517 Batch =  0 Loss =  [9.22411163] Gradient_max =  3.409360393697894 learning rate =  0.0001\n",
      "Epoch =  518 Batch =  0 Loss =  [9.22406408] Gradient_max =  3.409340379287749 learning rate =  0.0001\n",
      "Epoch =  519 Batch =  0 Loss =  [9.22401691] Gradient_max =  3.4093205148764207 learning rate =  0.0001\n",
      "Epoch =  520 Batch =  0 Loss =  [9.22397011] Gradient_max =  3.409300799260809 learning rate =  0.0001\n",
      "Epoch =  521 Batch =  0 Loss =  [9.22392368] Gradient_max =  3.4092812312483844 learning rate =  0.0001\n",
      "Epoch =  522 Batch =  0 Loss =  [9.22387762] Gradient_max =  3.4092618096570906 learning rate =  0.0001\n",
      "Epoch =  523 Batch =  0 Loss =  [9.22383192] Gradient_max =  3.409242533315234 learning rate =  0.0001\n",
      "Epoch =  524 Batch =  0 Loss =  [9.22378658] Gradient_max =  3.409223401061387 learning rate =  0.0001\n",
      "Epoch =  525 Batch =  0 Loss =  [9.2237416] Gradient_max =  3.4092044117442817 learning rate =  0.0001\n",
      "Epoch =  526 Batch =  0 Loss =  [9.22369698] Gradient_max =  3.4091855642227182 learning rate =  0.0001\n",
      "Epoch =  527 Batch =  0 Loss =  [9.2236527] Gradient_max =  3.4091668573654617 learning rate =  0.0001\n",
      "Epoch =  528 Batch =  0 Loss =  [9.22360877] Gradient_max =  3.4091482900511365 learning rate =  0.0001\n",
      "Epoch =  529 Batch =  0 Loss =  [9.22356519] Gradient_max =  3.4091298611681498 learning rate =  0.0001\n",
      "Epoch =  530 Batch =  0 Loss =  [9.22352196] Gradient_max =  3.4091115696145735 learning rate =  0.0001\n",
      "Epoch =  531 Batch =  0 Loss =  [9.22347906] Gradient_max =  3.4090934142980664 learning rate =  0.0001\n",
      "Epoch =  532 Batch =  0 Loss =  [9.2234365] Gradient_max =  3.409075394135772 learning rate =  0.0001\n",
      "Epoch =  533 Batch =  0 Loss =  [9.22339427] Gradient_max =  3.4090575080542314 learning rate =  0.0001\n",
      "Epoch =  534 Batch =  0 Loss =  [9.22335238] Gradient_max =  3.4090397549892875 learning rate =  0.0001\n",
      "Epoch =  535 Batch =  0 Loss =  [9.22331081] Gradient_max =  3.4090221338860003 learning rate =  0.0001\n",
      "Epoch =  536 Batch =  0 Loss =  [9.22326958] Gradient_max =  3.4090046436985526 learning rate =  0.0001\n",
      "Epoch =  537 Batch =  0 Loss =  [9.22322866] Gradient_max =  3.4089872833901627 learning rate =  0.0001\n",
      "Epoch =  538 Batch =  0 Loss =  [9.22318807] Gradient_max =  3.408970051933002 learning rate =  0.0001\n",
      "Epoch =  539 Batch =  0 Loss =  [9.22314779] Gradient_max =  3.408952948308099 learning rate =  0.0001\n",
      "Epoch =  540 Batch =  0 Loss =  [9.22310783] Gradient_max =  3.4089359715052665 learning rate =  0.0001\n",
      "Epoch =  541 Batch =  0 Loss =  [9.22306819] Gradient_max =  3.408919120523005 learning rate =  0.0001\n",
      "Epoch =  542 Batch =  0 Loss =  [9.22302885] Gradient_max =  3.4089023943684267 learning rate =  0.0001\n",
      "Epoch =  543 Batch =  0 Loss =  [9.22298983] Gradient_max =  3.4088857920571702 learning rate =  0.0001\n",
      "Epoch =  544 Batch =  0 Loss =  [9.2229511] Gradient_max =  3.4088693126133234 learning rate =  0.0001\n",
      "Epoch =  545 Batch =  0 Loss =  [9.22291269] Gradient_max =  3.408852955069333 learning rate =  0.0001\n",
      "Epoch =  546 Batch =  0 Loss =  [9.22287457] Gradient_max =  3.4088367184659316 learning rate =  0.0001\n",
      "Epoch =  547 Batch =  0 Loss =  [9.22283675] Gradient_max =  3.4088206018520615 learning rate =  0.0001\n",
      "Epoch =  548 Batch =  0 Loss =  [9.22279922] Gradient_max =  3.408804604284788 learning rate =  0.0001\n",
      "Epoch =  549 Batch =  0 Loss =  [9.22276199] Gradient_max =  3.408788724829229 learning rate =  0.0001\n",
      "Epoch =  550 Batch =  0 Loss =  [9.22272505] Gradient_max =  3.4087729625584746 learning rate =  0.0001\n",
      "Epoch =  551 Batch =  0 Loss =  [9.2226884] Gradient_max =  3.4087573165535123 learning rate =  0.0001\n",
      "Epoch =  552 Batch =  0 Loss =  [9.22265204] Gradient_max =  3.4087417859031532 learning rate =  0.0001\n",
      "Epoch =  553 Batch =  0 Loss =  [9.22261596] Gradient_max =  3.4087263697039605 learning rate =  0.0001\n",
      "Epoch =  554 Batch =  0 Loss =  [9.22258016] Gradient_max =  3.4087110670601666 learning rate =  0.0001\n",
      "Epoch =  555 Batch =  0 Loss =  [9.22254464] Gradient_max =  3.408695877083612 learning rate =  0.0001\n",
      "Epoch =  556 Batch =  0 Loss =  [9.22250939] Gradient_max =  3.408680798893667 learning rate =  0.0001\n",
      "Epoch =  557 Batch =  0 Loss =  [9.22247443] Gradient_max =  3.4086658316171636 learning rate =  0.0001\n",
      "Epoch =  558 Batch =  0 Loss =  [9.22243973] Gradient_max =  3.4086509743883218 learning rate =  0.0001\n",
      "Epoch =  559 Batch =  0 Loss =  [9.2224053] Gradient_max =  3.4086362263486865 learning rate =  0.0001\n",
      "Epoch =  560 Batch =  0 Loss =  [9.22237115] Gradient_max =  3.4086215866470475 learning rate =  0.0001\n",
      "Epoch =  561 Batch =  0 Loss =  [9.22233725] Gradient_max =  3.408607054439389 learning rate =  0.0001\n",
      "Epoch =  562 Batch =  0 Loss =  [9.22230362] Gradient_max =  3.4085926288888038 learning rate =  0.0001\n",
      "Epoch =  563 Batch =  0 Loss =  [9.22227026] Gradient_max =  3.408578309165437 learning rate =  0.0001\n",
      "Epoch =  564 Batch =  0 Loss =  [9.22223715] Gradient_max =  3.4085640944464206 learning rate =  0.0001\n",
      "Epoch =  565 Batch =  0 Loss =  [9.2222043] Gradient_max =  3.408549983915806 learning rate =  0.0001\n",
      "Epoch =  566 Batch =  0 Loss =  [9.22217171] Gradient_max =  3.4085359767644965 learning rate =  0.0001\n",
      "Epoch =  567 Batch =  0 Loss =  [9.22213936] Gradient_max =  3.4085220721901903 learning rate =  0.0001\n",
      "Epoch =  568 Batch =  0 Loss =  [9.22210727] Gradient_max =  3.4085082693973114 learning rate =  0.0001\n",
      "Epoch =  569 Batch =  0 Loss =  [9.22207543] Gradient_max =  3.408494567596952 learning rate =  0.0001\n",
      "Epoch =  570 Batch =  0 Loss =  [9.22204384] Gradient_max =  3.4084809660068043 learning rate =  0.0001\n",
      "Epoch =  571 Batch =  0 Loss =  [9.22201249] Gradient_max =  3.4084674638511094 learning rate =  0.0001\n",
      "Epoch =  572 Batch =  0 Loss =  [9.22198138] Gradient_max =  3.4084540603605835 learning rate =  0.0001\n",
      "Epoch =  573 Batch =  0 Loss =  [9.22195051] Gradient_max =  3.408440754772372 learning rate =  0.0001\n",
      "Epoch =  574 Batch =  0 Loss =  [9.22191989] Gradient_max =  3.408427546329979 learning rate =  0.0001\n",
      "Epoch =  575 Batch =  0 Loss =  [9.2218895] Gradient_max =  3.4084144342832174 learning rate =  0.0001\n",
      "Epoch =  576 Batch =  0 Loss =  [9.22185934] Gradient_max =  3.40840141788814 learning rate =  0.0001\n",
      "Epoch =  577 Batch =  0 Loss =  [9.22182942] Gradient_max =  3.4083884964069986 learning rate =  0.0001\n",
      "Epoch =  578 Batch =  0 Loss =  [9.22179973] Gradient_max =  3.408375669108169 learning rate =  0.0001\n",
      "Epoch =  579 Batch =  0 Loss =  [9.22177027] Gradient_max =  3.4083629352661067 learning rate =  0.0001\n",
      "Epoch =  580 Batch =  0 Loss =  [9.22174104] Gradient_max =  3.4083502941612878 learning rate =  0.0001\n",
      "Epoch =  581 Batch =  0 Loss =  [9.22171203] Gradient_max =  3.4083377450801535 learning rate =  0.0001\n",
      "Epoch =  582 Batch =  0 Loss =  [9.22168325] Gradient_max =  3.4083252873150567 learning rate =  0.0001\n",
      "Epoch =  583 Batch =  0 Loss =  [9.22165469] Gradient_max =  3.4083129201642066 learning rate =  0.0001\n",
      "Epoch =  584 Batch =  0 Loss =  [9.22162635] Gradient_max =  3.4083006429316156 learning rate =  0.0001\n",
      "Epoch =  585 Batch =  0 Loss =  [9.22159823] Gradient_max =  3.408288454927048 learning rate =  0.0001\n",
      "Epoch =  586 Batch =  0 Loss =  [9.22157032] Gradient_max =  3.4082763554659685 learning rate =  0.0001\n",
      "Epoch =  587 Batch =  0 Loss =  [9.22154264] Gradient_max =  3.4082643438694844 learning rate =  0.0001\n",
      "Epoch =  588 Batch =  0 Loss =  [9.22151516] Gradient_max =  3.4082524194643025 learning rate =  0.0001\n",
      "Epoch =  589 Batch =  0 Loss =  [9.2214879] Gradient_max =  3.408240581582671 learning rate =  0.0001\n",
      "Epoch =  590 Batch =  0 Loss =  [9.22146084] Gradient_max =  3.408228829562336 learning rate =  0.0001\n",
      "Epoch =  591 Batch =  0 Loss =  [9.22143399] Gradient_max =  3.408217162746487 learning rate =  0.0001\n",
      "Epoch =  592 Batch =  0 Loss =  [9.22140736] Gradient_max =  3.408205580483712 learning rate =  0.0001\n",
      "Epoch =  593 Batch =  0 Loss =  [9.22138092] Gradient_max =  3.4081940821279386 learning rate =  0.0001\n",
      "Epoch =  594 Batch =  0 Loss =  [9.22135469] Gradient_max =  3.408182667038403 learning rate =  0.0001\n",
      "Epoch =  595 Batch =  0 Loss =  [9.22132866] Gradient_max =  3.408171334579585 learning rate =  0.0001\n",
      "Epoch =  596 Batch =  0 Loss =  [9.22130283] Gradient_max =  3.408160084121175 learning rate =  0.0001\n",
      "Epoch =  597 Batch =  0 Loss =  [9.2212772] Gradient_max =  3.408148915038019 learning rate =  0.0001\n",
      "Epoch =  598 Batch =  0 Loss =  [9.22125176] Gradient_max =  3.408137826710068 learning rate =  0.0001\n",
      "Epoch =  599 Batch =  0 Loss =  [9.22122653] Gradient_max =  3.4081268185223443 learning rate =  0.0001\n",
      "Epoch =  600 Batch =  0 Loss =  [9.22120148] Gradient_max =  3.408115889864889 learning rate =  0.0001\n",
      "Epoch =  601 Batch =  0 Loss =  [9.22117663] Gradient_max =  3.408105040132722 learning rate =  0.0001\n",
      "Epoch =  602 Batch =  0 Loss =  [9.22115196] Gradient_max =  3.408094268725787 learning rate =  0.0001\n",
      "Epoch =  603 Batch =  0 Loss =  [9.22112749] Gradient_max =  3.408083575048916 learning rate =  0.0001\n",
      "Epoch =  604 Batch =  0 Loss =  [9.2211032] Gradient_max =  3.4080729585117866 learning rate =  0.0001\n",
      "Epoch =  605 Batch =  0 Loss =  [9.2210791] Gradient_max =  3.4080624185288735 learning rate =  0.0001\n",
      "Epoch =  606 Batch =  0 Loss =  [9.22105519] Gradient_max =  3.408051954519415 learning rate =  0.0001\n",
      "Epoch =  607 Batch =  0 Loss =  [9.22103145] Gradient_max =  3.4080415659073546 learning rate =  0.0001\n",
      "Epoch =  608 Batch =  0 Loss =  [9.2210079] Gradient_max =  3.408031252121318 learning rate =  0.0001\n",
      "Epoch =  609 Batch =  0 Loss =  [9.22098453] Gradient_max =  3.408021012594555 learning rate =  0.0001\n",
      "Epoch =  610 Batch =  0 Loss =  [9.22096134] Gradient_max =  3.4080108467649124 learning rate =  0.0001\n",
      "Epoch =  611 Batch =  0 Loss =  [9.22093832] Gradient_max =  3.4080007540747856 learning rate =  0.0001\n",
      "Epoch =  612 Batch =  0 Loss =  [9.22091548] Gradient_max =  3.407990733971076 learning rate =  0.0001\n",
      "Epoch =  613 Batch =  0 Loss =  [9.22089282] Gradient_max =  3.4079807859051585 learning rate =  0.0001\n",
      "Epoch =  614 Batch =  0 Loss =  [9.22087032] Gradient_max =  3.4079709093328403 learning rate =  0.0001\n",
      "Epoch =  615 Batch =  0 Loss =  [9.220848] Gradient_max =  3.407961103714316 learning rate =  0.0001\n",
      "Epoch =  616 Batch =  0 Loss =  [9.22082585] Gradient_max =  3.40795136851414 learning rate =  0.0001\n",
      "Epoch =  617 Batch =  0 Loss =  [9.22080387] Gradient_max =  3.4079417032011703 learning rate =  0.0001\n",
      "Epoch =  618 Batch =  0 Loss =  [9.22078206] Gradient_max =  3.4079321072485507 learning rate =  0.0001\n",
      "Epoch =  619 Batch =  0 Loss =  [9.22076041] Gradient_max =  3.407922580133662 learning rate =  0.0001\n",
      "Epoch =  620 Batch =  0 Loss =  [9.22073893] Gradient_max =  3.4079131213380873 learning rate =  0.0001\n",
      "Epoch =  621 Batch =  0 Loss =  [9.22071761] Gradient_max =  3.407903730347574 learning rate =  0.0001\n",
      "Epoch =  622 Batch =  0 Loss =  [9.22069645] Gradient_max =  3.407894406652001 learning rate =  0.0001\n",
      "Epoch =  623 Batch =  0 Loss =  [9.22067545] Gradient_max =  3.4078851497453337 learning rate =  0.0001\n",
      "Epoch =  624 Batch =  0 Loss =  [9.22065462] Gradient_max =  3.407875959125603 learning rate =  0.0001\n",
      "Epoch =  625 Batch =  0 Loss =  [9.22063394] Gradient_max =  3.4078668342948544 learning rate =  0.0001\n",
      "Epoch =  626 Batch =  0 Loss =  [9.22061342] Gradient_max =  3.407857774759126 learning rate =  0.0001\n",
      "Epoch =  627 Batch =  0 Loss =  [9.22059305] Gradient_max =  3.4078487800284036 learning rate =  0.0001\n",
      "Epoch =  628 Batch =  0 Loss =  [9.22057284] Gradient_max =  3.407839849616594 learning rate =  0.0001\n",
      "Epoch =  629 Batch =  0 Loss =  [9.22055279] Gradient_max =  3.4078309830414844 learning rate =  0.0001\n",
      "Epoch =  630 Batch =  0 Loss =  [9.22053288] Gradient_max =  3.407822179824713 learning rate =  0.0001\n",
      "Epoch =  631 Batch =  0 Loss =  [9.22051313] Gradient_max =  3.4078134394917323 learning rate =  0.0001\n",
      "Epoch =  632 Batch =  0 Loss =  [9.22049352] Gradient_max =  3.407804761571784 learning rate =  0.0001\n",
      "Epoch =  633 Batch =  0 Loss =  [9.22047407] Gradient_max =  3.4077961455978554 learning rate =  0.0001\n",
      "Epoch =  634 Batch =  0 Loss =  [9.22045476] Gradient_max =  3.407787591106654 learning rate =  0.0001\n",
      "Epoch =  635 Batch =  0 Loss =  [9.2204356] Gradient_max =  3.4077790976385733 learning rate =  0.0001\n",
      "Epoch =  636 Batch =  0 Loss =  [9.22041658] Gradient_max =  3.40777066473766 learning rate =  0.0001\n",
      "Epoch =  637 Batch =  0 Loss =  [9.22039771] Gradient_max =  3.407762291951587 learning rate =  0.0001\n",
      "Epoch =  638 Batch =  0 Loss =  [9.22037898] Gradient_max =  3.407753978831614 learning rate =  0.0001\n",
      "Epoch =  639 Batch =  0 Loss =  [9.22036039] Gradient_max =  3.407745724932566 learning rate =  0.0001\n",
      "Epoch =  640 Batch =  0 Loss =  [9.22034194] Gradient_max =  3.4077375298127945 learning rate =  0.0001\n",
      "Epoch =  641 Batch =  0 Loss =  [9.22032363] Gradient_max =  3.4077293930341517 learning rate =  0.0001\n",
      "Epoch =  642 Batch =  0 Loss =  [9.22030546] Gradient_max =  3.407721314161963 learning rate =  0.0001\n",
      "Epoch =  643 Batch =  0 Loss =  [9.22028743] Gradient_max =  3.4077132927649885 learning rate =  0.0001\n",
      "Epoch =  644 Batch =  0 Loss =  [9.22026953] Gradient_max =  3.4077053284154046 learning rate =  0.0001\n",
      "Epoch =  645 Batch =  0 Loss =  [9.22025177] Gradient_max =  3.407697420688762 learning rate =  0.0001\n",
      "Epoch =  646 Batch =  0 Loss =  [9.22023414] Gradient_max =  3.4076895691639706 learning rate =  0.0001\n",
      "Epoch =  647 Batch =  0 Loss =  [9.22021665] Gradient_max =  3.40768177342326 learning rate =  0.0001\n",
      "Epoch =  648 Batch =  0 Loss =  [9.22019928] Gradient_max =  3.4076740330521584 learning rate =  0.0001\n",
      "Epoch =  649 Batch =  0 Loss =  [9.22018205] Gradient_max =  3.407666347639456 learning rate =  0.0001\n",
      "Epoch =  650 Batch =  0 Loss =  [9.22016495] Gradient_max =  3.40765871677719 learning rate =  0.0001\n",
      "Epoch =  651 Batch =  0 Loss =  [9.22014798] Gradient_max =  3.4076511400606027 learning rate =  0.0001\n",
      "Epoch =  652 Batch =  0 Loss =  [9.22013113] Gradient_max =  3.4076436170881292 learning rate =  0.0001\n",
      "Epoch =  653 Batch =  0 Loss =  [9.22011441] Gradient_max =  3.407636147461349 learning rate =  0.0001\n",
      "Epoch =  654 Batch =  0 Loss =  [9.22009782] Gradient_max =  3.407628730784989 learning rate =  0.0001\n",
      "Epoch =  655 Batch =  0 Loss =  [9.22008135] Gradient_max =  3.407621366666866 learning rate =  0.0001\n",
      "Epoch =  656 Batch =  0 Loss =  [9.220065] Gradient_max =  3.407614054717881 learning rate =  0.0001\n",
      "Epoch =  657 Batch =  0 Loss =  [9.22004878] Gradient_max =  3.4076067945519872 learning rate =  0.0001\n",
      "Epoch =  658 Batch =  0 Loss =  [9.22003268] Gradient_max =  3.407599585786168 learning rate =  0.0001\n",
      "Epoch =  659 Batch =  0 Loss =  [9.2200167] Gradient_max =  3.4075924280403953 learning rate =  0.0001\n",
      "Epoch =  660 Batch =  0 Loss =  [9.22000084] Gradient_max =  3.4075853209376286 learning rate =  0.0001\n",
      "Epoch =  661 Batch =  0 Loss =  [9.2199851] Gradient_max =  3.4075782641037646 learning rate =  0.0001\n",
      "Epoch =  662 Batch =  0 Loss =  [9.21996948] Gradient_max =  3.4075712571676404 learning rate =  0.0001\n",
      "Epoch =  663 Batch =  0 Loss =  [9.21995397] Gradient_max =  3.407564299760985 learning rate =  0.0001\n",
      "Epoch =  664 Batch =  0 Loss =  [9.21993858] Gradient_max =  3.4075573915184023 learning rate =  0.0001\n",
      "Epoch =  665 Batch =  0 Loss =  [9.21992331] Gradient_max =  3.407550532077349 learning rate =  0.0001\n",
      "Epoch =  666 Batch =  0 Loss =  [9.21990815] Gradient_max =  3.4075437210781145 learning rate =  0.0001\n",
      "Epoch =  667 Batch =  0 Loss =  [9.21989311] Gradient_max =  3.4075369581637873 learning rate =  0.0001\n",
      "Epoch =  668 Batch =  0 Loss =  [9.21987817] Gradient_max =  3.4075302429802385 learning rate =  0.0001\n",
      "Epoch =  669 Batch =  0 Loss =  [9.21986335] Gradient_max =  3.4075235751760977 learning rate =  0.0001\n",
      "Epoch =  670 Batch =  0 Loss =  [9.21984864] Gradient_max =  3.407516954402727 learning rate =  0.0001\n",
      "Epoch =  671 Batch =  0 Loss =  [9.21983404] Gradient_max =  3.4075103803142013 learning rate =  0.0001\n",
      "Epoch =  672 Batch =  0 Loss =  [9.21981955] Gradient_max =  3.4075038525672814 learning rate =  0.0001\n",
      "Epoch =  673 Batch =  0 Loss =  [9.21980516] Gradient_max =  3.4074973708214 learning rate =  0.0001\n",
      "Epoch =  674 Batch =  0 Loss =  [9.21979089] Gradient_max =  3.4074909347386275 learning rate =  0.0001\n",
      "Epoch =  675 Batch =  0 Loss =  [9.21977672] Gradient_max =  3.4074845439836623 learning rate =  0.0001\n",
      "Epoch =  676 Batch =  0 Loss =  [9.21976265] Gradient_max =  3.407478198223799 learning rate =  0.0001\n",
      "Epoch =  677 Batch =  0 Loss =  [9.21974869] Gradient_max =  3.4074718971289104 learning rate =  0.0001\n",
      "Epoch =  678 Batch =  0 Loss =  [9.21973484] Gradient_max =  3.4074656403714316 learning rate =  0.0001\n",
      "Epoch =  679 Batch =  0 Loss =  [9.21972109] Gradient_max =  3.4074594276263257 learning rate =  0.0001\n",
      "Epoch =  680 Batch =  0 Loss =  [9.21970744] Gradient_max =  3.4074532585710773 learning rate =  0.0001\n",
      "Epoch =  681 Batch =  0 Loss =  [9.21969389] Gradient_max =  3.4074471328856606 learning rate =  0.0001\n",
      "Epoch =  682 Batch =  0 Loss =  [9.21968044] Gradient_max =  3.407441050252527 learning rate =  0.0001\n",
      "Epoch =  683 Batch =  0 Loss =  [9.2196671] Gradient_max =  3.4074350103565756 learning rate =  0.0001\n",
      "Epoch =  684 Batch =  0 Loss =  [9.21965385] Gradient_max =  3.4074290128851397 learning rate =  0.0001\n",
      "Epoch =  685 Batch =  0 Loss =  [9.2196407] Gradient_max =  3.4074230575279705 learning rate =  0.0001\n",
      "Epoch =  686 Batch =  0 Loss =  [9.21962765] Gradient_max =  3.4074171439772027 learning rate =  0.0001\n",
      "Epoch =  687 Batch =  0 Loss =  [9.2196147] Gradient_max =  3.407411271927348 learning rate =  0.0001\n",
      "Epoch =  688 Batch =  0 Loss =  [9.21960184] Gradient_max =  3.4074054410752708 learning rate =  0.0001\n",
      "Epoch =  689 Batch =  0 Loss =  [9.21958907] Gradient_max =  3.4073996511201727 learning rate =  0.0001\n",
      "Epoch =  690 Batch =  0 Loss =  [9.21957641] Gradient_max =  3.4073939017635615 learning rate =  0.0001\n",
      "Epoch =  691 Batch =  0 Loss =  [9.21956383] Gradient_max =  3.4073881927092464 learning rate =  0.0001\n",
      "Epoch =  692 Batch =  0 Loss =  [9.21955135] Gradient_max =  3.4073825236633124 learning rate =  0.0001\n",
      "Epoch =  693 Batch =  0 Loss =  [9.21953897] Gradient_max =  3.4073768943340985 learning rate =  0.0001\n",
      "Epoch =  694 Batch =  0 Loss =  [9.21952667] Gradient_max =  3.4073713044321874 learning rate =  0.0001\n",
      "Epoch =  695 Batch =  0 Loss =  [9.21951446] Gradient_max =  3.4073657536703803 learning rate =  0.0001\n",
      "Epoch =  696 Batch =  0 Loss =  [9.21950235] Gradient_max =  3.4073602417636826 learning rate =  0.0001\n",
      "Epoch =  697 Batch =  0 Loss =  [9.21949032] Gradient_max =  3.407354768429279 learning rate =  0.0001\n",
      "Epoch =  698 Batch =  0 Loss =  [9.21947839] Gradient_max =  3.4073493333865277 learning rate =  0.0001\n",
      "Epoch =  699 Batch =  0 Loss =  [9.21946654] Gradient_max =  3.407343936356929 learning rate =  0.0001\n",
      "Epoch =  700 Batch =  0 Loss =  [9.21945478] Gradient_max =  3.4073385770641242 learning rate =  0.0001\n",
      "Epoch =  701 Batch =  0 Loss =  [9.21944311] Gradient_max =  3.4073332552338553 learning rate =  0.0001\n",
      "Epoch =  702 Batch =  0 Loss =  [9.21943152] Gradient_max =  3.4073279705939714 learning rate =  0.0001\n",
      "Epoch =  703 Batch =  0 Loss =  [9.21942002] Gradient_max =  3.407322722874398 learning rate =  0.0001\n",
      "Epoch =  704 Batch =  0 Loss =  [9.2194086] Gradient_max =  3.407317511807122 learning rate =  0.0001\n",
      "Epoch =  705 Batch =  0 Loss =  [9.21939727] Gradient_max =  3.4073123371261738 learning rate =  0.0001\n",
      "Epoch =  706 Batch =  0 Loss =  [9.21938602] Gradient_max =  3.40730719856762 learning rate =  0.0001\n",
      "Epoch =  707 Batch =  0 Loss =  [9.21937486] Gradient_max =  3.4073020958695306 learning rate =  0.0001\n",
      "Epoch =  708 Batch =  0 Loss =  [9.21936378] Gradient_max =  3.407297028771981 learning rate =  0.0001\n",
      "Epoch =  709 Batch =  0 Loss =  [9.21935278] Gradient_max =  3.4072919970170186 learning rate =  0.0001\n",
      "Epoch =  710 Batch =  0 Loss =  [9.21934186] Gradient_max =  3.407287000348657 learning rate =  0.0001\n",
      "Epoch =  711 Batch =  0 Loss =  [9.21933102] Gradient_max =  3.4072820385128573 learning rate =  0.0001\n",
      "Epoch =  712 Batch =  0 Loss =  [9.21932026] Gradient_max =  3.4072771112575175 learning rate =  0.0001\n",
      "Epoch =  713 Batch =  0 Loss =  [9.21930958] Gradient_max =  3.407272218332449 learning rate =  0.0001\n",
      "Epoch =  714 Batch =  0 Loss =  [9.21929897] Gradient_max =  3.4072673594893597 learning rate =  0.0001\n",
      "Epoch =  715 Batch =  0 Loss =  [9.21928845] Gradient_max =  3.4072625344818475 learning rate =  0.0001\n",
      "Epoch =  716 Batch =  0 Loss =  [9.219278] Gradient_max =  3.4072577430653808 learning rate =  0.0001\n",
      "Epoch =  717 Batch =  0 Loss =  [9.21926763] Gradient_max =  3.4072529849972817 learning rate =  0.0001\n",
      "Epoch =  718 Batch =  0 Loss =  [9.21925734] Gradient_max =  3.407248260036712 learning rate =  0.0001\n",
      "Epoch =  719 Batch =  0 Loss =  [9.21924712] Gradient_max =  3.407243567944661 learning rate =  0.0001\n",
      "Epoch =  720 Batch =  0 Loss =  [9.21923698] Gradient_max =  3.4072389084839263 learning rate =  0.0001\n",
      "Epoch =  721 Batch =  0 Loss =  [9.21922691] Gradient_max =  3.4072342814191012 learning rate =  0.0001\n",
      "Epoch =  722 Batch =  0 Loss =  [9.21921692] Gradient_max =  3.4072296865165637 learning rate =  0.0001\n",
      "Epoch =  723 Batch =  0 Loss =  [9.21920699] Gradient_max =  3.4072251235444537 learning rate =  0.0001\n",
      "Epoch =  724 Batch =  0 Loss =  [9.21919715] Gradient_max =  3.4072205922726724 learning rate =  0.0001\n",
      "Epoch =  725 Batch =  0 Loss =  [9.21918737] Gradient_max =  3.4072160924728494 learning rate =  0.0001\n",
      "Epoch =  726 Batch =  0 Loss =  [9.21917767] Gradient_max =  3.4072116239183488 learning rate =  0.0001\n",
      "Epoch =  727 Batch =  0 Loss =  [9.21916803] Gradient_max =  3.4072071863842366 learning rate =  0.0001\n",
      "Epoch =  728 Batch =  0 Loss =  [9.21915847] Gradient_max =  3.4072027796472835 learning rate =  0.0001\n",
      "Epoch =  729 Batch =  0 Loss =  [9.21914898] Gradient_max =  3.407198403485943 learning rate =  0.0001\n",
      "Epoch =  730 Batch =  0 Loss =  [9.21913955] Gradient_max =  3.407194057680333 learning rate =  0.0001\n",
      "Epoch =  731 Batch =  0 Loss =  [9.2191302] Gradient_max =  3.4071897420122346 learning rate =  0.0001\n",
      "Epoch =  732 Batch =  0 Loss =  [9.21912091] Gradient_max =  3.4071854562650707 learning rate =  0.0001\n",
      "Epoch =  733 Batch =  0 Loss =  [9.21911169] Gradient_max =  3.4071812002238917 learning rate =  0.0001\n",
      "Epoch =  734 Batch =  0 Loss =  [9.21910254] Gradient_max =  3.4071769736753716 learning rate =  0.0001\n",
      "Epoch =  735 Batch =  0 Loss =  [9.21909346] Gradient_max =  3.407172776407779 learning rate =  0.0001\n",
      "Epoch =  736 Batch =  0 Loss =  [9.21908444] Gradient_max =  3.4071686082109855 learning rate =  0.0001\n",
      "Epoch =  737 Batch =  0 Loss =  [9.21907549] Gradient_max =  3.4071644688764335 learning rate =  0.0001\n",
      "Epoch =  738 Batch =  0 Loss =  [9.2190666] Gradient_max =  3.4071603581971366 learning rate =  0.0001\n",
      "Epoch =  739 Batch =  0 Loss =  [9.21905778] Gradient_max =  3.4071562759676595 learning rate =  0.0001\n",
      "Epoch =  740 Batch =  0 Loss =  [9.21904902] Gradient_max =  3.4071522219841057 learning rate =  0.0001\n",
      "Epoch =  741 Batch =  0 Loss =  [9.21904033] Gradient_max =  3.4071481960441172 learning rate =  0.0001\n",
      "Epoch =  742 Batch =  0 Loss =  [9.2190317] Gradient_max =  3.407144197946842 learning rate =  0.0001\n",
      "Epoch =  743 Batch =  0 Loss =  [9.21902313] Gradient_max =  3.4071402274929423 learning rate =  0.0001\n",
      "Epoch =  744 Batch =  0 Loss =  [9.21901463] Gradient_max =  3.4071362844845674 learning rate =  0.0001\n",
      "Epoch =  745 Batch =  0 Loss =  [9.21900618] Gradient_max =  3.4071323687253496 learning rate =  0.0001\n",
      "Epoch =  746 Batch =  0 Loss =  [9.2189978] Gradient_max =  3.407128480020393 learning rate =  0.0001\n",
      "Epoch =  747 Batch =  0 Loss =  [9.21898948] Gradient_max =  3.407124618176256 learning rate =  0.0001\n",
      "Epoch =  748 Batch =  0 Loss =  [9.21898122] Gradient_max =  3.407120783000945 learning rate =  0.0001\n",
      "Epoch =  749 Batch =  0 Loss =  [9.21897302] Gradient_max =  3.4071169743038996 learning rate =  0.0001\n",
      "Epoch =  750 Batch =  0 Loss =  [9.21896488] Gradient_max =  3.407113191895985 learning rate =  0.0001\n",
      "Epoch =  751 Batch =  0 Loss =  [9.2189568] Gradient_max =  3.4071094355894798 learning rate =  0.0001\n",
      "Epoch =  752 Batch =  0 Loss =  [9.21894878] Gradient_max =  3.407105705198059 learning rate =  0.0001\n",
      "Epoch =  753 Batch =  0 Loss =  [9.21894081] Gradient_max =  3.407102000536794 learning rate =  0.0001\n",
      "Epoch =  754 Batch =  0 Loss =  [9.21893291] Gradient_max =  3.4070983214221267 learning rate =  0.0001\n",
      "Epoch =  755 Batch =  0 Loss =  [9.21892506] Gradient_max =  3.407094667671873 learning rate =  0.0001\n",
      "Epoch =  756 Batch =  0 Loss =  [9.21891726] Gradient_max =  3.407091039105208 learning rate =  0.0001\n",
      "Epoch =  757 Batch =  0 Loss =  [9.21890953] Gradient_max =  3.407087435542647 learning rate =  0.0001\n",
      "Epoch =  758 Batch =  0 Loss =  [9.21890185] Gradient_max =  3.4070838568060466 learning rate =  0.0001\n",
      "Epoch =  759 Batch =  0 Loss =  [9.21889422] Gradient_max =  3.407080302718588 learning rate =  0.0001\n",
      "Epoch =  760 Batch =  0 Loss =  [9.21888666] Gradient_max =  3.4070767731047646 learning rate =  0.0001\n",
      "Epoch =  761 Batch =  0 Loss =  [9.21887914] Gradient_max =  3.407073267790378 learning rate =  0.0001\n",
      "Epoch =  762 Batch =  0 Loss =  [9.21887168] Gradient_max =  3.407069786602521 learning rate =  0.0001\n",
      "Epoch =  763 Batch =  0 Loss =  [9.21886428] Gradient_max =  3.4070663293695715 learning rate =  0.0001\n",
      "Epoch =  764 Batch =  0 Loss =  [9.21885692] Gradient_max =  3.407062895921186 learning rate =  0.0001\n",
      "Epoch =  765 Batch =  0 Loss =  [9.21884963] Gradient_max =  3.4070594860882797 learning rate =  0.0001\n",
      "Epoch =  766 Batch =  0 Loss =  [9.21884238] Gradient_max =  3.4070560997030266 learning rate =  0.0001\n",
      "Epoch =  767 Batch =  0 Loss =  [9.21883519] Gradient_max =  3.407052736598841 learning rate =  0.0001\n",
      "Epoch =  768 Batch =  0 Loss =  [9.21882804] Gradient_max =  3.4070493966103723 learning rate =  0.0001\n",
      "Epoch =  769 Batch =  0 Loss =  [9.21882095] Gradient_max =  3.4070460795735005 learning rate =  0.0001\n",
      "Epoch =  770 Batch =  0 Loss =  [9.21881391] Gradient_max =  3.4070427853253142 learning rate =  0.0001\n",
      "Epoch =  771 Batch =  0 Loss =  [9.21880693] Gradient_max =  3.4070395137041105 learning rate =  0.0001\n",
      "Epoch =  772 Batch =  0 Loss =  [9.21879999] Gradient_max =  3.4070362645493897 learning rate =  0.0001\n",
      "Epoch =  773 Batch =  0 Loss =  [9.2187931] Gradient_max =  3.407033037701828 learning rate =  0.0001\n",
      "Epoch =  774 Batch =  0 Loss =  [9.21878626] Gradient_max =  3.407029833003283 learning rate =  0.0001\n",
      "Epoch =  775 Batch =  0 Loss =  [9.21877947] Gradient_max =  3.4070266502967876 learning rate =  0.0001\n",
      "Epoch =  776 Batch =  0 Loss =  [9.21877273] Gradient_max =  3.4070234894265257 learning rate =  0.0001\n",
      "Epoch =  777 Batch =  0 Loss =  [9.21876604] Gradient_max =  3.4070203502378407 learning rate =  0.0001\n",
      "Epoch =  778 Batch =  0 Loss =  [9.2187594] Gradient_max =  3.4070172325772075 learning rate =  0.0001\n",
      "Epoch =  779 Batch =  0 Loss =  [9.2187528] Gradient_max =  3.407014136292243 learning rate =  0.0001\n",
      "Epoch =  780 Batch =  0 Loss =  [9.21874625] Gradient_max =  3.407011061231682 learning rate =  0.0001\n",
      "Epoch =  781 Batch =  0 Loss =  [9.21873975] Gradient_max =  3.4070080072453797 learning rate =  0.0001\n",
      "Epoch =  782 Batch =  0 Loss =  [9.2187333] Gradient_max =  3.407004974184289 learning rate =  0.0001\n",
      "Epoch =  783 Batch =  0 Loss =  [9.21872689] Gradient_max =  3.407001961900473 learning rate =  0.0001\n",
      "Epoch =  784 Batch =  0 Loss =  [9.21872053] Gradient_max =  3.406998970247075 learning rate =  0.0001\n",
      "Epoch =  785 Batch =  0 Loss =  [9.21871421] Gradient_max =  3.4069959990783234 learning rate =  0.0001\n",
      "Epoch =  786 Batch =  0 Loss =  [9.21870794] Gradient_max =  3.4069930482495163 learning rate =  0.0001\n",
      "Epoch =  787 Batch =  0 Loss =  [9.21870171] Gradient_max =  3.40699011761702 learning rate =  0.0001\n",
      "Epoch =  788 Batch =  0 Loss =  [9.21869553] Gradient_max =  3.406987207038252 learning rate =  0.0001\n",
      "Epoch =  789 Batch =  0 Loss =  [9.2186894] Gradient_max =  3.406984316371683 learning rate =  0.0001\n",
      "Epoch =  790 Batch =  0 Loss =  [9.2186833] Gradient_max =  3.4069814454768212 learning rate =  0.0001\n",
      "Epoch =  791 Batch =  0 Loss =  [9.21867725] Gradient_max =  3.4069785942142037 learning rate =  0.0001\n",
      "Epoch =  792 Batch =  0 Loss =  [9.21867125] Gradient_max =  3.4069757624453985 learning rate =  0.0001\n",
      "Epoch =  793 Batch =  0 Loss =  [9.21866529] Gradient_max =  3.4069729500329795 learning rate =  0.0001\n",
      "Epoch =  794 Batch =  0 Loss =  [9.21865937] Gradient_max =  3.406970156840536 learning rate =  0.0001\n",
      "Epoch =  795 Batch =  0 Loss =  [9.21865349] Gradient_max =  3.4069673827326583 learning rate =  0.0001\n",
      "Epoch =  796 Batch =  0 Loss =  [9.21864765] Gradient_max =  3.406964627574924 learning rate =  0.0001\n",
      "Epoch =  797 Batch =  0 Loss =  [9.21864186] Gradient_max =  3.4069618912338973 learning rate =  0.0001\n",
      "Epoch =  798 Batch =  0 Loss =  [9.21863611] Gradient_max =  3.406959173577122 learning rate =  0.0001\n",
      "Epoch =  799 Batch =  0 Loss =  [9.21863039] Gradient_max =  3.406956474473111 learning rate =  0.0001\n",
      "Epoch =  800 Batch =  0 Loss =  [9.21862472] Gradient_max =  3.4069537937913354 learning rate =  0.0001\n",
      "Epoch =  801 Batch =  0 Loss =  [9.21861909] Gradient_max =  3.406951131402229 learning rate =  0.0001\n",
      "Epoch =  802 Batch =  0 Loss =  [9.2186135] Gradient_max =  3.406948487177164 learning rate =  0.0001\n",
      "Epoch =  803 Batch =  0 Loss =  [9.21860795] Gradient_max =  3.406945860988458 learning rate =  0.0001\n",
      "Epoch =  804 Batch =  0 Loss =  [9.21860244] Gradient_max =  3.4069432527093633 learning rate =  0.0001\n",
      "Epoch =  805 Batch =  0 Loss =  [9.21859697] Gradient_max =  3.406940662214055 learning rate =  0.0001\n",
      "Epoch =  806 Batch =  0 Loss =  [9.21859154] Gradient_max =  3.4069380893776255 learning rate =  0.0001\n",
      "Epoch =  807 Batch =  0 Loss =  [9.21858615] Gradient_max =  3.4069355340760845 learning rate =  0.0001\n",
      "Epoch =  808 Batch =  0 Loss =  [9.21858079] Gradient_max =  3.4069329961863417 learning rate =  0.0001\n",
      "Epoch =  809 Batch =  0 Loss =  [9.21857547] Gradient_max =  3.406930475586204 learning rate =  0.0001\n",
      "Epoch =  810 Batch =  0 Loss =  [9.2185702] Gradient_max =  3.4069279721543735 learning rate =  0.0001\n",
      "Epoch =  811 Batch =  0 Loss =  [9.21856495] Gradient_max =  3.4069254857704334 learning rate =  0.0001\n",
      "Epoch =  812 Batch =  0 Loss =  [9.21855975] Gradient_max =  3.406923016314845 learning rate =  0.0001\n",
      "Epoch =  813 Batch =  0 Loss =  [9.21855458] Gradient_max =  3.406920563668939 learning rate =  0.0001\n",
      "Epoch =  814 Batch =  0 Loss =  [9.21854945] Gradient_max =  3.4069181277149143 learning rate =  0.0001\n",
      "Epoch =  815 Batch =  0 Loss =  [9.21854436] Gradient_max =  3.4069157083358217 learning rate =  0.0001\n",
      "Epoch =  816 Batch =  0 Loss =  [9.2185393] Gradient_max =  3.4069133054155643 learning rate =  0.0001\n",
      "Epoch =  817 Batch =  0 Loss =  [9.21853428] Gradient_max =  3.4069109188388915 learning rate =  0.0001\n",
      "Epoch =  818 Batch =  0 Loss =  [9.2185293] Gradient_max =  3.4069085484913906 learning rate =  0.0001\n",
      "Epoch =  819 Batch =  0 Loss =  [9.21852434] Gradient_max =  3.4069061942594776 learning rate =  0.0001\n",
      "Epoch =  820 Batch =  0 Loss =  [9.21851943] Gradient_max =  3.4069038560303953 learning rate =  0.0001\n",
      "Epoch =  821 Batch =  0 Loss =  [9.21851455] Gradient_max =  3.4069015336922073 learning rate =  0.0001\n",
      "Epoch =  822 Batch =  0 Loss =  [9.2185097] Gradient_max =  3.406899227133783 learning rate =  0.0001\n",
      "Epoch =  823 Batch =  0 Loss =  [9.21850489] Gradient_max =  3.4068969362448103 learning rate =  0.0001\n",
      "Epoch =  824 Batch =  0 Loss =  [9.21850011] Gradient_max =  3.406894660915762 learning rate =  0.0001\n",
      "Epoch =  825 Batch =  0 Loss =  [9.21849537] Gradient_max =  3.4068924010379216 learning rate =  0.0001\n",
      "Epoch =  826 Batch =  0 Loss =  [9.21849066] Gradient_max =  3.4068901565033456 learning rate =  0.0001\n",
      "Epoch =  827 Batch =  0 Loss =  [9.21848599] Gradient_max =  3.4068879272048846 learning rate =  0.0001\n",
      "Epoch =  828 Batch =  0 Loss =  [9.21848134] Gradient_max =  3.406885713036157 learning rate =  0.0001\n",
      "Epoch =  829 Batch =  0 Loss =  [9.21847673] Gradient_max =  3.406883513891557 learning rate =  0.0001\n",
      "Epoch =  830 Batch =  0 Loss =  [9.21847215] Gradient_max =  3.406881329666241 learning rate =  0.0001\n",
      "Epoch =  831 Batch =  0 Loss =  [9.21846761] Gradient_max =  3.4068791602561252 learning rate =  0.0001\n",
      "Epoch =  832 Batch =  0 Loss =  [9.2184631] Gradient_max =  3.4068770055578748 learning rate =  0.0001\n",
      "Epoch =  833 Batch =  0 Loss =  [9.21845861] Gradient_max =  3.406874865468909 learning rate =  0.0001\n",
      "Epoch =  834 Batch =  0 Loss =  [9.21845416] Gradient_max =  3.4068727398873837 learning rate =  0.0001\n",
      "Epoch =  835 Batch =  0 Loss =  [9.21844975] Gradient_max =  3.4068706287121895 learning rate =  0.0001\n",
      "Epoch =  836 Batch =  0 Loss =  [9.21844536] Gradient_max =  3.406868531842957 learning rate =  0.0001\n",
      "Epoch =  837 Batch =  0 Loss =  [9.218441] Gradient_max =  3.4068664491800296 learning rate =  0.0001\n",
      "Epoch =  838 Batch =  0 Loss =  [9.21843668] Gradient_max =  3.406864380624475 learning rate =  0.0001\n",
      "Epoch =  839 Batch =  0 Loss =  [9.21843238] Gradient_max =  3.4068623260780773 learning rate =  0.0001\n",
      "Epoch =  840 Batch =  0 Loss =  [9.21842812] Gradient_max =  3.406860285443326 learning rate =  0.0001\n",
      "Epoch =  841 Batch =  0 Loss =  [9.21842388] Gradient_max =  3.406858258623414 learning rate =  0.0001\n",
      "Epoch =  842 Batch =  0 Loss =  [9.21841968] Gradient_max =  3.406856245522235 learning rate =  0.0001\n",
      "Epoch =  843 Batch =  0 Loss =  [9.21841551] Gradient_max =  3.4068542460443716 learning rate =  0.0001\n",
      "Epoch =  844 Batch =  0 Loss =  [9.21841136] Gradient_max =  3.4068522600950986 learning rate =  0.0001\n",
      "Epoch =  845 Batch =  0 Loss =  [9.21840724] Gradient_max =  3.406850287580368 learning rate =  0.0001\n",
      "Epoch =  846 Batch =  0 Loss =  [9.21840316] Gradient_max =  3.4068483284068103 learning rate =  0.0001\n",
      "Epoch =  847 Batch =  0 Loss =  [9.2183991] Gradient_max =  3.4068463824817297 learning rate =  0.0001\n",
      "Epoch =  848 Batch =  0 Loss =  [9.21839507] Gradient_max =  3.406844449713097 learning rate =  0.0001\n",
      "Epoch =  849 Batch =  0 Loss =  [9.21839107] Gradient_max =  3.4068425300095435 learning rate =  0.0001\n",
      "Epoch =  850 Batch =  0 Loss =  [9.21838709] Gradient_max =  3.4068406232803574 learning rate =  0.0001\n",
      "Epoch =  851 Batch =  0 Loss =  [9.21838315] Gradient_max =  3.406838729435483 learning rate =  0.0001\n",
      "Epoch =  852 Batch =  0 Loss =  [9.21837923] Gradient_max =  3.4068368483855065 learning rate =  0.0001\n",
      "Epoch =  853 Batch =  0 Loss =  [9.21837534] Gradient_max =  3.406834980041659 learning rate =  0.0001\n",
      "Epoch =  854 Batch =  0 Loss =  [9.21837148] Gradient_max =  3.406833124315807 learning rate =  0.0001\n",
      "Epoch =  855 Batch =  0 Loss =  [9.21836764] Gradient_max =  3.4068312811204544 learning rate =  0.0001\n",
      "Epoch =  856 Batch =  0 Loss =  [9.21836383] Gradient_max =  3.40682945036873 learning rate =  0.0001\n",
      "Epoch =  857 Batch =  0 Loss =  [9.21836005] Gradient_max =  3.406827631974381 learning rate =  0.0001\n",
      "Epoch =  858 Batch =  0 Loss =  [9.2183563] Gradient_max =  3.406825825851782 learning rate =  0.0001\n",
      "Epoch =  859 Batch =  0 Loss =  [9.21835257] Gradient_max =  3.4068240319159155 learning rate =  0.0001\n",
      "Epoch =  860 Batch =  0 Loss =  [9.21834886] Gradient_max =  3.4068222500823757 learning rate =  0.0001\n",
      "Epoch =  861 Batch =  0 Loss =  [9.21834519] Gradient_max =  3.4068204802673625 learning rate =  0.0001\n",
      "Epoch =  862 Batch =  0 Loss =  [9.21834154] Gradient_max =  3.4068187223876696 learning rate =  0.0001\n",
      "Epoch =  863 Batch =  0 Loss =  [9.21833791] Gradient_max =  3.406816976360699 learning rate =  0.0001\n",
      "Epoch =  864 Batch =  0 Loss =  [9.21833431] Gradient_max =  3.406815242104432 learning rate =  0.0001\n",
      "Epoch =  865 Batch =  0 Loss =  [9.21833073] Gradient_max =  3.4068135195374394 learning rate =  0.0001\n",
      "Epoch =  866 Batch =  0 Loss =  [9.21832718] Gradient_max =  3.4068118085788828 learning rate =  0.0001\n",
      "Epoch =  867 Batch =  0 Loss =  [9.21832366] Gradient_max =  3.40681010914849 learning rate =  0.0001\n",
      "Epoch =  868 Batch =  0 Loss =  [9.21832016] Gradient_max =  3.406808421166569 learning rate =  0.0001\n",
      "Epoch =  869 Batch =  0 Loss =  [9.21831668] Gradient_max =  3.4068067445539967 learning rate =  0.0001\n",
      "Epoch =  870 Batch =  0 Loss =  [9.21831323] Gradient_max =  3.4068050792322158 learning rate =  0.0001\n",
      "Epoch =  871 Batch =  0 Loss =  [9.2183098] Gradient_max =  3.406803425123228 learning rate =  0.0001\n",
      "Epoch =  872 Batch =  0 Loss =  [9.2183064] Gradient_max =  3.406801782149593 learning rate =  0.0001\n",
      "Epoch =  873 Batch =  0 Loss =  [9.21830302] Gradient_max =  3.406800150234427 learning rate =  0.0001\n",
      "Epoch =  874 Batch =  0 Loss =  [9.21829966] Gradient_max =  3.4067985293013847 learning rate =  0.0001\n",
      "Epoch =  875 Batch =  0 Loss =  [9.21829633] Gradient_max =  3.406796919274681 learning rate =  0.0001\n",
      "Epoch =  876 Batch =  0 Loss =  [9.21829302] Gradient_max =  3.406795320079056 learning rate =  0.0001\n",
      "Epoch =  877 Batch =  0 Loss =  [9.21828973] Gradient_max =  3.406793731639795 learning rate =  0.0001\n",
      "Epoch =  878 Batch =  0 Loss =  [9.21828647] Gradient_max =  3.4067921538827126 learning rate =  0.0001\n",
      "Epoch =  879 Batch =  0 Loss =  [9.21828323] Gradient_max =  3.4067905867341506 learning rate =  0.0001\n",
      "Epoch =  880 Batch =  0 Loss =  [9.21828001] Gradient_max =  3.406789030120981 learning rate =  0.0001\n",
      "Epoch =  881 Batch =  0 Loss =  [9.21827682] Gradient_max =  3.406787483970593 learning rate =  0.0001\n",
      "Epoch =  882 Batch =  0 Loss =  [9.21827364] Gradient_max =  3.4067859482108918 learning rate =  0.0001\n",
      "Epoch =  883 Batch =  0 Loss =  [9.21827049] Gradient_max =  3.4067844227702944 learning rate =  0.0001\n",
      "Epoch =  884 Batch =  0 Loss =  [9.21826736] Gradient_max =  3.406782907577734 learning rate =  0.0001\n",
      "Epoch =  885 Batch =  0 Loss =  [9.21826425] Gradient_max =  3.4067814025626424 learning rate =  0.0001\n",
      "Epoch =  886 Batch =  0 Loss =  [9.21826117] Gradient_max =  3.4067799076549528 learning rate =  0.0001\n",
      "Epoch =  887 Batch =  0 Loss =  [9.2182581] Gradient_max =  3.4067784227851026 learning rate =  0.0001\n",
      "Epoch =  888 Batch =  0 Loss =  [9.21825506] Gradient_max =  3.4067769478840164 learning rate =  0.0001\n",
      "Epoch =  889 Batch =  0 Loss =  [9.21825204] Gradient_max =  3.406775482883116 learning rate =  0.0001\n",
      "Epoch =  890 Batch =  0 Loss =  [9.21824904] Gradient_max =  3.4067740277143046 learning rate =  0.0001\n",
      "Epoch =  891 Batch =  0 Loss =  [9.21824606] Gradient_max =  3.4067725823099724 learning rate =  0.0001\n",
      "Epoch =  892 Batch =  0 Loss =  [9.2182431] Gradient_max =  3.4067711466029893 learning rate =  0.0001\n",
      "Epoch =  893 Batch =  0 Loss =  [9.21824016] Gradient_max =  3.4067697205266976 learning rate =  0.0001\n",
      "Epoch =  894 Batch =  0 Loss =  [9.21823724] Gradient_max =  3.4067683040149186 learning rate =  0.0001\n",
      "Epoch =  895 Batch =  0 Loss =  [9.21823434] Gradient_max =  3.406766897001937 learning rate =  0.0001\n",
      "Epoch =  896 Batch =  0 Loss =  [9.21823146] Gradient_max =  3.406765499422508 learning rate =  0.0001\n",
      "Epoch =  897 Batch =  0 Loss =  [9.21822861] Gradient_max =  3.406764111211846 learning rate =  0.0001\n",
      "Epoch =  898 Batch =  0 Loss =  [9.21822577] Gradient_max =  3.4067627323056238 learning rate =  0.0001\n",
      "Epoch =  899 Batch =  0 Loss =  [9.21822295] Gradient_max =  3.4067613626399753 learning rate =  0.0001\n",
      "Epoch =  900 Batch =  0 Loss =  [9.21822015] Gradient_max =  3.4067600021514783 learning rate =  0.0001\n",
      "Epoch =  901 Batch =  0 Loss =  [9.21821737] Gradient_max =  3.4067586507771677 learning rate =  0.0001\n",
      "Epoch =  902 Batch =  0 Loss =  [9.21821461] Gradient_max =  3.406757308454519 learning rate =  0.0001\n",
      "Epoch =  903 Batch =  0 Loss =  [9.21821187] Gradient_max =  3.4067559751214525 learning rate =  0.0001\n",
      "Epoch =  904 Batch =  0 Loss =  [9.21820915] Gradient_max =  3.406754650716326 learning rate =  0.0001\n",
      "Epoch =  905 Batch =  0 Loss =  [9.21820644] Gradient_max =  3.406753335177931 learning rate =  0.0001\n",
      "Epoch =  906 Batch =  0 Loss =  [9.21820376] Gradient_max =  3.406752028445496 learning rate =  0.0001\n",
      "Epoch =  907 Batch =  0 Loss =  [9.21820109] Gradient_max =  3.40675073045868 learning rate =  0.0001\n",
      "Epoch =  908 Batch =  0 Loss =  [9.21819844] Gradient_max =  3.406749441157561 learning rate =  0.0001\n",
      "Epoch =  909 Batch =  0 Loss =  [9.21819581] Gradient_max =  3.406748160482648 learning rate =  0.0001\n",
      "Epoch =  910 Batch =  0 Loss =  [9.2181932] Gradient_max =  3.4067468883748653 learning rate =  0.0001\n",
      "Epoch =  911 Batch =  0 Loss =  [9.21819061] Gradient_max =  3.4067456247755543 learning rate =  0.0001\n",
      "Epoch =  912 Batch =  0 Loss =  [9.21818803] Gradient_max =  3.406744369626474 learning rate =  0.0001\n",
      "Epoch =  913 Batch =  0 Loss =  [9.21818548] Gradient_max =  3.406743122869791 learning rate =  0.0001\n",
      "Epoch =  914 Batch =  0 Loss =  [9.21818294] Gradient_max =  3.406741884448079 learning rate =  0.0001\n",
      "Epoch =  915 Batch =  0 Loss =  [9.21818042] Gradient_max =  3.4067406543043215 learning rate =  0.0001\n",
      "Epoch =  916 Batch =  0 Loss =  [9.21817791] Gradient_max =  3.4067394323818996 learning rate =  0.0001\n",
      "Epoch =  917 Batch =  0 Loss =  [9.21817542] Gradient_max =  3.4067382186245925 learning rate =  0.0001\n",
      "Epoch =  918 Batch =  0 Loss =  [9.21817295] Gradient_max =  3.406737012976581 learning rate =  0.0001\n",
      "Epoch =  919 Batch =  0 Loss =  [9.2181705] Gradient_max =  3.4067358153824308 learning rate =  0.0001\n",
      "Epoch =  920 Batch =  0 Loss =  [9.21816806] Gradient_max =  3.4067346257871063 learning rate =  0.0001\n",
      "Epoch =  921 Batch =  0 Loss =  [9.21816564] Gradient_max =  3.4067334441359556 learning rate =  0.0001\n",
      "Epoch =  922 Batch =  0 Loss =  [9.21816324] Gradient_max =  3.4067322703747096 learning rate =  0.0001\n",
      "Epoch =  923 Batch =  0 Loss =  [9.21816085] Gradient_max =  3.4067311044494852 learning rate =  0.0001\n",
      "Epoch =  924 Batch =  0 Loss =  [9.21815848] Gradient_max =  3.4067299463067724 learning rate =  0.0001\n",
      "Epoch =  925 Batch =  0 Loss =  [9.21815613] Gradient_max =  3.4067287958934416 learning rate =  0.0001\n",
      "Epoch =  926 Batch =  0 Loss =  [9.21815379] Gradient_max =  3.4067276531567408 learning rate =  0.0001\n",
      "Epoch =  927 Batch =  0 Loss =  [9.21815147] Gradient_max =  3.406726518044276 learning rate =  0.0001\n",
      "Epoch =  928 Batch =  0 Loss =  [9.21814916] Gradient_max =  3.406725390504037 learning rate =  0.0001\n",
      "Epoch =  929 Batch =  0 Loss =  [9.21814687] Gradient_max =  3.4067242704843665 learning rate =  0.0001\n",
      "Epoch =  930 Batch =  0 Loss =  [9.2181446] Gradient_max =  3.4067231579339743 learning rate =  0.0001\n",
      "Epoch =  931 Batch =  0 Loss =  [9.21814234] Gradient_max =  3.4067220528019315 learning rate =  0.0001\n",
      "Epoch =  932 Batch =  0 Loss =  [9.2181401] Gradient_max =  3.40672095503767 learning rate =  0.0001\n",
      "Epoch =  933 Batch =  0 Loss =  [9.21813787] Gradient_max =  3.4067198645909667 learning rate =  0.0001\n",
      "Epoch =  934 Batch =  0 Loss =  [9.21813566] Gradient_max =  3.4067187814119566 learning rate =  0.0001\n",
      "Epoch =  935 Batch =  0 Loss =  [9.21813346] Gradient_max =  3.4067177054511264 learning rate =  0.0001\n",
      "Epoch =  936 Batch =  0 Loss =  [9.21813128] Gradient_max =  3.406716636659308 learning rate =  0.0001\n",
      "Epoch =  937 Batch =  0 Loss =  [9.21812912] Gradient_max =  3.406715574987675 learning rate =  0.0001\n",
      "Epoch =  938 Batch =  0 Loss =  [9.21812696] Gradient_max =  3.406714520387748 learning rate =  0.0001\n",
      "Epoch =  939 Batch =  0 Loss =  [9.21812483] Gradient_max =  3.4067134728113833 learning rate =  0.0001\n",
      "Epoch =  940 Batch =  0 Loss =  [9.2181227] Gradient_max =  3.4067124322107762 learning rate =  0.0001\n",
      "Epoch =  941 Batch =  0 Loss =  [9.2181206] Gradient_max =  3.4067113985384583 learning rate =  0.0001\n",
      "Epoch =  942 Batch =  0 Loss =  [9.2181185] Gradient_max =  3.40671037174729 learning rate =  0.0001\n",
      "Epoch =  943 Batch =  0 Loss =  [9.21811642] Gradient_max =  3.4067093517904623 learning rate =  0.0001\n",
      "Epoch =  944 Batch =  0 Loss =  [9.21811436] Gradient_max =  3.406708338621497 learning rate =  0.0001\n",
      "Epoch =  945 Batch =  0 Loss =  [9.21811231] Gradient_max =  3.406707332194236 learning rate =  0.0001\n",
      "Epoch =  946 Batch =  0 Loss =  [9.21811027] Gradient_max =  3.40670633246285 learning rate =  0.0001\n",
      "Epoch =  947 Batch =  0 Loss =  [9.21810825] Gradient_max =  3.4067053393818214 learning rate =  0.0001\n",
      "Epoch =  948 Batch =  0 Loss =  [9.21810624] Gradient_max =  3.4067043529059626 learning rate =  0.0001\n",
      "Epoch =  949 Batch =  0 Loss =  [9.21810424] Gradient_max =  3.406703372990389 learning rate =  0.0001\n",
      "Epoch =  950 Batch =  0 Loss =  [9.21810226] Gradient_max =  3.4067023995905417 learning rate =  0.0001\n",
      "Epoch =  951 Batch =  0 Loss =  [9.21810029] Gradient_max =  3.406701432662165 learning rate =  0.0001\n",
      "Epoch =  952 Batch =  0 Loss =  [9.21809834] Gradient_max =  3.406700472161311 learning rate =  0.0001\n",
      "Epoch =  953 Batch =  0 Loss =  [9.2180964] Gradient_max =  3.406699518044344 learning rate =  0.0001\n",
      "Epoch =  954 Batch =  0 Loss =  [9.21809447] Gradient_max =  3.4066985702679298 learning rate =  0.0001\n",
      "Epoch =  955 Batch =  0 Loss =  [9.21809256] Gradient_max =  3.4066976287890376 learning rate =  0.0001\n",
      "Epoch =  956 Batch =  0 Loss =  [9.21809065] Gradient_max =  3.4066966935649385 learning rate =  0.0001\n",
      "Epoch =  957 Batch =  0 Loss =  [9.21808877] Gradient_max =  3.406695764553198 learning rate =  0.0001\n",
      "Epoch =  958 Batch =  0 Loss =  [9.21808689] Gradient_max =  3.406694841711677 learning rate =  0.0001\n",
      "Epoch =  959 Batch =  0 Loss =  [9.21808503] Gradient_max =  3.4066939249985335 learning rate =  0.0001\n",
      "Epoch =  960 Batch =  0 Loss =  [9.21808318] Gradient_max =  3.4066930143722165 learning rate =  0.0001\n",
      "Epoch =  961 Batch =  0 Loss =  [9.21808134] Gradient_max =  3.4066921097914653 learning rate =  0.0001\n",
      "Epoch =  962 Batch =  0 Loss =  [9.21807951] Gradient_max =  3.4066912112153003 learning rate =  0.0001\n",
      "Epoch =  963 Batch =  0 Loss =  [9.2180777] Gradient_max =  3.406690318603032 learning rate =  0.0001\n",
      "Epoch =  964 Batch =  0 Loss =  [9.2180759] Gradient_max =  3.4066894319142578 learning rate =  0.0001\n",
      "Epoch =  965 Batch =  0 Loss =  [9.21807411] Gradient_max =  3.4066885511088487 learning rate =  0.0001\n",
      "Epoch =  966 Batch =  0 Loss =  [9.21807234] Gradient_max =  3.406687676146962 learning rate =  0.0001\n",
      "Epoch =  967 Batch =  0 Loss =  [9.21807057] Gradient_max =  3.406686806989027 learning rate =  0.0001\n",
      "Epoch =  968 Batch =  0 Loss =  [9.21806882] Gradient_max =  3.4066859435957473 learning rate =  0.0001\n",
      "Epoch =  969 Batch =  0 Loss =  [9.21806708] Gradient_max =  3.4066850859281046 learning rate =  0.0001\n",
      "Epoch =  970 Batch =  0 Loss =  [9.21806535] Gradient_max =  3.406684233947352 learning rate =  0.0001\n",
      "Epoch =  971 Batch =  0 Loss =  [9.21806364] Gradient_max =  3.406683387615007 learning rate =  0.0001\n",
      "Epoch =  972 Batch =  0 Loss =  [9.21806193] Gradient_max =  3.406682546892855 learning rate =  0.0001\n",
      "Epoch =  973 Batch =  0 Loss =  [9.21806024] Gradient_max =  3.406681711742952 learning rate =  0.0001\n",
      "Epoch =  974 Batch =  0 Loss =  [9.21805856] Gradient_max =  3.4066808821276147 learning rate =  0.0001\n",
      "Epoch =  975 Batch =  0 Loss =  [9.21805689] Gradient_max =  3.4066800580094174 learning rate =  0.0001\n",
      "Epoch =  976 Batch =  0 Loss =  [9.21805523] Gradient_max =  3.4066792393511984 learning rate =  0.0001\n",
      "Epoch =  977 Batch =  0 Loss =  [9.21805358] Gradient_max =  3.406678426116056 learning rate =  0.0001\n",
      "Epoch =  978 Batch =  0 Loss =  [9.21805195] Gradient_max =  3.406677618267338 learning rate =  0.0001\n",
      "Epoch =  979 Batch =  0 Loss =  [9.21805032] Gradient_max =  3.4066768157686522 learning rate =  0.0001\n",
      "Epoch =  980 Batch =  0 Loss =  [9.21804871] Gradient_max =  3.406676018583857 learning rate =  0.0001\n",
      "Epoch =  981 Batch =  0 Loss =  [9.2180471] Gradient_max =  3.4066752266770615 learning rate =  0.0001\n",
      "Epoch =  982 Batch =  0 Loss =  [9.21804551] Gradient_max =  3.4066744400126203 learning rate =  0.0001\n",
      "Epoch =  983 Batch =  0 Loss =  [9.21804393] Gradient_max =  3.4066736585551407 learning rate =  0.0001\n",
      "Epoch =  984 Batch =  0 Loss =  [9.21804236] Gradient_max =  3.4066728822694707 learning rate =  0.0001\n",
      "Epoch =  985 Batch =  0 Loss =  [9.2180408] Gradient_max =  3.406672111120703 learning rate =  0.0001\n",
      "Epoch =  986 Batch =  0 Loss =  [9.21803925] Gradient_max =  3.4066713450741744 learning rate =  0.0001\n",
      "Epoch =  987 Batch =  0 Loss =  [9.21803771] Gradient_max =  3.4066705840954574 learning rate =  0.0001\n",
      "Epoch =  988 Batch =  0 Loss =  [9.21803618] Gradient_max =  3.4066698281503665 learning rate =  0.0001\n",
      "Epoch =  989 Batch =  0 Loss =  [9.21803466] Gradient_max =  3.406669077204952 learning rate =  0.0001\n",
      "Epoch =  990 Batch =  0 Loss =  [9.21803316] Gradient_max =  3.406668331225495 learning rate =  0.0001\n",
      "Epoch =  991 Batch =  0 Loss =  [9.21803166] Gradient_max =  3.406667590178517 learning rate =  0.0001\n",
      "Epoch =  992 Batch =  0 Loss =  [9.21803017] Gradient_max =  3.4066668540307643 learning rate =  0.0001\n",
      "Epoch =  993 Batch =  0 Loss =  [9.21802869] Gradient_max =  3.406666122749219 learning rate =  0.0001\n",
      "Epoch =  994 Batch =  0 Loss =  [9.21802723] Gradient_max =  3.4066653963010856 learning rate =  0.0001\n",
      "Epoch =  995 Batch =  0 Loss =  [9.21802577] Gradient_max =  3.4066646746537983 learning rate =  0.0001\n",
      "Epoch =  996 Batch =  0 Loss =  [9.21802432] Gradient_max =  3.406663957775018 learning rate =  0.0001\n",
      "Epoch =  997 Batch =  0 Loss =  [9.21802288] Gradient_max =  3.406663245632624 learning rate =  0.0001\n",
      "Epoch =  998 Batch =  0 Loss =  [9.21802146] Gradient_max =  3.4066625381947224 learning rate =  0.0001\n",
      "Epoch =  999 Batch =  0 Loss =  [9.21802004] Gradient_max =  3.406661835429639 learning rate =  0.0001\n",
      "Epoch =  1000 Batch =  0 Loss =  [9.21801863] Gradient_max =  3.406661137305914 learning rate =  0.0001\n",
      "Epoch =  1001 Batch =  0 Loss =  [9.21801723] Gradient_max =  3.4066604437923074 learning rate =  0.0001\n",
      "Epoch =  1002 Batch =  0 Loss =  [9.21801584] Gradient_max =  3.406659754857798 learning rate =  0.0001\n",
      "Epoch =  1003 Batch =  0 Loss =  [9.21801446] Gradient_max =  3.40665907047157 learning rate =  0.0001\n",
      "Epoch =  1004 Batch =  0 Loss =  [9.21801309] Gradient_max =  3.4066583906030306 learning rate =  0.0001\n",
      "Epoch =  1005 Batch =  0 Loss =  [9.21801172] Gradient_max =  3.4066577152217925 learning rate =  0.0001\n",
      "Epoch =  1006 Batch =  0 Loss =  [9.21801037] Gradient_max =  3.406657044297673 learning rate =  0.0001\n",
      "Epoch =  1007 Batch =  0 Loss =  [9.21800903] Gradient_max =  3.406656377800707 learning rate =  0.0001\n",
      "Epoch =  1008 Batch =  0 Loss =  [9.21800769] Gradient_max =  3.406655715701133 learning rate =  0.0001\n",
      "Epoch =  1009 Batch =  0 Loss =  [9.21800637] Gradient_max =  3.4066550579693873 learning rate =  0.0001\n",
      "Epoch =  1010 Batch =  0 Loss =  [9.21800505] Gradient_max =  3.4066544045761193 learning rate =  0.0001\n",
      "Epoch =  1011 Batch =  0 Loss =  [9.21800374] Gradient_max =  3.4066537554921763 learning rate =  0.0001\n",
      "Epoch =  1012 Batch =  0 Loss =  [9.21800244] Gradient_max =  3.406653110688605 learning rate =  0.0001\n",
      "Epoch =  1013 Batch =  0 Loss =  [9.21800115] Gradient_max =  3.406652470136654 learning rate =  0.0001\n",
      "Epoch =  1014 Batch =  0 Loss =  [9.21799987] Gradient_max =  3.4066518338077687 learning rate =  0.0001\n",
      "Epoch =  1015 Batch =  0 Loss =  [9.2179986] Gradient_max =  3.4066512016735917 learning rate =  0.0001\n",
      "Epoch =  1016 Batch =  0 Loss =  [9.21799734] Gradient_max =  3.4066505737059587 learning rate =  0.0001\n",
      "Epoch =  1017 Batch =  0 Loss =  [9.21799608] Gradient_max =  3.4066499498769 learning rate =  0.0001\n",
      "Epoch =  1018 Batch =  0 Loss =  [9.21799483] Gradient_max =  3.40664933015864 learning rate =  0.0001\n",
      "Epoch =  1019 Batch =  0 Loss =  [9.21799359] Gradient_max =  3.406648714523594 learning rate =  0.0001\n",
      "Epoch =  1020 Batch =  0 Loss =  [9.21799236] Gradient_max =  3.4066481029443616 learning rate =  0.0001\n",
      "Epoch =  1021 Batch =  0 Loss =  [9.21799114] Gradient_max =  3.406647495393736 learning rate =  0.0001\n",
      "Epoch =  1022 Batch =  0 Loss =  [9.21798993] Gradient_max =  3.4066468918446966 learning rate =  0.0001\n",
      "Epoch =  1023 Batch =  0 Loss =  [9.21798872] Gradient_max =  3.406646292270405 learning rate =  0.0001\n",
      "Epoch =  1024 Batch =  0 Loss =  [9.21798752] Gradient_max =  3.406645696644215 learning rate =  0.0001\n",
      "Epoch =  1025 Batch =  0 Loss =  [9.21798633] Gradient_max =  3.4066451049396527 learning rate =  0.0001\n",
      "Epoch =  1026 Batch =  0 Loss =  [9.21798515] Gradient_max =  3.406644517130431 learning rate =  0.0001\n",
      "Epoch =  1027 Batch =  0 Loss =  [9.21798398] Gradient_max =  3.4066439331904466 learning rate =  0.0001\n",
      "Epoch =  1028 Batch =  0 Loss =  [9.21798281] Gradient_max =  3.4066433530937705 learning rate =  0.0001\n",
      "Epoch =  1029 Batch =  0 Loss =  [9.21798165] Gradient_max =  3.406642776814651 learning rate =  0.0001\n",
      "Epoch =  1030 Batch =  0 Loss =  [9.2179805] Gradient_max =  3.406642204327518 learning rate =  0.0001\n",
      "Epoch =  1031 Batch =  0 Loss =  [9.21797936] Gradient_max =  3.4066416356069733 learning rate =  0.0001\n",
      "Epoch =  1032 Batch =  0 Loss =  [9.21797823] Gradient_max =  3.406641070627791 learning rate =  0.0001\n",
      "Epoch =  1033 Batch =  0 Loss =  [9.2179771] Gradient_max =  3.406640509364924 learning rate =  0.0001\n",
      "Epoch =  1034 Batch =  0 Loss =  [9.21797598] Gradient_max =  3.406639951793489 learning rate =  0.0001\n",
      "Epoch =  1035 Batch =  0 Loss =  [9.21797487] Gradient_max =  3.4066393978887795 learning rate =  0.0001\n",
      "Epoch =  1036 Batch =  0 Loss =  [9.21797376] Gradient_max =  3.406638847626257 learning rate =  0.0001\n",
      "Epoch =  1037 Batch =  0 Loss =  [9.21797266] Gradient_max =  3.406638300981544 learning rate =  0.0001\n",
      "Epoch =  1038 Batch =  0 Loss =  [9.21797157] Gradient_max =  3.406637757930446 learning rate =  0.0001\n",
      "Epoch =  1039 Batch =  0 Loss =  [9.21797049] Gradient_max =  3.4066372184489153 learning rate =  0.0001\n",
      "Epoch =  1040 Batch =  0 Loss =  [9.21796941] Gradient_max =  3.406636682513081 learning rate =  0.0001\n",
      "Epoch =  1041 Batch =  0 Loss =  [9.21796835] Gradient_max =  3.406636150099232 learning rate =  0.0001\n",
      "Epoch =  1042 Batch =  0 Loss =  [9.21796729] Gradient_max =  3.406635621183821 learning rate =  0.0001\n",
      "Epoch =  1043 Batch =  0 Loss =  [9.21796623] Gradient_max =  3.4066350957434555 learning rate =  0.0001\n",
      "Epoch =  1044 Batch =  0 Loss =  [9.21796518] Gradient_max =  3.4066345737549097 learning rate =  0.0001\n",
      "Epoch =  1045 Batch =  0 Loss =  [9.21796414] Gradient_max =  3.406634055195119 learning rate =  0.0001\n",
      "Epoch =  1046 Batch =  0 Loss =  [9.21796311] Gradient_max =  3.406633540041167 learning rate =  0.0001\n",
      "Epoch =  1047 Batch =  0 Loss =  [9.21796209] Gradient_max =  3.4066330282702992 learning rate =  0.0001\n",
      "Epoch =  1048 Batch =  0 Loss =  [9.21796107] Gradient_max =  3.406632519859915 learning rate =  0.0001\n",
      "Epoch =  1049 Batch =  0 Loss =  [9.21796005] Gradient_max =  3.406632014787574 learning rate =  0.0001\n",
      "Epoch =  1050 Batch =  0 Loss =  [9.21795905] Gradient_max =  3.406631513030982 learning rate =  0.0001\n",
      "Epoch =  1051 Batch =  0 Loss =  [9.21795805] Gradient_max =  3.4066310145679966 learning rate =  0.0001\n",
      "Epoch =  1052 Batch =  0 Loss =  [9.21795706] Gradient_max =  3.406630519376633 learning rate =  0.0001\n",
      "Epoch =  1053 Batch =  0 Loss =  [9.21795607] Gradient_max =  3.406630027435051 learning rate =  0.0001\n",
      "Epoch =  1054 Batch =  0 Loss =  [9.21795509] Gradient_max =  3.406629538721564 learning rate =  0.0001\n",
      "Epoch =  1055 Batch =  0 Loss =  [9.21795412] Gradient_max =  3.406629053214627 learning rate =  0.0001\n",
      "Epoch =  1056 Batch =  0 Loss =  [9.21795315] Gradient_max =  3.4066285708928477 learning rate =  0.0001\n",
      "Epoch =  1057 Batch =  0 Loss =  [9.21795219] Gradient_max =  3.4066280917349774 learning rate =  0.0001\n",
      "Epoch =  1058 Batch =  0 Loss =  [9.21795124] Gradient_max =  3.4066276157199122 learning rate =  0.0001\n",
      "Epoch =  1059 Batch =  0 Loss =  [9.21795029] Gradient_max =  3.406627142826693 learning rate =  0.0001\n",
      "Epoch =  1060 Batch =  0 Loss =  [9.21794935] Gradient_max =  3.406626673034502 learning rate =  0.0001\n",
      "Epoch =  1061 Batch =  0 Loss =  [9.21794842] Gradient_max =  3.406626206322665 learning rate =  0.0001\n",
      "Epoch =  1062 Batch =  0 Loss =  [9.21794749] Gradient_max =  3.4066257426706485 learning rate =  0.0001\n",
      "Epoch =  1063 Batch =  0 Loss =  [9.21794657] Gradient_max =  3.4066252820580574 learning rate =  0.0001\n",
      "Epoch =  1064 Batch =  0 Loss =  [9.21794565] Gradient_max =  3.4066248244646347 learning rate =  0.0001\n",
      "Epoch =  1065 Batch =  0 Loss =  [9.21794474] Gradient_max =  3.406624369870267 learning rate =  0.0001\n",
      "Epoch =  1066 Batch =  0 Loss =  [9.21794384] Gradient_max =  3.40662391825497 learning rate =  0.0001\n",
      "Epoch =  1067 Batch =  0 Loss =  [9.21794294] Gradient_max =  3.406623469598901 learning rate =  0.0001\n",
      "Epoch =  1068 Batch =  0 Loss =  [9.21794205] Gradient_max =  3.4066230238823496 learning rate =  0.0001\n",
      "Epoch =  1069 Batch =  0 Loss =  [9.21794117] Gradient_max =  3.406622581085742 learning rate =  0.0001\n",
      "Epoch =  1070 Batch =  0 Loss =  [9.21794029] Gradient_max =  3.4066221411896356 learning rate =  0.0001\n",
      "Epoch =  1071 Batch =  0 Loss =  [9.21793941] Gradient_max =  3.4066217041747198 learning rate =  0.0001\n",
      "Epoch =  1072 Batch =  0 Loss =  [9.21793854] Gradient_max =  3.406621270021814 learning rate =  0.0001\n",
      "Epoch =  1073 Batch =  0 Loss =  [9.21793768] Gradient_max =  3.4066208387118753 learning rate =  0.0001\n",
      "Epoch =  1074 Batch =  0 Loss =  [9.21793683] Gradient_max =  3.4066204102259814 learning rate =  0.0001\n",
      "Epoch =  1075 Batch =  0 Loss =  [9.21793597] Gradient_max =  3.406619984545343 learning rate =  0.0001\n",
      "Epoch =  1076 Batch =  0 Loss =  [9.21793513] Gradient_max =  3.4066195616512975 learning rate =  0.0001\n",
      "Epoch =  1077 Batch =  0 Loss =  [9.21793429] Gradient_max =  3.406619141525307 learning rate =  0.0001\n",
      "Epoch =  1078 Batch =  0 Loss =  [9.21793346] Gradient_max =  3.406618724148968 learning rate =  0.0001\n",
      "Epoch =  1079 Batch =  0 Loss =  [9.21793263] Gradient_max =  3.4066183095039886 learning rate =  0.0001\n",
      "Epoch =  1080 Batch =  0 Loss =  [9.2179318] Gradient_max =  3.406617897572212 learning rate =  0.0001\n",
      "Epoch =  1081 Batch =  0 Loss =  [9.21793099] Gradient_max =  3.4066174883355997 learning rate =  0.0001\n",
      "Epoch =  1082 Batch =  0 Loss =  [9.21793018] Gradient_max =  3.406617081776238 learning rate =  0.0001\n",
      "Epoch =  1083 Batch =  0 Loss =  [9.21792937] Gradient_max =  3.406616677876333 learning rate =  0.0001\n",
      "Epoch =  1084 Batch =  0 Loss =  [9.21792857] Gradient_max =  3.4066162766182106 learning rate =  0.0001\n",
      "Epoch =  1085 Batch =  0 Loss =  [9.21792777] Gradient_max =  3.406615877984321 learning rate =  0.0001\n",
      "Epoch =  1086 Batch =  0 Loss =  [9.21792698] Gradient_max =  3.4066154819572296 learning rate =  0.0001\n",
      "Epoch =  1087 Batch =  0 Loss =  [9.2179262] Gradient_max =  3.4066150885196187 learning rate =  0.0001\n",
      "Epoch =  1088 Batch =  0 Loss =  [9.21792542] Gradient_max =  3.4066146976542955 learning rate =  0.0001\n",
      "Epoch =  1089 Batch =  0 Loss =  [9.21792464] Gradient_max =  3.406614309344172 learning rate =  0.0001\n",
      "Epoch =  1090 Batch =  0 Loss =  [9.21792387] Gradient_max =  3.406613923572288 learning rate =  0.0001\n",
      "Epoch =  1091 Batch =  0 Loss =  [9.21792311] Gradient_max =  3.406613540321788 learning rate =  0.0001\n",
      "Epoch =  1092 Batch =  0 Loss =  [9.21792235] Gradient_max =  3.4066131595759384 learning rate =  0.0001\n",
      "Epoch =  1093 Batch =  0 Loss =  [9.21792159] Gradient_max =  3.4066127813181186 learning rate =  0.0001\n",
      "Epoch =  1094 Batch =  0 Loss =  [9.21792084] Gradient_max =  3.4066124055318103 learning rate =  0.0001\n",
      "Epoch =  1095 Batch =  0 Loss =  [9.2179201] Gradient_max =  3.406612032200622 learning rate =  0.0001\n",
      "Epoch =  1096 Batch =  0 Loss =  [9.21791936] Gradient_max =  3.4066116613082627 learning rate =  0.0001\n",
      "Epoch =  1097 Batch =  0 Loss =  [9.21791862] Gradient_max =  3.4066112928385555 learning rate =  0.0001\n",
      "Epoch =  1098 Batch =  0 Loss =  [9.21791789] Gradient_max =  3.4066109267754334 learning rate =  0.0001\n",
      "Epoch =  1099 Batch =  0 Loss =  [9.21791717] Gradient_max =  3.406610563102935 learning rate =  0.0001\n",
      "Epoch =  1100 Batch =  0 Loss =  [9.21791645] Gradient_max =  3.4066102018052096 learning rate =  0.0001\n",
      "Epoch =  1101 Batch =  0 Loss =  [9.21791573] Gradient_max =  3.406609842866514 learning rate =  0.0001\n",
      "Epoch =  1102 Batch =  0 Loss =  [9.21791502] Gradient_max =  3.406609486271208 learning rate =  0.0001\n",
      "Epoch =  1103 Batch =  0 Loss =  [9.21791431] Gradient_max =  3.4066091320037635 learning rate =  0.0001\n",
      "Epoch =  1104 Batch =  0 Loss =  [9.21791361] Gradient_max =  3.406608780048749 learning rate =  0.0001\n",
      "Epoch =  1105 Batch =  0 Loss =  [9.21791292] Gradient_max =  3.4066084303908477 learning rate =  0.0001\n",
      "Epoch =  1106 Batch =  0 Loss =  [9.21791222] Gradient_max =  3.4066080830148366 learning rate =  0.0001\n",
      "Epoch =  1107 Batch =  0 Loss =  [9.21791154] Gradient_max =  3.4066077379056 learning rate =  0.0001\n",
      "Epoch =  1108 Batch =  0 Loss =  [9.21791085] Gradient_max =  3.406607395048124 learning rate =  0.0001\n",
      "Epoch =  1109 Batch =  0 Loss =  [9.21791017] Gradient_max =  3.4066070544274956 learning rate =  0.0001\n",
      "Epoch =  1110 Batch =  0 Loss =  [9.2179095] Gradient_max =  3.4066067160289024 learning rate =  0.0001\n",
      "Epoch =  1111 Batch =  0 Loss =  [9.21790883] Gradient_max =  3.4066063798376334 learning rate =  0.0001\n",
      "Epoch =  1112 Batch =  0 Loss =  [9.21790817] Gradient_max =  3.4066060458390766 learning rate =  0.0001\n",
      "Epoch =  1113 Batch =  0 Loss =  [9.2179075] Gradient_max =  3.4066057140187143 learning rate =  0.0001\n",
      "Epoch =  1114 Batch =  0 Loss =  [9.21790685] Gradient_max =  3.406605384362135 learning rate =  0.0001\n",
      "Epoch =  1115 Batch =  0 Loss =  [9.2179062] Gradient_max =  3.406605056855018 learning rate =  0.0001\n",
      "Epoch =  1116 Batch =  0 Loss =  [9.21790555] Gradient_max =  3.4066047314831396 learning rate =  0.0001\n",
      "Epoch =  1117 Batch =  0 Loss =  [9.2179049] Gradient_max =  3.4066044082323734 learning rate =  0.0001\n",
      "Epoch =  1118 Batch =  0 Loss =  [9.21790427] Gradient_max =  3.406604087088693 learning rate =  0.0001\n",
      "Epoch =  1119 Batch =  0 Loss =  [9.21790363] Gradient_max =  3.4066037680381567 learning rate =  0.0001\n",
      "Epoch =  1120 Batch =  0 Loss =  [9.217903] Gradient_max =  3.4066034510669234 learning rate =  0.0001\n",
      "Epoch =  1121 Batch =  0 Loss =  [9.21790237] Gradient_max =  3.4066031361612463 learning rate =  0.0001\n",
      "Epoch =  1122 Batch =  0 Loss =  [9.21790175] Gradient_max =  3.406602823307467 learning rate =  0.0001\n",
      "Epoch =  1123 Batch =  0 Loss =  [9.21790113] Gradient_max =  3.40660251249202 learning rate =  0.0001\n",
      "Epoch =  1124 Batch =  0 Loss =  [9.21790052] Gradient_max =  3.4066022037014334 learning rate =  0.0001\n",
      "Epoch =  1125 Batch =  0 Loss =  [9.21789991] Gradient_max =  3.4066018969223246 learning rate =  0.0001\n",
      "Epoch =  1126 Batch =  0 Loss =  [9.2178993] Gradient_max =  3.406601592141403 learning rate =  0.0001\n",
      "Epoch =  1127 Batch =  0 Loss =  [9.2178987] Gradient_max =  3.4066012893454634 learning rate =  0.0001\n",
      "Epoch =  1128 Batch =  0 Loss =  [9.2178981] Gradient_max =  3.4066009885213937 learning rate =  0.0001\n",
      "Epoch =  1129 Batch =  0 Loss =  [9.21789751] Gradient_max =  3.4066006896561687 learning rate =  0.0001\n",
      "Epoch =  1130 Batch =  0 Loss =  [9.21789691] Gradient_max =  3.4066003927368476 learning rate =  0.0001\n",
      "Epoch =  1131 Batch =  0 Loss =  [9.21789633] Gradient_max =  3.4066000977505815 learning rate =  0.0001\n",
      "Epoch =  1132 Batch =  0 Loss =  [9.21789575] Gradient_max =  3.4065998046846064 learning rate =  0.0001\n",
      "Epoch =  1133 Batch =  0 Loss =  [9.21789517] Gradient_max =  3.4065995135262437 learning rate =  0.0001\n",
      "Epoch =  1134 Batch =  0 Loss =  [9.21789459] Gradient_max =  3.406599224262899 learning rate =  0.0001\n",
      "Epoch =  1135 Batch =  0 Loss =  [9.21789402] Gradient_max =  3.406598936882066 learning rate =  0.0001\n",
      "Epoch =  1136 Batch =  0 Loss =  [9.21789345] Gradient_max =  3.40659865137132 learning rate =  0.0001\n",
      "Epoch =  1137 Batch =  0 Loss =  [9.21789289] Gradient_max =  3.406598367718317 learning rate =  0.0001\n",
      "Epoch =  1138 Batch =  0 Loss =  [9.21789233] Gradient_max =  3.406598085910801 learning rate =  0.0001\n",
      "Epoch =  1139 Batch =  0 Loss =  [9.21789177] Gradient_max =  3.406597805936597 learning rate =  0.0001\n",
      "Epoch =  1140 Batch =  0 Loss =  [9.21789122] Gradient_max =  3.4065975277836125 learning rate =  0.0001\n",
      "Epoch =  1141 Batch =  0 Loss =  [9.21789067] Gradient_max =  3.406597251439833 learning rate =  0.0001\n",
      "Epoch =  1142 Batch =  0 Loss =  [9.21789012] Gradient_max =  3.406596976893325 learning rate =  0.0001\n",
      "Epoch =  1143 Batch =  0 Loss =  [9.21788958] Gradient_max =  3.4065967041322405 learning rate =  0.0001\n",
      "Epoch =  1144 Batch =  0 Loss =  [9.21788904] Gradient_max =  3.406596433144803 learning rate =  0.0001\n",
      "Epoch =  1145 Batch =  0 Loss =  [9.21788851] Gradient_max =  3.4065961639193247 learning rate =  0.0001\n",
      "Epoch =  1146 Batch =  0 Loss =  [9.21788798] Gradient_max =  3.4065958964441885 learning rate =  0.0001\n",
      "Epoch =  1147 Batch =  0 Loss =  [9.21788745] Gradient_max =  3.4065956307078546 learning rate =  0.0001\n",
      "Epoch =  1148 Batch =  0 Loss =  [9.21788693] Gradient_max =  3.406595366698868 learning rate =  0.0001\n",
      "Epoch =  1149 Batch =  0 Loss =  [9.2178864] Gradient_max =  3.406595104405845 learning rate =  0.0001\n",
      "Epoch =  1150 Batch =  0 Loss =  [9.21788589] Gradient_max =  3.406594843817482 learning rate =  0.0001\n",
      "Epoch =  1151 Batch =  0 Loss =  [9.21788537] Gradient_max =  3.4065945849225443 learning rate =  0.0001\n",
      "Epoch =  1152 Batch =  0 Loss =  [9.21788486] Gradient_max =  3.4065943277098807 learning rate =  0.0001\n",
      "Epoch =  1153 Batch =  0 Loss =  [9.21788436] Gradient_max =  3.406594072168412 learning rate =  0.0001\n",
      "Epoch =  1154 Batch =  0 Loss =  [9.21788385] Gradient_max =  3.406593818287126 learning rate =  0.0001\n",
      "Epoch =  1155 Batch =  0 Loss =  [9.21788335] Gradient_max =  3.4065935660550983 learning rate =  0.0001\n",
      "Epoch =  1156 Batch =  0 Loss =  [9.21788285] Gradient_max =  3.4065933154614663 learning rate =  0.0001\n",
      "Epoch =  1157 Batch =  0 Loss =  [9.21788236] Gradient_max =  3.4065930664954425 learning rate =  0.0001\n",
      "Epoch =  1158 Batch =  0 Loss =  [9.21788187] Gradient_max =  3.4065928191463186 learning rate =  0.0001\n",
      "Epoch =  1159 Batch =  0 Loss =  [9.21788138] Gradient_max =  3.4065925734034486 learning rate =  0.0001\n",
      "Epoch =  1160 Batch =  0 Loss =  [9.2178809] Gradient_max =  3.4065923292562625 learning rate =  0.0001\n",
      "Epoch =  1161 Batch =  0 Loss =  [9.21788041] Gradient_max =  3.4065920866942587 learning rate =  0.0001\n",
      "Epoch =  1162 Batch =  0 Loss =  [9.21787994] Gradient_max =  3.406591845707013 learning rate =  0.0001\n",
      "Epoch =  1163 Batch =  0 Loss =  [9.21787946] Gradient_max =  3.4065916062841612 learning rate =  0.0001\n",
      "Epoch =  1164 Batch =  0 Loss =  [9.21787899] Gradient_max =  3.406591368415416 learning rate =  0.0001\n",
      "Epoch =  1165 Batch =  0 Loss =  [9.21787852] Gradient_max =  3.406591132090552 learning rate =  0.0001\n",
      "Epoch =  1166 Batch =  0 Loss =  [9.21787805] Gradient_max =  3.4065908972994183 learning rate =  0.0001\n",
      "Epoch =  1167 Batch =  0 Loss =  [9.21787759] Gradient_max =  3.4065906640319312 learning rate =  0.0001\n",
      "Epoch =  1168 Batch =  0 Loss =  [9.21787713] Gradient_max =  3.406590432278071 learning rate =  0.0001\n",
      "Epoch =  1169 Batch =  0 Loss =  [9.21787667] Gradient_max =  3.4065902020278878 learning rate =  0.0001\n",
      "Epoch =  1170 Batch =  0 Loss =  [9.21787622] Gradient_max =  3.4065899732714957 learning rate =  0.0001\n",
      "Epoch =  1171 Batch =  0 Loss =  [9.21787577] Gradient_max =  3.4065897459990793 learning rate =  0.0001\n",
      "Epoch =  1172 Batch =  0 Loss =  [9.21787532] Gradient_max =  3.4065895202008845 learning rate =  0.0001\n",
      "Epoch =  1173 Batch =  0 Loss =  [9.21787488] Gradient_max =  3.4065892958672253 learning rate =  0.0001\n",
      "Epoch =  1174 Batch =  0 Loss =  [9.21787444] Gradient_max =  3.406589072988479 learning rate =  0.0001\n",
      "Epoch =  1175 Batch =  0 Loss =  [9.217874] Gradient_max =  3.4065888515550853 learning rate =  0.0001\n",
      "Epoch =  1176 Batch =  0 Loss =  [9.21787356] Gradient_max =  3.406588631557553 learning rate =  0.0001\n",
      "Epoch =  1177 Batch =  0 Loss =  [9.21787313] Gradient_max =  3.4065884129864497 learning rate =  0.0001\n",
      "Epoch =  1178 Batch =  0 Loss =  [9.2178727] Gradient_max =  3.406588195832407 learning rate =  0.0001\n",
      "Epoch =  1179 Batch =  0 Loss =  [9.21787227] Gradient_max =  3.406587980086122 learning rate =  0.0001\n",
      "Epoch =  1180 Batch =  0 Loss =  [9.21787184] Gradient_max =  3.406587765738348 learning rate =  0.0001\n",
      "Epoch =  1181 Batch =  0 Loss =  [9.21787142] Gradient_max =  3.4065875527799077 learning rate =  0.0001\n",
      "Epoch =  1182 Batch =  0 Loss =  [9.217871] Gradient_max =  3.4065873412016763 learning rate =  0.0001\n",
      "Epoch =  1183 Batch =  0 Loss =  [9.21787059] Gradient_max =  3.4065871309945965 learning rate =  0.0001\n",
      "Epoch =  1184 Batch =  0 Loss =  [9.21787017] Gradient_max =  3.406586922149672 learning rate =  0.0001\n",
      "Epoch =  1185 Batch =  0 Loss =  [9.21786976] Gradient_max =  3.4065867146579594 learning rate =  0.0001\n",
      "Epoch =  1186 Batch =  0 Loss =  [9.21786935] Gradient_max =  3.4065865085105846 learning rate =  0.0001\n",
      "Epoch =  1187 Batch =  0 Loss =  [9.21786895] Gradient_max =  3.4065863036987225 learning rate =  0.0001\n",
      "Epoch =  1188 Batch =  0 Loss =  [9.21786854] Gradient_max =  3.4065861002136146 learning rate =  0.0001\n",
      "Epoch =  1189 Batch =  0 Loss =  [9.21786814] Gradient_max =  3.4065858980465573 learning rate =  0.0001\n",
      "Epoch =  1190 Batch =  0 Loss =  [9.21786774] Gradient_max =  3.4065856971889086 learning rate =  0.0001\n",
      "Epoch =  1191 Batch =  0 Loss =  [9.21786735] Gradient_max =  3.4065854976320775 learning rate =  0.0001\n",
      "Epoch =  1192 Batch =  0 Loss =  [9.21786696] Gradient_max =  3.406585299367538 learning rate =  0.0001\n",
      "Epoch =  1193 Batch =  0 Loss =  [9.21786657] Gradient_max =  3.4065851023868152 learning rate =  0.0001\n",
      "Epoch =  1194 Batch =  0 Loss =  [9.21786618] Gradient_max =  3.4065849066814935 learning rate =  0.0001\n",
      "Epoch =  1195 Batch =  0 Loss =  [9.21786579] Gradient_max =  3.4065847122432107 learning rate =  0.0001\n",
      "Epoch =  1196 Batch =  0 Loss =  [9.21786541] Gradient_max =  3.406584519063665 learning rate =  0.0001\n",
      "Epoch =  1197 Batch =  0 Loss =  [9.21786503] Gradient_max =  3.406584327134604 learning rate =  0.0001\n",
      "Epoch =  1198 Batch =  0 Loss =  [9.21786465] Gradient_max =  3.4065841364478366 learning rate =  0.0001\n",
      "Epoch =  1199 Batch =  0 Loss =  [9.21786428] Gradient_max =  3.4065839469952195 learning rate =  0.0001\n",
      "Epoch =  1200 Batch =  0 Loss =  [9.2178639] Gradient_max =  3.4065837587686683 learning rate =  0.0001\n",
      "Epoch =  1201 Batch =  0 Loss =  [9.21786353] Gradient_max =  3.4065835717601525 learning rate =  0.0001\n",
      "Epoch =  1202 Batch =  0 Loss =  [9.21786317] Gradient_max =  3.4065833859616927 learning rate =  0.0001\n",
      "Epoch =  1203 Batch =  0 Loss =  [9.2178628] Gradient_max =  3.406583201365364 learning rate =  0.0001\n",
      "Epoch =  1204 Batch =  0 Loss =  [9.21786244] Gradient_max =  3.406583017963291 learning rate =  0.0001\n",
      "Epoch =  1205 Batch =  0 Loss =  [9.21786208] Gradient_max =  3.406582835747659 learning rate =  0.0001\n",
      "Epoch =  1206 Batch =  0 Loss =  [9.21786172] Gradient_max =  3.406582654710695 learning rate =  0.0001\n",
      "Epoch =  1207 Batch =  0 Loss =  [9.21786136] Gradient_max =  3.406582474844685 learning rate =  0.0001\n",
      "Epoch =  1208 Batch =  0 Loss =  [9.21786101] Gradient_max =  3.4065822961419654 learning rate =  0.0001\n",
      "Epoch =  1209 Batch =  0 Loss =  [9.21786066] Gradient_max =  3.406582118594918 learning rate =  0.0001\n",
      "Epoch =  1210 Batch =  0 Loss =  [9.21786031] Gradient_max =  3.4065819421959835 learning rate =  0.0001\n",
      "Epoch =  1211 Batch =  0 Loss =  [9.21785996] Gradient_max =  3.406581766937646 learning rate =  0.0001\n",
      "Epoch =  1212 Batch =  0 Loss =  [9.21785961] Gradient_max =  3.4065815928124406 learning rate =  0.0001\n",
      "Epoch =  1213 Batch =  0 Loss =  [9.21785927] Gradient_max =  3.406581419812958 learning rate =  0.0001\n",
      "Epoch =  1214 Batch =  0 Loss =  [9.21785893] Gradient_max =  3.406581247931829 learning rate =  0.0001\n",
      "Epoch =  1215 Batch =  0 Loss =  [9.21785859] Gradient_max =  3.4065810771617424 learning rate =  0.0001\n",
      "Epoch =  1216 Batch =  0 Loss =  [9.21785826] Gradient_max =  3.40658090749543 learning rate =  0.0001\n",
      "Epoch =  1217 Batch =  0 Loss =  [9.21785792] Gradient_max =  3.406580738925671 learning rate =  0.0001\n",
      "Epoch =  1218 Batch =  0 Loss =  [9.21785759] Gradient_max =  3.4065805714452955 learning rate =  0.0001\n",
      "Epoch =  1219 Batch =  0 Loss =  [9.21785726] Gradient_max =  3.406580405047183 learning rate =  0.0001\n",
      "Epoch =  1220 Batch =  0 Loss =  [9.21785694] Gradient_max =  3.406580239724257 learning rate =  0.0001\n",
      "Epoch =  1221 Batch =  0 Loss =  [9.21785661] Gradient_max =  3.406580075469486 learning rate =  0.0001\n",
      "Epoch =  1222 Batch =  0 Loss =  [9.21785629] Gradient_max =  3.4065799122758897 learning rate =  0.0001\n",
      "Epoch =  1223 Batch =  0 Loss =  [9.21785597] Gradient_max =  3.406579750136533 learning rate =  0.0001\n",
      "Epoch =  1224 Batch =  0 Loss =  [9.21785565] Gradient_max =  3.406579589044524 learning rate =  0.0001\n",
      "Epoch =  1225 Batch =  0 Loss =  [9.21785533] Gradient_max =  3.406579428993022 learning rate =  0.0001\n",
      "Epoch =  1226 Batch =  0 Loss =  [9.21785502] Gradient_max =  3.4065792699752233 learning rate =  0.0001\n",
      "Epoch =  1227 Batch =  0 Loss =  [9.21785471] Gradient_max =  3.406579111984379 learning rate =  0.0001\n",
      "Epoch =  1228 Batch =  0 Loss =  [9.21785439] Gradient_max =  3.406578955013779 learning rate =  0.0001\n",
      "Epoch =  1229 Batch =  0 Loss =  [9.21785409] Gradient_max =  3.406578799056758 learning rate =  0.0001\n",
      "Epoch =  1230 Batch =  0 Loss =  [9.21785378] Gradient_max =  3.406578644106696 learning rate =  0.0001\n",
      "Epoch =  1231 Batch =  0 Loss =  [9.21785348] Gradient_max =  3.4065784901570213 learning rate =  0.0001\n",
      "Epoch =  1232 Batch =  0 Loss =  [9.21785317] Gradient_max =  3.4065783372011937 learning rate =  0.0001\n",
      "Epoch =  1233 Batch =  0 Loss =  [9.21785287] Gradient_max =  3.406578185232728 learning rate =  0.0001\n",
      "Epoch =  1234 Batch =  0 Loss =  [9.21785257] Gradient_max =  3.4065780342451792 learning rate =  0.0001\n",
      "Epoch =  1235 Batch =  0 Loss =  [9.21785228] Gradient_max =  3.4065778842321404 learning rate =  0.0001\n",
      "Epoch =  1236 Batch =  0 Loss =  [9.21785198] Gradient_max =  3.4065777351872533 learning rate =  0.0001\n",
      "Epoch =  1237 Batch =  0 Loss =  [9.21785169] Gradient_max =  3.4065775871041977 learning rate =  0.0001\n",
      "Epoch =  1238 Batch =  0 Loss =  [9.2178514] Gradient_max =  3.406577439976694 learning rate =  0.0001\n",
      "Epoch =  1239 Batch =  0 Loss =  [9.21785111] Gradient_max =  3.406577293798513 learning rate =  0.0001\n",
      "Epoch =  1240 Batch =  0 Loss =  [9.21785082] Gradient_max =  3.406577148563455 learning rate =  0.0001\n",
      "Epoch =  1241 Batch =  0 Loss =  [9.21785054] Gradient_max =  3.406577004265367 learning rate =  0.0001\n",
      "Epoch =  1242 Batch =  0 Loss =  [9.21785025] Gradient_max =  3.40657686089814 learning rate =  0.0001\n",
      "Epoch =  1243 Batch =  0 Loss =  [9.21784997] Gradient_max =  3.406576718455702 learning rate =  0.0001\n",
      "Epoch =  1244 Batch =  0 Loss =  [9.21784969] Gradient_max =  3.4065765769320153 learning rate =  0.0001\n",
      "Epoch =  1245 Batch =  0 Loss =  [9.21784941] Gradient_max =  3.406576436321092 learning rate =  0.0001\n",
      "Epoch =  1246 Batch =  0 Loss =  [9.21784914] Gradient_max =  3.406576296616982 learning rate =  0.0001\n",
      "Epoch =  1247 Batch =  0 Loss =  [9.21784886] Gradient_max =  3.406576157813768 learning rate =  0.0001\n",
      "Epoch =  1248 Batch =  0 Loss =  [9.21784859] Gradient_max =  3.406576019905578 learning rate =  0.0001\n",
      "Epoch =  1249 Batch =  0 Loss =  [9.21784832] Gradient_max =  3.406575882886577 learning rate =  0.0001\n",
      "Epoch =  1250 Batch =  0 Loss =  [9.21784805] Gradient_max =  3.406575746750972 learning rate =  0.0001\n",
      "Epoch =  1251 Batch =  0 Loss =  [9.21784778] Gradient_max =  3.4065756114929986 learning rate =  0.0001\n",
      "Epoch =  1252 Batch =  0 Loss =  [9.21784752] Gradient_max =  3.4065754771069408 learning rate =  0.0001\n",
      "Epoch =  1253 Batch =  0 Loss =  [9.21784725] Gradient_max =  3.4065753435871153 learning rate =  0.0001\n",
      "Epoch =  1254 Batch =  0 Loss =  [9.21784699] Gradient_max =  3.4065752109278784 learning rate =  0.0001\n",
      "Epoch =  1255 Batch =  0 Loss =  [9.21784673] Gradient_max =  3.406575079123624 learning rate =  0.0001\n",
      "Epoch =  1256 Batch =  0 Loss =  [9.21784647] Gradient_max =  3.4065749481687773 learning rate =  0.0001\n",
      "Epoch =  1257 Batch =  0 Loss =  [9.21784621] Gradient_max =  3.4065748180578095 learning rate =  0.0001\n",
      "Epoch =  1258 Batch =  0 Loss =  [9.21784596] Gradient_max =  3.4065746887852226 learning rate =  0.0001\n",
      "Epoch =  1259 Batch =  0 Loss =  [9.2178457] Gradient_max =  3.406574560345555 learning rate =  0.0001\n",
      "Epoch =  1260 Batch =  0 Loss =  [9.21784545] Gradient_max =  3.4065744327333833 learning rate =  0.0001\n",
      "Epoch =  1261 Batch =  0 Loss =  [9.2178452] Gradient_max =  3.4065743059433173 learning rate =  0.0001\n",
      "Epoch =  1262 Batch =  0 Loss =  [9.21784495] Gradient_max =  3.4065741799700056 learning rate =  0.0001\n",
      "Epoch =  1263 Batch =  0 Loss =  [9.21784471] Gradient_max =  3.406574054808132 learning rate =  0.0001\n",
      "Epoch =  1264 Batch =  0 Loss =  [9.21784446] Gradient_max =  3.40657393045241 learning rate =  0.0001\n",
      "Epoch =  1265 Batch =  0 Loss =  [9.21784422] Gradient_max =  3.4065738068975953 learning rate =  0.0001\n",
      "Epoch =  1266 Batch =  0 Loss =  [9.21784397] Gradient_max =  3.4065736841384737 learning rate =  0.0001\n",
      "Epoch =  1267 Batch =  0 Loss =  [9.21784373] Gradient_max =  3.4065735621698643 learning rate =  0.0001\n",
      "Epoch =  1268 Batch =  0 Loss =  [9.21784349] Gradient_max =  3.406573440986625 learning rate =  0.0001\n",
      "Epoch =  1269 Batch =  0 Loss =  [9.21784325] Gradient_max =  3.4065733205836466 learning rate =  0.0001\n",
      "Epoch =  1270 Batch =  0 Loss =  [9.21784302] Gradient_max =  3.406573200955849 learning rate =  0.0001\n",
      "Epoch =  1271 Batch =  0 Loss =  [9.21784278] Gradient_max =  3.4065730820981908 learning rate =  0.0001\n",
      "Epoch =  1272 Batch =  0 Loss =  [9.21784255] Gradient_max =  3.406572964005661 learning rate =  0.0001\n",
      "Epoch =  1273 Batch =  0 Loss =  [9.21784232] Gradient_max =  3.406572846673285 learning rate =  0.0001\n",
      "Epoch =  1274 Batch =  0 Loss =  [9.21784209] Gradient_max =  3.406572730096116 learning rate =  0.0001\n",
      "Epoch =  1275 Batch =  0 Loss =  [9.21784186] Gradient_max =  3.406572614269243 learning rate =  0.0001\n",
      "Epoch =  1276 Batch =  0 Loss =  [9.21784163] Gradient_max =  3.406572499187787 learning rate =  0.0001\n",
      "Epoch =  1277 Batch =  0 Loss =  [9.2178414] Gradient_max =  3.4065723848469003 learning rate =  0.0001\n",
      "Epoch =  1278 Batch =  0 Loss =  [9.21784118] Gradient_max =  3.406572271241768 learning rate =  0.0001\n",
      "Epoch =  1279 Batch =  0 Loss =  [9.21784096] Gradient_max =  3.406572158367609 learning rate =  0.0001\n",
      "Epoch =  1280 Batch =  0 Loss =  [9.21784074] Gradient_max =  3.406572046219669 learning rate =  0.0001\n",
      "Epoch =  1281 Batch =  0 Loss =  [9.21784052] Gradient_max =  3.406571934793227 learning rate =  0.0001\n",
      "Epoch =  1282 Batch =  0 Loss =  [9.2178403] Gradient_max =  3.4065718240835934 learning rate =  0.0001\n",
      "Epoch =  1283 Batch =  0 Loss =  [9.21784008] Gradient_max =  3.4065717140861125 learning rate =  0.0001\n",
      "Epoch =  1284 Batch =  0 Loss =  [9.21783986] Gradient_max =  3.406571604796155 learning rate =  0.0001\n",
      "Epoch =  1285 Batch =  0 Loss =  [9.21783965] Gradient_max =  3.406571496209122 learning rate =  0.0001\n",
      "Epoch =  1286 Batch =  0 Loss =  [9.21783944] Gradient_max =  3.406571388320449 learning rate =  0.0001\n",
      "Epoch =  1287 Batch =  0 Loss =  [9.21783922] Gradient_max =  3.4065712811255966 learning rate =  0.0001\n",
      "Epoch =  1288 Batch =  0 Loss =  [9.21783901] Gradient_max =  3.406571174620058 learning rate =  0.0001\n",
      "Epoch =  1289 Batch =  0 Loss =  [9.2178388] Gradient_max =  3.4065710687993573 learning rate =  0.0001\n",
      "Epoch =  1290 Batch =  0 Loss =  [9.2178386] Gradient_max =  3.406570963659043 learning rate =  0.0001\n",
      "Epoch =  1291 Batch =  0 Loss =  [9.21783839] Gradient_max =  3.4065708591947015 learning rate =  0.0001\n",
      "Epoch =  1292 Batch =  0 Loss =  [9.21783819] Gradient_max =  3.4065707554019395 learning rate =  0.0001\n",
      "Epoch =  1293 Batch =  0 Loss =  [9.21783798] Gradient_max =  3.4065706522763977 learning rate =  0.0001\n",
      "Epoch =  1294 Batch =  0 Loss =  [9.21783778] Gradient_max =  3.4065705498137424 learning rate =  0.0001\n",
      "Epoch =  1295 Batch =  0 Loss =  [9.21783758] Gradient_max =  3.4065704480096706 learning rate =  0.0001\n",
      "Epoch =  1296 Batch =  0 Loss =  [9.21783738] Gradient_max =  3.4065703468599082 learning rate =  0.0001\n",
      "Epoch =  1297 Batch =  0 Loss =  [9.21783718] Gradient_max =  3.4065702463602077 learning rate =  0.0001\n",
      "Epoch =  1298 Batch =  0 Loss =  [9.21783698] Gradient_max =  3.406570146506347 learning rate =  0.0001\n",
      "Epoch =  1299 Batch =  0 Loss =  [9.21783679] Gradient_max =  3.4065700472941383 learning rate =  0.0001\n",
      "Epoch =  1300 Batch =  0 Loss =  [9.21783659] Gradient_max =  3.4065699487194148 learning rate =  0.0001\n",
      "Epoch =  1301 Batch =  0 Loss =  [9.2178364] Gradient_max =  3.4065698507780424 learning rate =  0.0001\n",
      "Epoch =  1302 Batch =  0 Loss =  [9.21783621] Gradient_max =  3.4065697534659094 learning rate =  0.0001\n",
      "Epoch =  1303 Batch =  0 Loss =  [9.21783602] Gradient_max =  3.4065696567789354 learning rate =  0.0001\n",
      "Epoch =  1304 Batch =  0 Loss =  [9.21783583] Gradient_max =  3.406569560713065 learning rate =  0.0001\n",
      "Epoch =  1305 Batch =  0 Loss =  [9.21783564] Gradient_max =  3.406569465264266 learning rate =  0.0001\n",
      "Epoch =  1306 Batch =  0 Loss =  [9.21783545] Gradient_max =  3.4065693704285387 learning rate =  0.0001\n",
      "Epoch =  1307 Batch =  0 Loss =  [9.21783526] Gradient_max =  3.406569276201909 learning rate =  0.0001\n",
      "Epoch =  1308 Batch =  0 Loss =  [9.21783508] Gradient_max =  3.4065691825804216 learning rate =  0.0001\n",
      "Epoch =  1309 Batch =  0 Loss =  [9.21783489] Gradient_max =  3.406569089560156 learning rate =  0.0001\n",
      "Epoch =  1310 Batch =  0 Loss =  [9.21783471] Gradient_max =  3.406568997137211 learning rate =  0.0001\n",
      "Epoch =  1311 Batch =  0 Loss =  [9.21783453] Gradient_max =  3.4065689053077173 learning rate =  0.0001\n",
      "Epoch =  1312 Batch =  0 Loss =  [9.21783435] Gradient_max =  3.406568814067826 learning rate =  0.0001\n",
      "Epoch =  1313 Batch =  0 Loss =  [9.21783417] Gradient_max =  3.4065687234137143 learning rate =  0.0001\n",
      "Epoch =  1314 Batch =  0 Loss =  [9.21783399] Gradient_max =  3.4065686333415894 learning rate =  0.0001\n",
      "Epoch =  1315 Batch =  0 Loss =  [9.21783382] Gradient_max =  3.4065685438476727 learning rate =  0.0001\n",
      "Epoch =  1316 Batch =  0 Loss =  [9.21783364] Gradient_max =  3.4065684549282187 learning rate =  0.0001\n",
      "Epoch =  1317 Batch =  0 Loss =  [9.21783347] Gradient_max =  3.406568366579508 learning rate =  0.0001\n",
      "Epoch =  1318 Batch =  0 Loss =  [9.21783329] Gradient_max =  3.406568278797842 learning rate =  0.0001\n",
      "Epoch =  1319 Batch =  0 Loss =  [9.21783312] Gradient_max =  3.406568191579544 learning rate =  0.0001\n",
      "Epoch =  1320 Batch =  0 Loss =  [9.21783295] Gradient_max =  3.4065681049209653 learning rate =  0.0001\n",
      "Epoch =  1321 Batch =  0 Loss =  [9.21783278] Gradient_max =  3.4065680188184806 learning rate =  0.0001\n",
      "Epoch =  1322 Batch =  0 Loss =  [9.21783261] Gradient_max =  3.4065679332684846 learning rate =  0.0001\n",
      "Epoch =  1323 Batch =  0 Loss =  [9.21783244] Gradient_max =  3.406567848267405 learning rate =  0.0001\n",
      "Epoch =  1324 Batch =  0 Loss =  [9.21783228] Gradient_max =  3.406567763811679 learning rate =  0.0001\n",
      "Epoch =  1325 Batch =  0 Loss =  [9.21783211] Gradient_max =  3.40656767989778 learning rate =  0.0001\n",
      "Epoch =  1326 Batch =  0 Loss =  [9.21783195] Gradient_max =  3.406567596522199 learning rate =  0.0001\n",
      "Epoch =  1327 Batch =  0 Loss =  [9.21783178] Gradient_max =  3.406567513681448 learning rate =  0.0001\n",
      "Epoch =  1328 Batch =  0 Loss =  [9.21783162] Gradient_max =  3.4065674313720655 learning rate =  0.0001\n",
      "Epoch =  1329 Batch =  0 Loss =  [9.21783146] Gradient_max =  3.4065673495906124 learning rate =  0.0001\n",
      "Epoch =  1330 Batch =  0 Loss =  [9.2178313] Gradient_max =  3.406567268333672 learning rate =  0.0001\n",
      "Epoch =  1331 Batch =  0 Loss =  [9.21783114] Gradient_max =  3.4065671875978443 learning rate =  0.0001\n",
      "Epoch =  1332 Batch =  0 Loss =  [9.21783098] Gradient_max =  3.406567107379762 learning rate =  0.0001\n",
      "Epoch =  1333 Batch =  0 Loss =  [9.21783082] Gradient_max =  3.406567027676069 learning rate =  0.0001\n",
      "Epoch =  1334 Batch =  0 Loss =  [9.21783067] Gradient_max =  3.4065669484834427 learning rate =  0.0001\n",
      "Epoch =  1335 Batch =  0 Loss =  [9.21783051] Gradient_max =  3.406566869798571 learning rate =  0.0001\n",
      "Epoch =  1336 Batch =  0 Loss =  [9.21783036] Gradient_max =  3.4065667916181703 learning rate =  0.0001\n",
      "Epoch =  1337 Batch =  0 Loss =  [9.2178302] Gradient_max =  3.4065667139389753 learning rate =  0.0001\n",
      "Epoch =  1338 Batch =  0 Loss =  [9.21783005] Gradient_max =  3.4065666367577463 learning rate =  0.0001\n",
      "Epoch =  1339 Batch =  0 Loss =  [9.2178299] Gradient_max =  3.4065665600712607 learning rate =  0.0001\n",
      "Epoch =  1340 Batch =  0 Loss =  [9.21782975] Gradient_max =  3.4065664838763157 learning rate =  0.0001\n",
      "Epoch =  1341 Batch =  0 Loss =  [9.2178296] Gradient_max =  3.406566408169737 learning rate =  0.0001\n",
      "Epoch =  1342 Batch =  0 Loss =  [9.21782945] Gradient_max =  3.4065663329483633 learning rate =  0.0001\n",
      "Epoch =  1343 Batch =  0 Loss =  [9.2178293] Gradient_max =  3.4065662582090535 learning rate =  0.0001\n",
      "Epoch =  1344 Batch =  0 Loss =  [9.21782916] Gradient_max =  3.406566183948698 learning rate =  0.0001\n",
      "Epoch =  1345 Batch =  0 Loss =  [9.21782901] Gradient_max =  3.406566110164193 learning rate =  0.0001\n",
      "Epoch =  1346 Batch =  0 Loss =  [9.21782887] Gradient_max =  3.406566036852466 learning rate =  0.0001\n",
      "Epoch =  1347 Batch =  0 Loss =  [9.21782872] Gradient_max =  3.406565964010458 learning rate =  0.0001\n",
      "Epoch =  1348 Batch =  0 Loss =  [9.21782858] Gradient_max =  3.4065658916351333 learning rate =  0.0001\n",
      "Epoch =  1349 Batch =  0 Loss =  [9.21782844] Gradient_max =  3.4065658197234763 learning rate =  0.0001\n",
      "Epoch =  1350 Batch =  0 Loss =  [9.2178283] Gradient_max =  3.4065657482724903 learning rate =  0.0001\n",
      "Epoch =  1351 Batch =  0 Loss =  [9.21782816] Gradient_max =  3.4065656772791955 learning rate =  0.0001\n",
      "Epoch =  1352 Batch =  0 Loss =  [9.21782802] Gradient_max =  3.406565606740637 learning rate =  0.0001\n",
      "Epoch =  1353 Batch =  0 Loss =  [9.21782788] Gradient_max =  3.4065655366538756 learning rate =  0.0001\n",
      "Epoch =  1354 Batch =  0 Loss =  [9.21782774] Gradient_max =  3.406565467015991 learning rate =  0.0001\n",
      "Epoch =  1355 Batch =  0 Loss =  [9.2178276] Gradient_max =  3.406565397824082 learning rate =  0.0001\n",
      "Epoch =  1356 Batch =  0 Loss =  [9.21782747] Gradient_max =  3.4065653290752684 learning rate =  0.0001\n",
      "Epoch =  1357 Batch =  0 Loss =  [9.21782733] Gradient_max =  3.4065652607666896 learning rate =  0.0001\n",
      "Epoch =  1358 Batch =  0 Loss =  [9.2178272] Gradient_max =  3.406565192895501 learning rate =  0.0001\n",
      "Epoch =  1359 Batch =  0 Loss =  [9.21782707] Gradient_max =  3.4065651254588762 learning rate =  0.0001\n",
      "Epoch =  1360 Batch =  0 Loss =  [9.21782693] Gradient_max =  3.4065650584540097 learning rate =  0.0001\n",
      "Epoch =  1361 Batch =  0 Loss =  [9.2178268] Gradient_max =  3.4065649918781102 learning rate =  0.0001\n",
      "Epoch =  1362 Batch =  0 Loss =  [9.21782667] Gradient_max =  3.406564925728412 learning rate =  0.0001\n",
      "Epoch =  1363 Batch =  0 Loss =  [9.21782654] Gradient_max =  3.406564860002162 learning rate =  0.0001\n",
      "Epoch =  1364 Batch =  0 Loss =  [9.21782641] Gradient_max =  3.4065647946966218 learning rate =  0.0001\n",
      "Epoch =  1365 Batch =  0 Loss =  [9.21782628] Gradient_max =  3.4065647298090798 learning rate =  0.0001\n",
      "Epoch =  1366 Batch =  0 Loss =  [9.21782616] Gradient_max =  3.40656466533684 learning rate =  0.0001\n",
      "Epoch =  1367 Batch =  0 Loss =  [9.21782603] Gradient_max =  3.406564601277213 learning rate =  0.0001\n",
      "Epoch =  1368 Batch =  0 Loss =  [9.2178259] Gradient_max =  3.4065645376275415 learning rate =  0.0001\n",
      "Epoch =  1369 Batch =  0 Loss =  [9.21782578] Gradient_max =  3.406564474385181 learning rate =  0.0001\n",
      "Epoch =  1370 Batch =  0 Loss =  [9.21782566] Gradient_max =  3.406564411547498 learning rate =  0.0001\n",
      "Epoch =  1371 Batch =  0 Loss =  [9.21782553] Gradient_max =  3.4065643491118824 learning rate =  0.0001\n",
      "Epoch =  1372 Batch =  0 Loss =  [9.21782541] Gradient_max =  3.406564287075741 learning rate =  0.0001\n",
      "Epoch =  1373 Batch =  0 Loss =  [9.21782529] Gradient_max =  3.4065642254364943 learning rate =  0.0001\n",
      "Epoch =  1374 Batch =  0 Loss =  [9.21782517] Gradient_max =  3.4065641641915847 learning rate =  0.0001\n",
      "Epoch =  1375 Batch =  0 Loss =  [9.21782505] Gradient_max =  3.4065641033384626 learning rate =  0.0001\n",
      "Epoch =  1376 Batch =  0 Loss =  [9.21782493] Gradient_max =  3.406564042874606 learning rate =  0.0001\n",
      "Epoch =  1377 Batch =  0 Loss =  [9.21782481] Gradient_max =  3.4065639827975014 learning rate =  0.0001\n",
      "Epoch =  1378 Batch =  0 Loss =  [9.21782469] Gradient_max =  3.4065639231046565 learning rate =  0.0001\n",
      "Epoch =  1379 Batch =  0 Loss =  [9.21782457] Gradient_max =  3.4065638637935893 learning rate =  0.0001\n",
      "Epoch =  1380 Batch =  0 Loss =  [9.21782446] Gradient_max =  3.406563804861841 learning rate =  0.0001\n",
      "Epoch =  1381 Batch =  0 Loss =  [9.21782434] Gradient_max =  3.4065637463069622 learning rate =  0.0001\n",
      "Epoch =  1382 Batch =  0 Loss =  [9.21782423] Gradient_max =  3.4065636881265258 learning rate =  0.0001\n",
      "Epoch =  1383 Batch =  0 Loss =  [9.21782411] Gradient_max =  3.4065636303181135 learning rate =  0.0001\n",
      "Epoch =  1384 Batch =  0 Loss =  [9.217824] Gradient_max =  3.406563572879332 learning rate =  0.0001\n",
      "Epoch =  1385 Batch =  0 Loss =  [9.21782389] Gradient_max =  3.406563515807796 learning rate =  0.0001\n",
      "Epoch =  1386 Batch =  0 Loss =  [9.21782377] Gradient_max =  3.4065634591011356 learning rate =  0.0001\n",
      "Epoch =  1387 Batch =  0 Loss =  [9.21782366] Gradient_max =  3.4065634027570026 learning rate =  0.0001\n",
      "Epoch =  1388 Batch =  0 Loss =  [9.21782355] Gradient_max =  3.4065633467730576 learning rate =  0.0001\n",
      "Epoch =  1389 Batch =  0 Loss =  [9.21782344] Gradient_max =  3.4065632911469796 learning rate =  0.0001\n",
      "Epoch =  1390 Batch =  0 Loss =  [9.21782333] Gradient_max =  3.406563235876463 learning rate =  0.0001\n",
      "Epoch =  1391 Batch =  0 Loss =  [9.21782323] Gradient_max =  3.4065631809592163 learning rate =  0.0001\n",
      "Epoch =  1392 Batch =  0 Loss =  [9.21782312] Gradient_max =  3.4065631263929634 learning rate =  0.0001\n",
      "Epoch =  1393 Batch =  0 Loss =  [9.21782301] Gradient_max =  3.4065630721754414 learning rate =  0.0001\n",
      "Epoch =  1394 Batch =  0 Loss =  [9.2178229] Gradient_max =  3.4065630183044058 learning rate =  0.0001\n",
      "Epoch =  1395 Batch =  0 Loss =  [9.2178228] Gradient_max =  3.406562964777622 learning rate =  0.0001\n",
      "Epoch =  1396 Batch =  0 Loss =  [9.21782269] Gradient_max =  3.4065629115928733 learning rate =  0.0001\n",
      "Epoch =  1397 Batch =  0 Loss =  [9.21782259] Gradient_max =  3.4065628587479555 learning rate =  0.0001\n",
      "Epoch =  1398 Batch =  0 Loss =  [9.21782248] Gradient_max =  3.4065628062406805 learning rate =  0.0001\n",
      "Epoch =  1399 Batch =  0 Loss =  [9.21782238] Gradient_max =  3.4065627540688754 learning rate =  0.0001\n",
      "Epoch =  1400 Batch =  0 Loss =  [9.21782228] Gradient_max =  3.406562702230378 learning rate =  0.0001\n",
      "Epoch =  1401 Batch =  0 Loss =  [9.21782218] Gradient_max =  3.40656265072304 learning rate =  0.0001\n",
      "Epoch =  1402 Batch =  0 Loss =  [9.21782208] Gradient_max =  3.4065625995447317 learning rate =  0.0001\n",
      "Epoch =  1403 Batch =  0 Loss =  [9.21782198] Gradient_max =  3.406562548693333 learning rate =  0.0001\n",
      "Epoch =  1404 Batch =  0 Loss =  [9.21782188] Gradient_max =  3.4065624981667395 learning rate =  0.0001\n",
      "Epoch =  1405 Batch =  0 Loss =  [9.21782178] Gradient_max =  3.4065624479628607 learning rate =  0.0001\n",
      "Epoch =  1406 Batch =  0 Loss =  [9.21782168] Gradient_max =  3.406562398079618 learning rate =  0.0001\n",
      "Epoch =  1407 Batch =  0 Loss =  [9.21782158] Gradient_max =  3.406562348514947 learning rate =  0.0001\n",
      "Epoch =  1408 Batch =  0 Loss =  [9.21782148] Gradient_max =  3.406562299266799 learning rate =  0.0001\n",
      "Epoch =  1409 Batch =  0 Loss =  [9.21782139] Gradient_max =  3.4065622503331356 learning rate =  0.0001\n",
      "Epoch =  1410 Batch =  0 Loss =  [9.21782129] Gradient_max =  3.4065622017119352 learning rate =  0.0001\n",
      "Epoch =  1411 Batch =  0 Loss =  [9.2178212] Gradient_max =  3.406562153401182 learning rate =  0.0001\n",
      "Epoch =  1412 Batch =  0 Loss =  [9.2178211] Gradient_max =  3.4065621053988813 learning rate =  0.0001\n",
      "Epoch =  1413 Batch =  0 Loss =  [9.21782101] Gradient_max =  3.4065620577030478 learning rate =  0.0001\n",
      "Epoch =  1414 Batch =  0 Loss =  [9.21782091] Gradient_max =  3.40656201031171 learning rate =  0.0001\n",
      "Epoch =  1415 Batch =  0 Loss =  [9.21782082] Gradient_max =  3.406561963222909 learning rate =  0.0001\n",
      "Epoch =  1416 Batch =  0 Loss =  [9.21782073] Gradient_max =  3.4065619164346987 learning rate =  0.0001\n",
      "Epoch =  1417 Batch =  0 Loss =  [9.21782063] Gradient_max =  3.4065618699451434 learning rate =  0.0001\n",
      "Epoch =  1418 Batch =  0 Loss =  [9.21782054] Gradient_max =  3.406561823752325 learning rate =  0.0001\n",
      "Epoch =  1419 Batch =  0 Loss =  [9.21782045] Gradient_max =  3.406561777854331 learning rate =  0.0001\n",
      "Epoch =  1420 Batch =  0 Loss =  [9.21782036] Gradient_max =  3.406561732249267 learning rate =  0.0001\n",
      "Epoch =  1421 Batch =  0 Loss =  [9.21782027] Gradient_max =  3.4065616869352526 learning rate =  0.0001\n",
      "Epoch =  1422 Batch =  0 Loss =  [9.21782018] Gradient_max =  3.4065616419104097 learning rate =  0.0001\n",
      "Epoch =  1423 Batch =  0 Loss =  [9.2178201] Gradient_max =  3.4065615971728826 learning rate =  0.0001\n",
      "Epoch =  1424 Batch =  0 Loss =  [9.21782001] Gradient_max =  3.4065615527208237 learning rate =  0.0001\n",
      "Epoch =  1425 Batch =  0 Loss =  [9.21781992] Gradient_max =  3.406561508552397 learning rate =  0.0001\n",
      "Epoch =  1426 Batch =  0 Loss =  [9.21781983] Gradient_max =  3.4065614646657805 learning rate =  0.0001\n",
      "Epoch =  1427 Batch =  0 Loss =  [9.21781975] Gradient_max =  3.40656142105916 learning rate =  0.0001\n",
      "Epoch =  1428 Batch =  0 Loss =  [9.21781966] Gradient_max =  3.406561377730736 learning rate =  0.0001\n",
      "Epoch =  1429 Batch =  0 Loss =  [9.21781958] Gradient_max =  3.40656133467872 learning rate =  0.0001\n",
      "Epoch =  1430 Batch =  0 Loss =  [9.21781949] Gradient_max =  3.4065612919013386 learning rate =  0.0001\n",
      "Epoch =  1431 Batch =  0 Loss =  [9.21781941] Gradient_max =  3.406561249396823 learning rate =  0.0001\n",
      "Epoch =  1432 Batch =  0 Loss =  [9.21781933] Gradient_max =  3.406561207163421 learning rate =  0.0001\n",
      "Epoch =  1433 Batch =  0 Loss =  [9.21781924] Gradient_max =  3.4065611651993906 learning rate =  0.0001\n",
      "Epoch =  1434 Batch =  0 Loss =  [9.21781916] Gradient_max =  3.4065611235029998 learning rate =  0.0001\n",
      "Epoch =  1435 Batch =  0 Loss =  [9.21781908] Gradient_max =  3.406561082072529 learning rate =  0.0001\n",
      "Epoch =  1436 Batch =  0 Loss =  [9.217819] Gradient_max =  3.406561040906272 learning rate =  0.0001\n",
      "Epoch =  1437 Batch =  0 Loss =  [9.21781892] Gradient_max =  3.40656100000253 learning rate =  0.0001\n",
      "Epoch =  1438 Batch =  0 Loss =  [9.21781884] Gradient_max =  3.406560959359614 learning rate =  0.0001\n",
      "Epoch =  1439 Batch =  0 Loss =  [9.21781876] Gradient_max =  3.4065609189758486 learning rate =  0.0001\n",
      "Epoch =  1440 Batch =  0 Loss =  [9.21781868] Gradient_max =  3.406560878849573 learning rate =  0.0001\n",
      "Epoch =  1441 Batch =  0 Loss =  [9.2178186] Gradient_max =  3.4065608389791318 learning rate =  0.0001\n",
      "Epoch =  1442 Batch =  0 Loss =  [9.21781852] Gradient_max =  3.4065607993628797 learning rate =  0.0001\n",
      "Epoch =  1443 Batch =  0 Loss =  [9.21781844] Gradient_max =  3.4065607599991856 learning rate =  0.0001\n",
      "Epoch =  1444 Batch =  0 Loss =  [9.21781836] Gradient_max =  3.4065607208864286 learning rate =  0.0001\n",
      "Epoch =  1445 Batch =  0 Loss =  [9.21781829] Gradient_max =  3.406560682022996 learning rate =  0.0001\n",
      "Epoch =  1446 Batch =  0 Loss =  [9.21781821] Gradient_max =  3.4065606434072877 learning rate =  0.0001\n",
      "Epoch =  1447 Batch =  0 Loss =  [9.21781813] Gradient_max =  3.406560605037713 learning rate =  0.0001\n",
      "Epoch =  1448 Batch =  0 Loss =  [9.21781806] Gradient_max =  3.4065605669126917 learning rate =  0.0001\n",
      "Epoch =  1449 Batch =  0 Loss =  [9.21781798] Gradient_max =  3.4065605290306533 learning rate =  0.0001\n",
      "Epoch =  1450 Batch =  0 Loss =  [9.21781791] Gradient_max =  3.406560491390038 learning rate =  0.0001\n",
      "Epoch =  1451 Batch =  0 Loss =  [9.21781784] Gradient_max =  3.406560453989296 learning rate =  0.0001\n",
      "Epoch =  1452 Batch =  0 Loss =  [9.21781776] Gradient_max =  3.406560416826888 learning rate =  0.0001\n",
      "Epoch =  1453 Batch =  0 Loss =  [9.21781769] Gradient_max =  3.4065603799012827 learning rate =  0.0001\n",
      "Epoch =  1454 Batch =  0 Loss =  [9.21781762] Gradient_max =  3.4065603432109635 learning rate =  0.0001\n",
      "Epoch =  1455 Batch =  0 Loss =  [9.21781754] Gradient_max =  3.406560306754415 learning rate =  0.0001\n",
      "Epoch =  1456 Batch =  0 Loss =  [9.21781747] Gradient_max =  3.4065602705301448 learning rate =  0.0001\n",
      "Epoch =  1457 Batch =  0 Loss =  [9.2178174] Gradient_max =  3.4065602345366566 learning rate =  0.0001\n",
      "Epoch =  1458 Batch =  0 Loss =  [9.21781733] Gradient_max =  3.406560198772471 learning rate =  0.0001\n",
      "Epoch =  1459 Batch =  0 Loss =  [9.21781726] Gradient_max =  3.4065601632361178 learning rate =  0.0001\n",
      "Epoch =  1460 Batch =  0 Loss =  [9.21781719] Gradient_max =  3.406560127926137 learning rate =  0.0001\n",
      "Epoch =  1461 Batch =  0 Loss =  [9.21781712] Gradient_max =  3.4065600928410698 learning rate =  0.0001\n",
      "Epoch =  1462 Batch =  0 Loss =  [9.21781705] Gradient_max =  3.406560057979478 learning rate =  0.0001\n",
      "Epoch =  1463 Batch =  0 Loss =  [9.21781698] Gradient_max =  3.40656002333993 learning rate =  0.0001\n",
      "Epoch =  1464 Batch =  0 Loss =  [9.21781692] Gradient_max =  3.406559988920998 learning rate =  0.0001\n",
      "Epoch =  1465 Batch =  0 Loss =  [9.21781685] Gradient_max =  3.4065599547212666 learning rate =  0.0001\n",
      "Epoch =  1466 Batch =  0 Loss =  [9.21781678] Gradient_max =  3.4065599207393324 learning rate =  0.0001\n",
      "Epoch =  1467 Batch =  0 Loss =  [9.21781671] Gradient_max =  3.4065598869737985 learning rate =  0.0001\n",
      "Epoch =  1468 Batch =  0 Loss =  [9.21781665] Gradient_max =  3.406559853423275 learning rate =  0.0001\n",
      "Epoch =  1469 Batch =  0 Loss =  [9.21781658] Gradient_max =  3.406559820086385 learning rate =  0.0001\n",
      "Epoch =  1470 Batch =  0 Loss =  [9.21781652] Gradient_max =  3.4065597869617577 learning rate =  0.0001\n",
      "Epoch =  1471 Batch =  0 Loss =  [9.21781645] Gradient_max =  3.406559754048035 learning rate =  0.0001\n",
      "Epoch =  1472 Batch =  0 Loss =  [9.21781639] Gradient_max =  3.4065597213438608 learning rate =  0.0001\n",
      "Epoch =  1473 Batch =  0 Loss =  [9.21781632] Gradient_max =  3.406559688847895 learning rate =  0.0001\n",
      "Epoch =  1474 Batch =  0 Loss =  [9.21781626] Gradient_max =  3.4065596565587994 learning rate =  0.0001\n",
      "Epoch =  1475 Batch =  0 Loss =  [9.2178162] Gradient_max =  3.4065596244752503 learning rate =  0.0001\n",
      "Epoch =  1476 Batch =  0 Loss =  [9.21781613] Gradient_max =  3.4065595925959316 learning rate =  0.0001\n",
      "Epoch =  1477 Batch =  0 Loss =  [9.21781607] Gradient_max =  3.40655956091953 learning rate =  0.0001\n",
      "Epoch =  1478 Batch =  0 Loss =  [9.21781601] Gradient_max =  3.406559529444749 learning rate =  0.0001\n",
      "Epoch =  1479 Batch =  0 Loss =  [9.21781595] Gradient_max =  3.406559498170296 learning rate =  0.0001\n",
      "Epoch =  1480 Batch =  0 Loss =  [9.21781588] Gradient_max =  3.4065594670948864 learning rate =  0.0001\n",
      "Epoch =  1481 Batch =  0 Loss =  [9.21781582] Gradient_max =  3.4065594362172433 learning rate =  0.0001\n",
      "Epoch =  1482 Batch =  0 Loss =  [9.21781576] Gradient_max =  3.406559405536102 learning rate =  0.0001\n",
      "Epoch =  1483 Batch =  0 Loss =  [9.2178157] Gradient_max =  3.406559375050203 learning rate =  0.0001\n",
      "Epoch =  1484 Batch =  0 Loss =  [9.21781564] Gradient_max =  3.4065593447582945 learning rate =  0.0001\n",
      "Epoch =  1485 Batch =  0 Loss =  [9.21781558] Gradient_max =  3.4065593146591335 learning rate =  0.0001\n",
      "Epoch =  1486 Batch =  0 Loss =  [9.21781552] Gradient_max =  3.4065592847514865 learning rate =  0.0001\n",
      "Epoch =  1487 Batch =  0 Loss =  [9.21781546] Gradient_max =  3.4065592550341264 learning rate =  0.0001\n",
      "Epoch =  1488 Batch =  0 Loss =  [9.21781541] Gradient_max =  3.4065592255058315 learning rate =  0.0001\n",
      "Epoch =  1489 Batch =  0 Loss =  [9.21781535] Gradient_max =  3.4065591961653943 learning rate =  0.0001\n",
      "Epoch =  1490 Batch =  0 Loss =  [9.21781529] Gradient_max =  3.40655916701161 learning rate =  0.0001\n",
      "Epoch =  1491 Batch =  0 Loss =  [9.21781523] Gradient_max =  3.4065591380432867 learning rate =  0.0001\n",
      "Epoch =  1492 Batch =  0 Loss =  [9.21781518] Gradient_max =  3.4065591092592316 learning rate =  0.0001\n",
      "Epoch =  1493 Batch =  0 Loss =  [9.21781512] Gradient_max =  3.406559080658268 learning rate =  0.0001\n",
      "Epoch =  1494 Batch =  0 Loss =  [9.21781506] Gradient_max =  3.4065590522392224 learning rate =  0.0001\n",
      "Epoch =  1495 Batch =  0 Loss =  [9.21781501] Gradient_max =  3.4065590240009307 learning rate =  0.0001\n",
      "Epoch =  1496 Batch =  0 Loss =  [9.21781495] Gradient_max =  3.4065589959422358 learning rate =  0.0001\n",
      "Epoch =  1497 Batch =  0 Loss =  [9.2178149] Gradient_max =  3.406558968061986 learning rate =  0.0001\n",
      "Epoch =  1498 Batch =  0 Loss =  [9.21781484] Gradient_max =  3.4065589403590435 learning rate =  0.0001\n",
      "Epoch =  1499 Batch =  0 Loss =  [9.21781479] Gradient_max =  3.406558912832267 learning rate =  0.0001\n",
      "Epoch =  1500 Batch =  0 Loss =  [9.21781473] Gradient_max =  3.4065588854805333 learning rate =  0.0001\n",
      "Epoch =  1501 Batch =  0 Loss =  [9.21781468] Gradient_max =  3.40655885830272 learning rate =  0.0001\n",
      "Epoch =  1502 Batch =  0 Loss =  [9.21781463] Gradient_max =  3.4065588312977146 learning rate =  0.0001\n",
      "Epoch =  1503 Batch =  0 Loss =  [9.21781457] Gradient_max =  3.4065588044644106 learning rate =  0.0001\n",
      "Epoch =  1504 Batch =  0 Loss =  [9.21781452] Gradient_max =  3.406558777801711 learning rate =  0.0001\n",
      "Epoch =  1505 Batch =  0 Loss =  [9.21781447] Gradient_max =  3.406558751308524 learning rate =  0.0001\n",
      "Epoch =  1506 Batch =  0 Loss =  [9.21781442] Gradient_max =  3.4065587249837606 learning rate =  0.0001\n",
      "Epoch =  1507 Batch =  0 Loss =  [9.21781436] Gradient_max =  3.406558698826346 learning rate =  0.0001\n",
      "Epoch =  1508 Batch =  0 Loss =  [9.21781431] Gradient_max =  3.4065586728352115 learning rate =  0.0001\n",
      "Epoch =  1509 Batch =  0 Loss =  [9.21781426] Gradient_max =  3.4065586470092915 learning rate =  0.0001\n",
      "Epoch =  1510 Batch =  0 Loss =  [9.21781421] Gradient_max =  3.4065586213475285 learning rate =  0.0001\n",
      "Epoch =  1511 Batch =  0 Loss =  [9.21781416] Gradient_max =  3.4065585958488724 learning rate =  0.0001\n",
      "Epoch =  1512 Batch =  0 Loss =  [9.21781411] Gradient_max =  3.4065585705122814 learning rate =  0.0001\n",
      "Epoch =  1513 Batch =  0 Loss =  [9.21781406] Gradient_max =  3.406558545336716 learning rate =  0.0001\n",
      "Epoch =  1514 Batch =  0 Loss =  [9.21781401] Gradient_max =  3.40655852032115 learning rate =  0.0001\n",
      "Epoch =  1515 Batch =  0 Loss =  [9.21781396] Gradient_max =  3.4065584954645556 learning rate =  0.0001\n",
      "Epoch =  1516 Batch =  0 Loss =  [9.21781391] Gradient_max =  3.406558470765921 learning rate =  0.0001\n",
      "Epoch =  1517 Batch =  0 Loss =  [9.21781386] Gradient_max =  3.4065584462242304 learning rate =  0.0001\n",
      "Epoch =  1518 Batch =  0 Loss =  [9.21781382] Gradient_max =  3.4065584218384863 learning rate =  0.0001\n",
      "Epoch =  1519 Batch =  0 Loss =  [9.21781377] Gradient_max =  3.4065583976076863 learning rate =  0.0001\n",
      "Epoch =  1520 Batch =  0 Loss =  [9.21781372] Gradient_max =  3.406558373530845 learning rate =  0.0001\n",
      "Epoch =  1521 Batch =  0 Loss =  [9.21781367] Gradient_max =  3.4065583496069722 learning rate =  0.0001\n",
      "Epoch =  1522 Batch =  0 Loss =  [9.21781363] Gradient_max =  3.4065583258350927 learning rate =  0.0001\n",
      "Epoch =  1523 Batch =  0 Loss =  [9.21781358] Gradient_max =  3.4065583022142345 learning rate =  0.0001\n",
      "Epoch =  1524 Batch =  0 Loss =  [9.21781353] Gradient_max =  3.4065582787434314 learning rate =  0.0001\n",
      "Epoch =  1525 Batch =  0 Loss =  [9.21781349] Gradient_max =  3.4065582554217277 learning rate =  0.0001\n",
      "Epoch =  1526 Batch =  0 Loss =  [9.21781344] Gradient_max =  3.4065582322481656 learning rate =  0.0001\n",
      "Epoch =  1527 Batch =  0 Loss =  [9.21781339] Gradient_max =  3.406558209221802 learning rate =  0.0001\n",
      "Epoch =  1528 Batch =  0 Loss =  [9.21781335] Gradient_max =  3.406558186341692 learning rate =  0.0001\n",
      "Epoch =  1529 Batch =  0 Loss =  [9.2178133] Gradient_max =  3.4065581636069067 learning rate =  0.0001\n",
      "Epoch =  1530 Batch =  0 Loss =  [9.21781326] Gradient_max =  3.406558141016513 learning rate =  0.0001\n",
      "Epoch =  1531 Batch =  0 Loss =  [9.21781321] Gradient_max =  3.4065581185695875 learning rate =  0.0001\n",
      "Epoch =  1532 Batch =  0 Loss =  [9.21781317] Gradient_max =  3.4065580962652153 learning rate =  0.0001\n",
      "Epoch =  1533 Batch =  0 Loss =  [9.21781313] Gradient_max =  3.406558074102487 learning rate =  0.0001\n",
      "Epoch =  1534 Batch =  0 Loss =  [9.21781308] Gradient_max =  3.4065580520804954 learning rate =  0.0001\n",
      "Epoch =  1535 Batch =  0 Loss =  [9.21781304] Gradient_max =  3.4065580301983402 learning rate =  0.0001\n",
      "Epoch =  1536 Batch =  0 Loss =  [9.217813] Gradient_max =  3.4065580084551326 learning rate =  0.0001\n",
      "Epoch =  1537 Batch =  0 Loss =  [9.21781295] Gradient_max =  3.40655798684998 learning rate =  0.0001\n",
      "Epoch =  1538 Batch =  0 Loss =  [9.21781291] Gradient_max =  3.4065579653820017 learning rate =  0.0001\n",
      "Epoch =  1539 Batch =  0 Loss =  [9.21781287] Gradient_max =  3.406557944050322 learning rate =  0.0001\n",
      "Epoch =  1540 Batch =  0 Loss =  [9.21781283] Gradient_max =  3.4065579228540734 learning rate =  0.0001\n",
      "Epoch =  1541 Batch =  0 Loss =  [9.21781278] Gradient_max =  3.406557901792385 learning rate =  0.0001\n",
      "Epoch =  1542 Batch =  0 Loss =  [9.21781274] Gradient_max =  3.4065578808643995 learning rate =  0.0001\n",
      "Epoch =  1543 Batch =  0 Loss =  [9.2178127] Gradient_max =  3.406557860069265 learning rate =  0.0001\n",
      "Epoch =  1544 Batch =  0 Loss =  [9.21781266] Gradient_max =  3.40655783940613 learning rate =  0.0001\n",
      "Epoch =  1545 Batch =  0 Loss =  [9.21781262] Gradient_max =  3.406557818874155 learning rate =  0.0001\n",
      "Epoch =  1546 Batch =  0 Loss =  [9.21781258] Gradient_max =  3.4065577984725 learning rate =  0.0001\n",
      "Epoch =  1547 Batch =  0 Loss =  [9.21781254] Gradient_max =  3.4065577782003333 learning rate =  0.0001\n",
      "Epoch =  1548 Batch =  0 Loss =  [9.2178125] Gradient_max =  3.4065577580568323 learning rate =  0.0001\n",
      "Epoch =  1549 Batch =  0 Loss =  [9.21781246] Gradient_max =  3.406557738041168 learning rate =  0.0001\n",
      "Epoch =  1550 Batch =  0 Loss =  [9.21781242] Gradient_max =  3.4065577181525297 learning rate =  0.0001\n",
      "Epoch =  1551 Batch =  0 Loss =  [9.21781238] Gradient_max =  3.4065576983901042 learning rate =  0.0001\n",
      "Epoch =  1552 Batch =  0 Loss =  [9.21781234] Gradient_max =  3.4065576787530865 learning rate =  0.0001\n",
      "Epoch =  1553 Batch =  0 Loss =  [9.2178123] Gradient_max =  3.4065576592406734 learning rate =  0.0001\n",
      "Epoch =  1554 Batch =  0 Loss =  [9.21781227] Gradient_max =  3.4065576398520747 learning rate =  0.0001\n",
      "Epoch =  1555 Batch =  0 Loss =  [9.21781223] Gradient_max =  3.4065576205864985 learning rate =  0.0001\n",
      "Epoch =  1556 Batch =  0 Loss =  [9.21781219] Gradient_max =  3.406557601443156 learning rate =  0.0001\n",
      "Epoch =  1557 Batch =  0 Loss =  [9.21781215] Gradient_max =  3.40655758242127 learning rate =  0.0001\n",
      "Epoch =  1558 Batch =  0 Loss =  [9.21781211] Gradient_max =  3.4065575635200647 learning rate =  0.0001\n",
      "Epoch =  1559 Batch =  0 Loss =  [9.21781208] Gradient_max =  3.406557544738773 learning rate =  0.0001\n",
      "Epoch =  1560 Batch =  0 Loss =  [9.21781204] Gradient_max =  3.406557526076624 learning rate =  0.0001\n",
      "Epoch =  1561 Batch =  0 Loss =  [9.217812] Gradient_max =  3.406557507532862 learning rate =  0.0001\n",
      "Epoch =  1562 Batch =  0 Loss =  [9.21781197] Gradient_max =  3.4065574891067283 learning rate =  0.0001\n",
      "Epoch =  1563 Batch =  0 Loss =  [9.21781193] Gradient_max =  3.4065574707974755 learning rate =  0.0001\n",
      "Epoch =  1564 Batch =  0 Loss =  [9.21781189] Gradient_max =  3.4065574526043565 learning rate =  0.0001\n",
      "Epoch =  1565 Batch =  0 Loss =  [9.21781186] Gradient_max =  3.406557434526632 learning rate =  0.0001\n",
      "Epoch =  1566 Batch =  0 Loss =  [9.21781182] Gradient_max =  3.4065574165635626 learning rate =  0.0001\n",
      "Epoch =  1567 Batch =  0 Loss =  [9.21781179] Gradient_max =  3.4065573987144195 learning rate =  0.0001\n",
      "Epoch =  1568 Batch =  0 Loss =  [9.21781175] Gradient_max =  3.406557380978474 learning rate =  0.0001\n",
      "Epoch =  1569 Batch =  0 Loss =  [9.21781172] Gradient_max =  3.4065573633550055 learning rate =  0.0001\n",
      "Epoch =  1570 Batch =  0 Loss =  [9.21781168] Gradient_max =  3.4065573458432965 learning rate =  0.0001\n",
      "Epoch =  1571 Batch =  0 Loss =  [9.21781165] Gradient_max =  3.4065573284426334 learning rate =  0.0001\n",
      "Epoch =  1572 Batch =  0 Loss =  [9.21781161] Gradient_max =  3.406557311152309 learning rate =  0.0001\n",
      "Epoch =  1573 Batch =  0 Loss =  [9.21781158] Gradient_max =  3.40655729397162 learning rate =  0.0001\n",
      "Epoch =  1574 Batch =  0 Loss =  [9.21781155] Gradient_max =  3.406557276899864 learning rate =  0.0001\n",
      "Epoch =  1575 Batch =  0 Loss =  [9.21781151] Gradient_max =  3.4065572599363514 learning rate =  0.0001\n",
      "Epoch =  1576 Batch =  0 Loss =  [9.21781148] Gradient_max =  3.4065572430803863 learning rate =  0.0001\n",
      "Epoch =  1577 Batch =  0 Loss =  [9.21781145] Gradient_max =  3.4065572263312895 learning rate =  0.0001\n",
      "Epoch =  1578 Batch =  0 Loss =  [9.21781141] Gradient_max =  3.406557209688374 learning rate =  0.0001\n",
      "Epoch =  1579 Batch =  0 Loss =  [9.21781138] Gradient_max =  3.406557193150967 learning rate =  0.0001\n",
      "Epoch =  1580 Batch =  0 Loss =  [9.21781135] Gradient_max =  3.406557176718395 learning rate =  0.0001\n",
      "Epoch =  1581 Batch =  0 Loss =  [9.21781131] Gradient_max =  3.4065571603899856 learning rate =  0.0001\n",
      "Epoch =  1582 Batch =  0 Loss =  [9.21781128] Gradient_max =  3.406557144165078 learning rate =  0.0001\n",
      "Epoch =  1583 Batch =  0 Loss =  [9.21781125] Gradient_max =  3.4065571280430147 learning rate =  0.0001\n",
      "Epoch =  1584 Batch =  0 Loss =  [9.21781122] Gradient_max =  3.4065571120231355 learning rate =  0.0001\n",
      "Epoch =  1585 Batch =  0 Loss =  [9.21781119] Gradient_max =  3.4065570961047924 learning rate =  0.0001\n",
      "Epoch =  1586 Batch =  0 Loss =  [9.21781116] Gradient_max =  3.406557080287338 learning rate =  0.0001\n",
      "Epoch =  1587 Batch =  0 Loss =  [9.21781112] Gradient_max =  3.4065570645701277 learning rate =  0.0001\n",
      "Epoch =  1588 Batch =  0 Loss =  [9.21781109] Gradient_max =  3.4065570489525254 learning rate =  0.0001\n",
      "Epoch =  1589 Batch =  0 Loss =  [9.21781106] Gradient_max =  3.406557033433891 learning rate =  0.0001\n",
      "Epoch =  1590 Batch =  0 Loss =  [9.21781103] Gradient_max =  3.4065570180136016 learning rate =  0.0001\n",
      "Epoch =  1591 Batch =  0 Loss =  [9.217811] Gradient_max =  3.406557002691022 learning rate =  0.0001\n",
      "Epoch =  1592 Batch =  0 Loss =  [9.21781097] Gradient_max =  3.406556987465536 learning rate =  0.0001\n",
      "Epoch =  1593 Batch =  0 Loss =  [9.21781094] Gradient_max =  3.4065569723365243 learning rate =  0.0001\n",
      "Epoch =  1594 Batch =  0 Loss =  [9.21781091] Gradient_max =  3.406556957303369 learning rate =  0.0001\n",
      "Epoch =  1595 Batch =  0 Loss =  [9.21781088] Gradient_max =  3.406556942365461 learning rate =  0.0001\n",
      "Epoch =  1596 Batch =  0 Loss =  [9.21781085] Gradient_max =  3.406556927522194 learning rate =  0.0001\n",
      "Epoch =  1597 Batch =  0 Loss =  [9.21781082] Gradient_max =  3.406556912772967 learning rate =  0.0001\n",
      "Epoch =  1598 Batch =  0 Loss =  [9.21781079] Gradient_max =  3.4065568981171768 learning rate =  0.0001\n",
      "Epoch =  1599 Batch =  0 Loss =  [9.21781077] Gradient_max =  3.4065568835542304 learning rate =  0.0001\n",
      "Epoch =  1600 Batch =  0 Loss =  [9.21781074] Gradient_max =  3.4065568690835355 learning rate =  0.0001\n",
      "Epoch =  1601 Batch =  0 Loss =  [9.21781071] Gradient_max =  3.4065568547045078 learning rate =  0.0001\n",
      "Epoch =  1602 Batch =  0 Loss =  [9.21781068] Gradient_max =  3.40655684041656 learning rate =  0.0001\n",
      "Epoch =  1603 Batch =  0 Loss =  [9.21781065] Gradient_max =  3.4065568262191124 learning rate =  0.0001\n",
      "Epoch =  1604 Batch =  0 Loss =  [9.21781062] Gradient_max =  3.40655681211159 learning rate =  0.0001\n",
      "Epoch =  1605 Batch =  0 Loss =  [9.2178106] Gradient_max =  3.406556798093421 learning rate =  0.0001\n",
      "Epoch =  1606 Batch =  0 Loss =  [9.21781057] Gradient_max =  3.4065567841640303 learning rate =  0.0001\n",
      "Epoch =  1607 Batch =  0 Loss =  [9.21781054] Gradient_max =  3.406556770322863 learning rate =  0.0001\n",
      "Epoch =  1608 Batch =  0 Loss =  [9.21781051] Gradient_max =  3.4065567565693486 learning rate =  0.0001\n",
      "Epoch =  1609 Batch =  0 Loss =  [9.21781049] Gradient_max =  3.4065567429029344 learning rate =  0.0001\n",
      "Epoch =  1610 Batch =  0 Loss =  [9.21781046] Gradient_max =  3.4065567293230616 learning rate =  0.0001\n",
      "Epoch =  1611 Batch =  0 Loss =  [9.21781043] Gradient_max =  3.4065567158291814 learning rate =  0.0001\n",
      "Epoch =  1612 Batch =  0 Loss =  [9.21781041] Gradient_max =  3.406556702420746 learning rate =  0.0001\n",
      "Epoch =  1613 Batch =  0 Loss =  [9.21781038] Gradient_max =  3.406556689097215 learning rate =  0.0001\n",
      "Epoch =  1614 Batch =  0 Loss =  [9.21781035] Gradient_max =  3.4065566758580412 learning rate =  0.0001\n",
      "Epoch =  1615 Batch =  0 Loss =  [9.21781033] Gradient_max =  3.4065566627026933 learning rate =  0.0001\n",
      "Epoch =  1616 Batch =  0 Loss =  [9.2178103] Gradient_max =  3.4065566496306325 learning rate =  0.0001\n",
      "Epoch =  1617 Batch =  0 Loss =  [9.21781027] Gradient_max =  3.406556636641334 learning rate =  0.0001\n",
      "Epoch =  1618 Batch =  0 Loss =  [9.21781025] Gradient_max =  3.4065566237342693 learning rate =  0.0001\n",
      "Epoch =  1619 Batch =  0 Loss =  [9.21781022] Gradient_max =  3.4065566109089143 learning rate =  0.0001\n",
      "Epoch =  1620 Batch =  0 Loss =  [9.2178102] Gradient_max =  3.4065565981647485 learning rate =  0.0001\n",
      "Epoch =  1621 Batch =  0 Loss =  [9.21781017] Gradient_max =  3.406556585501257 learning rate =  0.0001\n",
      "Epoch =  1622 Batch =  0 Loss =  [9.21781015] Gradient_max =  3.406556572917923 learning rate =  0.0001\n",
      "Epoch =  1623 Batch =  0 Loss =  [9.21781012] Gradient_max =  3.406556560414243 learning rate =  0.0001\n",
      "Epoch =  1624 Batch =  0 Loss =  [9.2178101] Gradient_max =  3.4065565479897058 learning rate =  0.0001\n",
      "Epoch =  1625 Batch =  0 Loss =  [9.21781007] Gradient_max =  3.406556535643806 learning rate =  0.0001\n",
      "Epoch =  1626 Batch =  0 Loss =  [9.21781005] Gradient_max =  3.406556523376047 learning rate =  0.0001\n",
      "Epoch =  1627 Batch =  0 Loss =  [9.21781003] Gradient_max =  3.406556511185929 learning rate =  0.0001\n",
      "Epoch =  1628 Batch =  0 Loss =  [9.21781] Gradient_max =  3.40655649907296 learning rate =  0.0001\n",
      "Epoch =  1629 Batch =  0 Loss =  [9.21780998] Gradient_max =  3.4065564870366494 learning rate =  0.0001\n",
      "Epoch =  1630 Batch =  0 Loss =  [9.21780995] Gradient_max =  3.406556475076507 learning rate =  0.0001\n",
      "Epoch =  1631 Batch =  0 Loss =  [9.21780993] Gradient_max =  3.4065564631920515 learning rate =  0.0001\n",
      "Epoch =  1632 Batch =  0 Loss =  [9.21780991] Gradient_max =  3.4065564513827997 learning rate =  0.0001\n",
      "Epoch =  1633 Batch =  0 Loss =  [9.21780988] Gradient_max =  3.4065564396482726 learning rate =  0.0001\n",
      "Epoch =  1634 Batch =  0 Loss =  [9.21780986] Gradient_max =  3.406556427987999 learning rate =  0.0001\n",
      "Epoch =  1635 Batch =  0 Loss =  [9.21780984] Gradient_max =  3.406556416401501 learning rate =  0.0001\n",
      "Epoch =  1636 Batch =  0 Loss =  [9.21780981] Gradient_max =  3.406556404888312 learning rate =  0.0001\n",
      "Epoch =  1637 Batch =  0 Loss =  [9.21780979] Gradient_max =  3.4065563934479672 learning rate =  0.0001\n",
      "Epoch =  1638 Batch =  0 Loss =  [9.21780977] Gradient_max =  3.40655638208 learning rate =  0.0001\n",
      "Epoch =  1639 Batch =  0 Loss =  [9.21780975] Gradient_max =  3.4065563707839535 learning rate =  0.0001\n",
      "Epoch =  1640 Batch =  0 Loss =  [9.21780972] Gradient_max =  3.4065563595593673 learning rate =  0.0001\n",
      "Epoch =  1641 Batch =  0 Loss =  [9.2178097] Gradient_max =  3.40655634840579 learning rate =  0.0001\n",
      "Epoch =  1642 Batch =  0 Loss =  [9.21780968] Gradient_max =  3.40655633732277 learning rate =  0.0001\n",
      "Epoch =  1643 Batch =  0 Loss =  [9.21780966] Gradient_max =  3.406556326309857 learning rate =  0.0001\n",
      "Epoch =  1644 Batch =  0 Loss =  [9.21780964] Gradient_max =  3.406556315366606 learning rate =  0.0001\n",
      "Epoch =  1645 Batch =  0 Loss =  [9.21780961] Gradient_max =  3.4065563044925757 learning rate =  0.0001\n",
      "Epoch =  1646 Batch =  0 Loss =  [9.21780959] Gradient_max =  3.4065562936873213 learning rate =  0.0001\n",
      "Epoch =  1647 Batch =  0 Loss =  [9.21780957] Gradient_max =  3.406556282950411 learning rate =  0.0001\n",
      "Epoch =  1648 Batch =  0 Loss =  [9.21780955] Gradient_max =  3.4065562722814082 learning rate =  0.0001\n",
      "Epoch =  1649 Batch =  0 Loss =  [9.21780953] Gradient_max =  3.40655626167988 learning rate =  0.0001\n",
      "Epoch =  1650 Batch =  0 Loss =  [9.21780951] Gradient_max =  3.4065562511454 learning rate =  0.0001\n",
      "Epoch =  1651 Batch =  0 Loss =  [9.21780949] Gradient_max =  3.4065562406775403 learning rate =  0.0001\n",
      "Epoch =  1652 Batch =  0 Loss =  [9.21780947] Gradient_max =  3.406556230275877 learning rate =  0.0001\n",
      "Epoch =  1653 Batch =  0 Loss =  [9.21780945] Gradient_max =  3.406556219939994 learning rate =  0.0001\n",
      "Epoch =  1654 Batch =  0 Loss =  [9.21780943] Gradient_max =  3.406556209669467 learning rate =  0.0001\n",
      "Epoch =  1655 Batch =  0 Loss =  [9.21780941] Gradient_max =  3.406556199463884 learning rate =  0.0001\n",
      "Epoch =  1656 Batch =  0 Loss =  [9.21780939] Gradient_max =  3.4065561893228313 learning rate =  0.0001\n",
      "Epoch =  1657 Batch =  0 Loss =  [9.21780937] Gradient_max =  3.4065561792458996 learning rate =  0.0001\n",
      "Epoch =  1658 Batch =  0 Loss =  [9.21780935] Gradient_max =  3.4065561692326827 learning rate =  0.0001\n",
      "Epoch =  1659 Batch =  0 Loss =  [9.21780933] Gradient_max =  3.4065561592827738 learning rate =  0.0001\n",
      "Epoch =  1660 Batch =  0 Loss =  [9.21780931] Gradient_max =  3.4065561493957697 learning rate =  0.0001\n",
      "Epoch =  1661 Batch =  0 Loss =  [9.21780929] Gradient_max =  3.406556139571275 learning rate =  0.0001\n",
      "Epoch =  1662 Batch =  0 Loss =  [9.21780927] Gradient_max =  3.40655612980889 learning rate =  0.0001\n",
      "Epoch =  1663 Batch =  0 Loss =  [9.21780925] Gradient_max =  3.406556120108219 learning rate =  0.0001\n",
      "Epoch =  1664 Batch =  0 Loss =  [9.21780923] Gradient_max =  3.4065561104688715 learning rate =  0.0001\n",
      "Epoch =  1665 Batch =  0 Loss =  [9.21780921] Gradient_max =  3.406556100890457 learning rate =  0.0001\n",
      "Epoch =  1666 Batch =  0 Loss =  [9.21780919] Gradient_max =  3.4065560913725887 learning rate =  0.0001\n",
      "Epoch =  1667 Batch =  0 Loss =  [9.21780917] Gradient_max =  3.4065560819148843 learning rate =  0.0001\n",
      "Epoch =  1668 Batch =  0 Loss =  [9.21780915] Gradient_max =  3.406556072516959 learning rate =  0.0001\n",
      "Epoch =  1669 Batch =  0 Loss =  [9.21780913] Gradient_max =  3.4065560631784324 learning rate =  0.0001\n",
      "Epoch =  1670 Batch =  0 Loss =  [9.21780912] Gradient_max =  3.40655605389893 learning rate =  0.0001\n",
      "Epoch =  1671 Batch =  0 Loss =  [9.2178091] Gradient_max =  3.4065560446780747 learning rate =  0.0001\n",
      "Epoch =  1672 Batch =  0 Loss =  [9.21780908] Gradient_max =  3.4065560355154956 learning rate =  0.0001\n",
      "Epoch =  1673 Batch =  0 Loss =  [9.21780906] Gradient_max =  3.4065560264108212 learning rate =  0.0001\n",
      "Epoch =  1674 Batch =  0 Loss =  [9.21780904] Gradient_max =  3.4065560173636857 learning rate =  0.0001\n",
      "Epoch =  1675 Batch =  0 Loss =  [9.21780903] Gradient_max =  3.4065560083737227 learning rate =  0.0001\n",
      "Epoch =  1676 Batch =  0 Loss =  [9.21780901] Gradient_max =  3.4065559994405694 learning rate =  0.0001\n",
      "Epoch =  1677 Batch =  0 Loss =  [9.21780899] Gradient_max =  3.4065559905638643 learning rate =  0.0001\n",
      "Epoch =  1678 Batch =  0 Loss =  [9.21780897] Gradient_max =  3.4065559817432516 learning rate =  0.0001\n",
      "Epoch =  1679 Batch =  0 Loss =  [9.21780895] Gradient_max =  3.4065559729783716 learning rate =  0.0001\n",
      "Epoch =  1680 Batch =  0 Loss =  [9.21780894] Gradient_max =  3.4065559642688705 learning rate =  0.0001\n",
      "Epoch =  1681 Batch =  0 Loss =  [9.21780892] Gradient_max =  3.406555955614401 learning rate =  0.0001\n",
      "Epoch =  1682 Batch =  0 Loss =  [9.2178089] Gradient_max =  3.4065559470146107 learning rate =  0.0001\n",
      "Epoch =  1683 Batch =  0 Loss =  [9.21780889] Gradient_max =  3.406555938469153 learning rate =  0.0001\n",
      "Epoch =  1684 Batch =  0 Loss =  [9.21780887] Gradient_max =  3.4065559299776833 learning rate =  0.0001\n",
      "Epoch =  1685 Batch =  0 Loss =  [9.21780885] Gradient_max =  3.4065559215398586 learning rate =  0.0001\n",
      "Epoch =  1686 Batch =  0 Loss =  [9.21780884] Gradient_max =  3.4065559131553385 learning rate =  0.0001\n",
      "Epoch =  1687 Batch =  0 Loss =  [9.21780882] Gradient_max =  3.4065559048237852 learning rate =  0.0001\n",
      "Epoch =  1688 Batch =  0 Loss =  [9.2178088] Gradient_max =  3.406555896544862 learning rate =  0.0001\n",
      "Epoch =  1689 Batch =  0 Loss =  [9.21780879] Gradient_max =  3.406555888318233 learning rate =  0.0001\n",
      "Epoch =  1690 Batch =  0 Loss =  [9.21780877] Gradient_max =  3.4065558801435705 learning rate =  0.0001\n",
      "Epoch =  1691 Batch =  0 Loss =  [9.21780875] Gradient_max =  3.4065558720205416 learning rate =  0.0001\n",
      "Epoch =  1692 Batch =  0 Loss =  [9.21780874] Gradient_max =  3.4065558639488205 learning rate =  0.0001\n",
      "Epoch =  1693 Batch =  0 Loss =  [9.21780872] Gradient_max =  3.406555855928079 learning rate =  0.0001\n",
      "Epoch =  1694 Batch =  0 Loss =  [9.21780871] Gradient_max =  3.406555847957996 learning rate =  0.0001\n",
      "Epoch =  1695 Batch =  0 Loss =  [9.21780869] Gradient_max =  3.40655584003825 learning rate =  0.0001\n",
      "Epoch =  1696 Batch =  0 Loss =  [9.21780867] Gradient_max =  3.4065558321685225 learning rate =  0.0001\n",
      "Epoch =  1697 Batch =  0 Loss =  [9.21780866] Gradient_max =  3.4065558243484935 learning rate =  0.0001\n",
      "Epoch =  1698 Batch =  0 Loss =  [9.21780864] Gradient_max =  3.406555816577848 learning rate =  0.0001\n",
      "Epoch =  1699 Batch =  0 Loss =  [9.21780863] Gradient_max =  3.406555808856274 learning rate =  0.0001\n",
      "Epoch =  1700 Batch =  0 Loss =  [9.21780861] Gradient_max =  3.4065558011834614 learning rate =  0.0001\n",
      "Epoch =  1701 Batch =  0 Loss =  [9.2178086] Gradient_max =  3.4065557935590967 learning rate =  0.0001\n",
      "Epoch =  1702 Batch =  0 Loss =  [9.21780858] Gradient_max =  3.406555785982879 learning rate =  0.0001\n",
      "Epoch =  1703 Batch =  0 Loss =  [9.21780857] Gradient_max =  3.4065557784544964 learning rate =  0.0001\n",
      "Epoch =  1704 Batch =  0 Loss =  [9.21780855] Gradient_max =  3.406555770973649 learning rate =  0.0001\n",
      "Epoch =  1705 Batch =  0 Loss =  [9.21780854] Gradient_max =  3.4065557635400356 learning rate =  0.0001\n",
      "Epoch =  1706 Batch =  0 Loss =  [9.21780852] Gradient_max =  3.406555756153356 learning rate =  0.0001\n",
      "Epoch =  1707 Batch =  0 Loss =  [9.21780851] Gradient_max =  3.406555748813311 learning rate =  0.0001\n",
      "Epoch =  1708 Batch =  0 Loss =  [9.21780849] Gradient_max =  3.406555741519606 learning rate =  0.0001\n",
      "Epoch =  1709 Batch =  0 Loss =  [9.21780848] Gradient_max =  3.406555734271947 learning rate =  0.0001\n",
      "Epoch =  1710 Batch =  0 Loss =  [9.21780846] Gradient_max =  3.406555727070043 learning rate =  0.0001\n",
      "Epoch =  1711 Batch =  0 Loss =  [9.21780845] Gradient_max =  3.406555719913602 learning rate =  0.0001\n",
      "Epoch =  1712 Batch =  0 Loss =  [9.21780844] Gradient_max =  3.406555712802338 learning rate =  0.0001\n",
      "Epoch =  1713 Batch =  0 Loss =  [9.21780842] Gradient_max =  3.406555705735962 learning rate =  0.0001\n",
      "Epoch =  1714 Batch =  0 Loss =  [9.21780841] Gradient_max =  3.406555698714191 learning rate =  0.0001\n",
      "Epoch =  1715 Batch =  0 Loss =  [9.21780839] Gradient_max =  3.406555691736742 learning rate =  0.0001\n",
      "Epoch =  1716 Batch =  0 Loss =  [9.21780838] Gradient_max =  3.406555684803336 learning rate =  0.0001\n",
      "Epoch =  1717 Batch =  0 Loss =  [9.21780837] Gradient_max =  3.4065556779136896 learning rate =  0.0001\n",
      "Epoch =  1718 Batch =  0 Loss =  [9.21780835] Gradient_max =  3.406555671067529 learning rate =  0.0001\n",
      "Epoch =  1719 Batch =  0 Loss =  [9.21780834] Gradient_max =  3.406555664264575 learning rate =  0.0001\n",
      "Epoch =  1720 Batch =  0 Loss =  [9.21780833] Gradient_max =  3.4065556575045584 learning rate =  0.0001\n",
      "Epoch =  1721 Batch =  0 Loss =  [9.21780831] Gradient_max =  3.4065556507872032 learning rate =  0.0001\n",
      "Epoch =  1722 Batch =  0 Loss =  [9.2178083] Gradient_max =  3.40655564411224 learning rate =  0.0001\n",
      "Epoch =  1723 Batch =  0 Loss =  [9.21780829] Gradient_max =  3.4065556374793995 learning rate =  0.0001\n",
      "Epoch =  1724 Batch =  0 Loss =  [9.21780827] Gradient_max =  3.4065556308884184 learning rate =  0.0001\n",
      "Epoch =  1725 Batch =  0 Loss =  [9.21780826] Gradient_max =  3.4065556243390263 learning rate =  0.0001\n",
      "Epoch =  1726 Batch =  0 Loss =  [9.21780825] Gradient_max =  3.406555617830962 learning rate =  0.0001\n",
      "Epoch =  1727 Batch =  0 Loss =  [9.21780823] Gradient_max =  3.4065556113639652 learning rate =  0.0001\n",
      "Epoch =  1728 Batch =  0 Loss =  [9.21780822] Gradient_max =  3.406555604937771 learning rate =  0.0001\n",
      "Epoch =  1729 Batch =  0 Loss =  [9.21780821] Gradient_max =  3.4065555985521243 learning rate =  0.0001\n",
      "Epoch =  1730 Batch =  0 Loss =  [9.2178082] Gradient_max =  3.4065555922067694 learning rate =  0.0001\n",
      "Epoch =  1731 Batch =  0 Loss =  [9.21780818] Gradient_max =  3.4065555859014482 learning rate =  0.0001\n",
      "Epoch =  1732 Batch =  0 Loss =  [9.21780817] Gradient_max =  3.406555579635906 learning rate =  0.0001\n",
      "Epoch =  1733 Batch =  0 Loss =  [9.21780816] Gradient_max =  3.4065555734098956 learning rate =  0.0001\n",
      "Epoch =  1734 Batch =  0 Loss =  [9.21780815] Gradient_max =  3.4065555672231644 learning rate =  0.0001\n",
      "Epoch =  1735 Batch =  0 Loss =  [9.21780813] Gradient_max =  3.40655556107546 learning rate =  0.0001\n",
      "Epoch =  1736 Batch =  0 Loss =  [9.21780812] Gradient_max =  3.4065555549665385 learning rate =  0.0001\n",
      "Epoch =  1737 Batch =  0 Loss =  [9.21780811] Gradient_max =  3.4065555488961534 learning rate =  0.0001\n",
      "Epoch =  1738 Batch =  0 Loss =  [9.2178081] Gradient_max =  3.4065555428640635 learning rate =  0.0001\n",
      "Epoch =  1739 Batch =  0 Loss =  [9.21780809] Gradient_max =  3.406555536870021 learning rate =  0.0001\n",
      "Epoch =  1740 Batch =  0 Loss =  [9.21780807] Gradient_max =  3.4065555309137894 learning rate =  0.0001\n",
      "Epoch =  1741 Batch =  0 Loss =  [9.21780806] Gradient_max =  3.406555524995125 learning rate =  0.0001\n",
      "Epoch =  1742 Batch =  0 Loss =  [9.21780805] Gradient_max =  3.4065555191137933 learning rate =  0.0001\n",
      "Epoch =  1743 Batch =  0 Loss =  [9.21780804] Gradient_max =  3.4065555132695557 learning rate =  0.0001\n",
      "Epoch =  1744 Batch =  0 Loss =  [9.21780803] Gradient_max =  3.406555507462178 learning rate =  0.0001\n",
      "Epoch =  1745 Batch =  0 Loss =  [9.21780802] Gradient_max =  3.4065555016914266 learning rate =  0.0001\n",
      "Epoch =  1746 Batch =  0 Loss =  [9.217808] Gradient_max =  3.4065554959570714 learning rate =  0.0001\n",
      "Epoch =  1747 Batch =  0 Loss =  [9.21780799] Gradient_max =  3.4065554902588806 learning rate =  0.0001\n",
      "Epoch =  1748 Batch =  0 Loss =  [9.21780798] Gradient_max =  3.4065554845966224 learning rate =  0.0001\n",
      "Epoch =  1749 Batch =  0 Loss =  [9.21780797] Gradient_max =  3.406555478970075 learning rate =  0.0001\n",
      "Epoch =  1750 Batch =  0 Loss =  [9.21780796] Gradient_max =  3.40655547337901 learning rate =  0.0001\n",
      "Epoch =  1751 Batch =  0 Loss =  [9.21780795] Gradient_max =  3.4065554678231984 learning rate =  0.0001\n",
      "Epoch =  1752 Batch =  0 Loss =  [9.21780794] Gradient_max =  3.4065554623024226 learning rate =  0.0001\n",
      "Epoch =  1753 Batch =  0 Loss =  [9.21780793] Gradient_max =  3.406555456816456 learning rate =  0.0001\n",
      "Epoch =  1754 Batch =  0 Loss =  [9.21780791] Gradient_max =  3.406555451365082 learning rate =  0.0001\n",
      "Epoch =  1755 Batch =  0 Loss =  [9.2178079] Gradient_max =  3.4065554459480802 learning rate =  0.0001\n",
      "Epoch =  1756 Batch =  0 Loss =  [9.21780789] Gradient_max =  3.406555440565233 learning rate =  0.0001\n",
      "Epoch =  1757 Batch =  0 Loss =  [9.21780788] Gradient_max =  3.406555435216325 learning rate =  0.0001\n",
      "Epoch =  1758 Batch =  0 Loss =  [9.21780787] Gradient_max =  3.40655542990114 learning rate =  0.0001\n",
      "Epoch =  1759 Batch =  0 Loss =  [9.21780786] Gradient_max =  3.406555424619465 learning rate =  0.0001\n",
      "Epoch =  1760 Batch =  0 Loss =  [9.21780785] Gradient_max =  3.406555419371087 learning rate =  0.0001\n",
      "Epoch =  1761 Batch =  0 Loss =  [9.21780784] Gradient_max =  3.406555414155797 learning rate =  0.0001\n",
      "Epoch =  1762 Batch =  0 Loss =  [9.21780783] Gradient_max =  3.4065554089733863 learning rate =  0.0001\n",
      "Epoch =  1763 Batch =  0 Loss =  [9.21780782] Gradient_max =  3.4065554038236443 learning rate =  0.0001\n",
      "Epoch =  1764 Batch =  0 Loss =  [9.21780781] Gradient_max =  3.4065553987063644 learning rate =  0.0001\n",
      "Epoch =  1765 Batch =  0 Loss =  [9.2178078] Gradient_max =  3.406555393621342 learning rate =  0.0001\n",
      "Epoch =  1766 Batch =  0 Loss =  [9.21780779] Gradient_max =  3.4065553885683735 learning rate =  0.0001\n",
      "Epoch =  1767 Batch =  0 Loss =  [9.21780778] Gradient_max =  3.4065553835472553 learning rate =  0.0001\n",
      "Epoch =  1768 Batch =  0 Loss =  [9.21780777] Gradient_max =  3.4065553785577842 learning rate =  0.0001\n",
      "Epoch =  1769 Batch =  0 Loss =  [9.21780776] Gradient_max =  3.406555373599764 learning rate =  0.0001\n",
      "Epoch =  1770 Batch =  0 Loss =  [9.21780775] Gradient_max =  3.4065553686729912 learning rate =  0.0001\n",
      "Epoch =  1771 Batch =  0 Loss =  [9.21780774] Gradient_max =  3.4065553637772714 learning rate =  0.0001\n",
      "Epoch =  1772 Batch =  0 Loss =  [9.21780773] Gradient_max =  3.406555358912408 learning rate =  0.0001\n",
      "Epoch =  1773 Batch =  0 Loss =  [9.21780772] Gradient_max =  3.4065553540782014 learning rate =  0.0001\n",
      "Epoch =  1774 Batch =  0 Loss =  [9.21780771] Gradient_max =  3.406555349274463 learning rate =  0.0001\n",
      "Epoch =  1775 Batch =  0 Loss =  [9.2178077] Gradient_max =  3.4065553445009966 learning rate =  0.0001\n",
      "Epoch =  1776 Batch =  0 Loss =  [9.21780769] Gradient_max =  3.406555339757613 learning rate =  0.0001\n",
      "Epoch =  1777 Batch =  0 Loss =  [9.21780768] Gradient_max =  3.406555335044121 learning rate =  0.0001\n",
      "Epoch =  1778 Batch =  0 Loss =  [9.21780767] Gradient_max =  3.406555330360329 learning rate =  0.0001\n",
      "Epoch =  1779 Batch =  0 Loss =  [9.21780766] Gradient_max =  3.4065553257060546 learning rate =  0.0001\n",
      "Epoch =  1780 Batch =  0 Loss =  [9.21780765] Gradient_max =  3.4065553210811053 learning rate =  0.0001\n",
      "Epoch =  1781 Batch =  0 Loss =  [9.21780765] Gradient_max =  3.406555316485299 learning rate =  0.0001\n",
      "Epoch =  1782 Batch =  0 Loss =  [9.21780764] Gradient_max =  3.4065553119184506 learning rate =  0.0001\n",
      "Epoch =  1783 Batch =  0 Loss =  [9.21780763] Gradient_max =  3.4065553073803763 learning rate =  0.0001\n",
      "Epoch =  1784 Batch =  0 Loss =  [9.21780762] Gradient_max =  3.4065553028708955 learning rate =  0.0001\n",
      "Epoch =  1785 Batch =  0 Loss =  [9.21780761] Gradient_max =  3.4065552983898235 learning rate =  0.0001\n",
      "Epoch =  1786 Batch =  0 Loss =  [9.2178076] Gradient_max =  3.406555293936987 learning rate =  0.0001\n",
      "Epoch =  1787 Batch =  0 Loss =  [9.21780759] Gradient_max =  3.4065552895122013 learning rate =  0.0001\n",
      "Epoch =  1788 Batch =  0 Loss =  [9.21780758] Gradient_max =  3.4065552851152936 learning rate =  0.0001\n",
      "Epoch =  1789 Batch =  0 Loss =  [9.21780757] Gradient_max =  3.4065552807460837 learning rate =  0.0001\n",
      "Epoch =  1790 Batch =  0 Loss =  [9.21780757] Gradient_max =  3.4065552764044 learning rate =  0.0001\n",
      "Epoch =  1791 Batch =  0 Loss =  [9.21780756] Gradient_max =  3.4065552720900647 learning rate =  0.0001\n",
      "Epoch =  1792 Batch =  0 Loss =  [9.21780755] Gradient_max =  3.4065552678029083 learning rate =  0.0001\n",
      "Epoch =  1793 Batch =  0 Loss =  [9.21780754] Gradient_max =  3.4065552635427574 learning rate =  0.0001\n",
      "Epoch =  1794 Batch =  0 Loss =  [9.21780753] Gradient_max =  3.4065552593094424 learning rate =  0.0001\n",
      "Epoch =  1795 Batch =  0 Loss =  [9.21780752] Gradient_max =  3.4065552551027922 learning rate =  0.0001\n",
      "Epoch =  1796 Batch =  0 Loss =  [9.21780751] Gradient_max =  3.4065552509226387 learning rate =  0.0001\n",
      "Epoch =  1797 Batch =  0 Loss =  [9.21780751] Gradient_max =  3.406555246768814 learning rate =  0.0001\n",
      "Epoch =  1798 Batch =  0 Loss =  [9.2178075] Gradient_max =  3.4065552426411503 learning rate =  0.0001\n",
      "Epoch =  1799 Batch =  0 Loss =  [9.21780749] Gradient_max =  3.406555238539486 learning rate =  0.0001\n",
      "Epoch =  1800 Batch =  0 Loss =  [9.21780748] Gradient_max =  3.406555234463654 learning rate =  0.0001\n",
      "Epoch =  1801 Batch =  0 Loss =  [9.21780747] Gradient_max =  3.4065552304134905 learning rate =  0.0001\n",
      "Epoch =  1802 Batch =  0 Loss =  [9.21780747] Gradient_max =  3.4065552263888343 learning rate =  0.0001\n",
      "Epoch =  1803 Batch =  0 Loss =  [9.21780746] Gradient_max =  3.406555222389525 learning rate =  0.0001\n",
      "Epoch =  1804 Batch =  0 Loss =  [9.21780745] Gradient_max =  3.4065552184154018 learning rate =  0.0001\n",
      "Epoch =  1805 Batch =  0 Loss =  [9.21780744] Gradient_max =  3.4065552144663034 learning rate =  0.0001\n",
      "Epoch =  1806 Batch =  0 Loss =  [9.21780743] Gradient_max =  3.4065552105420753 learning rate =  0.0001\n",
      "Epoch =  1807 Batch =  0 Loss =  [9.21780743] Gradient_max =  3.4065552066425564 learning rate =  0.0001\n",
      "Epoch =  1808 Batch =  0 Loss =  [9.21780742] Gradient_max =  3.4065552027675934 learning rate =  0.0001\n",
      "Epoch =  1809 Batch =  0 Loss =  [9.21780741] Gradient_max =  3.40655519891703 learning rate =  0.0001\n",
      "Epoch =  1810 Batch =  0 Loss =  [9.2178074] Gradient_max =  3.406555195090713 learning rate =  0.0001\n",
      "Epoch =  1811 Batch =  0 Loss =  [9.21780739] Gradient_max =  3.406555191288487 learning rate =  0.0001\n",
      "Epoch =  1812 Batch =  0 Loss =  [9.21780739] Gradient_max =  3.406555187510203 learning rate =  0.0001\n",
      "Epoch =  1813 Batch =  0 Loss =  [9.21780738] Gradient_max =  3.4065551837557044 learning rate =  0.0001\n",
      "Epoch =  1814 Batch =  0 Loss =  [9.21780737] Gradient_max =  3.4065551800248466 learning rate =  0.0001\n",
      "Epoch =  1815 Batch =  0 Loss =  [9.21780736] Gradient_max =  3.406555176317478 learning rate =  0.0001\n",
      "Epoch =  1816 Batch =  0 Loss =  [9.21780736] Gradient_max =  3.406555172633449 learning rate =  0.0001\n",
      "Epoch =  1817 Batch =  0 Loss =  [9.21780735] Gradient_max =  3.406555168972611 learning rate =  0.0001\n",
      "Epoch =  1818 Batch =  0 Loss =  [9.21780734] Gradient_max =  3.406555165334823 learning rate =  0.0001\n",
      "Epoch =  1819 Batch =  0 Loss =  [9.21780734] Gradient_max =  3.4065551617199334 learning rate =  0.0001\n",
      "Epoch =  1820 Batch =  0 Loss =  [9.21780733] Gradient_max =  3.406555158127802 learning rate =  0.0001\n",
      "Epoch =  1821 Batch =  0 Loss =  [9.21780732] Gradient_max =  3.406555154558281 learning rate =  0.0001\n",
      "Epoch =  1822 Batch =  0 Loss =  [9.21780731] Gradient_max =  3.406555151011231 learning rate =  0.0001\n",
      "Epoch =  1823 Batch =  0 Loss =  [9.21780731] Gradient_max =  3.406555147486509 learning rate =  0.0001\n",
      "Epoch =  1824 Batch =  0 Loss =  [9.2178073] Gradient_max =  3.4065551439839714 learning rate =  0.0001\n",
      "Epoch =  1825 Batch =  0 Loss =  [9.21780729] Gradient_max =  3.406555140503482 learning rate =  0.0001\n",
      "Epoch =  1826 Batch =  0 Loss =  [9.21780729] Gradient_max =  3.4065551370448985 learning rate =  0.0001\n",
      "Epoch =  1827 Batch =  0 Loss =  [9.21780728] Gradient_max =  3.4065551336080855 learning rate =  0.0001\n",
      "Epoch =  1828 Batch =  0 Loss =  [9.21780727] Gradient_max =  3.406555130192903 learning rate =  0.0001\n",
      "Epoch =  1829 Batch =  0 Loss =  [9.21780727] Gradient_max =  3.4065551267992142 learning rate =  0.0001\n",
      "Epoch =  1830 Batch =  0 Loss =  [9.21780726] Gradient_max =  3.4065551234268847 learning rate =  0.0001\n",
      "Epoch =  1831 Batch =  0 Loss =  [9.21780725] Gradient_max =  3.406555120075778 learning rate =  0.0001\n",
      "Epoch =  1832 Batch =  0 Loss =  [9.21780725] Gradient_max =  3.406555116745762 learning rate =  0.0001\n",
      "Epoch =  1833 Batch =  0 Loss =  [9.21780724] Gradient_max =  3.4065551134367027 learning rate =  0.0001\n",
      "Epoch =  1834 Batch =  0 Loss =  [9.21780723] Gradient_max =  3.406555110148468 learning rate =  0.0001\n",
      "Epoch =  1835 Batch =  0 Loss =  [9.21780723] Gradient_max =  3.4065551068809254 learning rate =  0.0001\n",
      "Epoch =  1836 Batch =  0 Loss =  [9.21780722] Gradient_max =  3.406555103633946 learning rate =  0.0001\n",
      "Epoch =  1837 Batch =  0 Loss =  [9.21780721] Gradient_max =  3.4065551004073975 learning rate =  0.0001\n",
      "Epoch =  1838 Batch =  0 Loss =  [9.21780721] Gradient_max =  3.4065550972011525 learning rate =  0.0001\n",
      "Epoch =  1839 Batch =  0 Loss =  [9.2178072] Gradient_max =  3.4065550940150824 learning rate =  0.0001\n",
      "Epoch =  1840 Batch =  0 Loss =  [9.21780719] Gradient_max =  3.4065550908490585 learning rate =  0.0001\n",
      "Epoch =  1841 Batch =  0 Loss =  [9.21780719] Gradient_max =  3.406555087702958 learning rate =  0.0001\n",
      "Epoch =  1842 Batch =  0 Loss =  [9.21780718] Gradient_max =  3.4065550845766506 learning rate =  0.0001\n",
      "Epoch =  1843 Batch =  0 Loss =  [9.21780717] Gradient_max =  3.4065550814700134 learning rate =  0.0001\n",
      "Epoch =  1844 Batch =  0 Loss =  [9.21780717] Gradient_max =  3.406555078382923 learning rate =  0.0001\n",
      "Epoch =  1845 Batch =  0 Loss =  [9.21780716] Gradient_max =  3.4065550753152567 learning rate =  0.0001\n",
      "Epoch =  1846 Batch =  0 Loss =  [9.21780716] Gradient_max =  3.406555072266887 learning rate =  0.0001\n",
      "Epoch =  1847 Batch =  0 Loss =  [9.21780715] Gradient_max =  3.406555069237696 learning rate =  0.0001\n",
      "Epoch =  1848 Batch =  0 Loss =  [9.21780714] Gradient_max =  3.406555066227564 learning rate =  0.0001\n",
      "Epoch =  1849 Batch =  0 Loss =  [9.21780714] Gradient_max =  3.406555063236366 learning rate =  0.0001\n",
      "Epoch =  1850 Batch =  0 Loss =  [9.21780713] Gradient_max =  3.406555060263987 learning rate =  0.0001\n",
      "Epoch =  1851 Batch =  0 Loss =  [9.21780713] Gradient_max =  3.406555057310306 learning rate =  0.0001\n",
      "Epoch =  1852 Batch =  0 Loss =  [9.21780712] Gradient_max =  3.4065550543752043 learning rate =  0.0001\n",
      "Epoch =  1853 Batch =  0 Loss =  [9.21780711] Gradient_max =  3.4065550514585663 learning rate =  0.0001\n",
      "Epoch =  1854 Batch =  0 Loss =  [9.21780711] Gradient_max =  3.406555048560275 learning rate =  0.0001\n",
      "Epoch =  1855 Batch =  0 Loss =  [9.2178071] Gradient_max =  3.406555045680215 learning rate =  0.0001\n",
      "Epoch =  1856 Batch =  0 Loss =  [9.2178071] Gradient_max =  3.4065550428182694 learning rate =  0.0001\n",
      "Epoch =  1857 Batch =  0 Loss =  [9.21780709] Gradient_max =  3.406555039974326 learning rate =  0.0001\n",
      "Epoch =  1858 Batch =  0 Loss =  [9.21780709] Gradient_max =  3.406555037148268 learning rate =  0.0001\n",
      "Epoch =  1859 Batch =  0 Loss =  [9.21780708] Gradient_max =  3.406555034339986 learning rate =  0.0001\n",
      "Epoch =  1860 Batch =  0 Loss =  [9.21780707] Gradient_max =  3.4065550315493676 learning rate =  0.0001\n",
      "Epoch =  1861 Batch =  0 Loss =  [9.21780707] Gradient_max =  3.406555028776298 learning rate =  0.0001\n",
      "Epoch =  1862 Batch =  0 Loss =  [9.21780706] Gradient_max =  3.406555026020672 learning rate =  0.0001\n",
      "Epoch =  1863 Batch =  0 Loss =  [9.21780706] Gradient_max =  3.4065550232823716 learning rate =  0.0001\n",
      "Epoch =  1864 Batch =  0 Loss =  [9.21780705] Gradient_max =  3.4065550205612967 learning rate =  0.0001\n",
      "Epoch =  1865 Batch =  0 Loss =  [9.21780705] Gradient_max =  3.4065550178573316 learning rate =  0.0001\n",
      "Epoch =  1866 Batch =  0 Loss =  [9.21780704] Gradient_max =  3.4065550151703703 learning rate =  0.0001\n",
      "Epoch =  1867 Batch =  0 Loss =  [9.21780704] Gradient_max =  3.406555012500308 learning rate =  0.0001\n",
      "Epoch =  1868 Batch =  0 Loss =  [9.21780703] Gradient_max =  3.4065550098470325 learning rate =  0.0001\n",
      "Epoch =  1869 Batch =  0 Loss =  [9.21780703] Gradient_max =  3.406555007210444 learning rate =  0.0001\n",
      "Epoch =  1870 Batch =  0 Loss =  [9.21780702] Gradient_max =  3.4065550045904334 learning rate =  0.0001\n",
      "Epoch =  1871 Batch =  0 Loss =  [9.21780702] Gradient_max =  3.4065550019868986 learning rate =  0.0001\n",
      "Epoch =  1872 Batch =  0 Loss =  [9.21780701] Gradient_max =  3.406554999399733 learning rate =  0.0001\n",
      "Epoch =  1873 Batch =  0 Loss =  [9.21780701] Gradient_max =  3.406554996828835 learning rate =  0.0001\n",
      "Epoch =  1874 Batch =  0 Loss =  [9.217807] Gradient_max =  3.406554994274102 learning rate =  0.0001\n",
      "Epoch =  1875 Batch =  0 Loss =  [9.217807] Gradient_max =  3.4065549917354314 learning rate =  0.0001\n",
      "Epoch =  1876 Batch =  0 Loss =  [9.21780699] Gradient_max =  3.4065549892127227 learning rate =  0.0001\n",
      "Epoch =  1877 Batch =  0 Loss =  [9.21780698] Gradient_max =  3.406554986705875 learning rate =  0.0001\n",
      "Epoch =  1878 Batch =  0 Loss =  [9.21780698] Gradient_max =  3.406554984214785 learning rate =  0.0001\n",
      "Epoch =  1879 Batch =  0 Loss =  [9.21780697] Gradient_max =  3.4065549817393608 learning rate =  0.0001\n",
      "Epoch =  1880 Batch =  0 Loss =  [9.21780697] Gradient_max =  3.406554979279495 learning rate =  0.0001\n",
      "Epoch =  1881 Batch =  0 Loss =  [9.21780697] Gradient_max =  3.406554976835094 learning rate =  0.0001\n",
      "Epoch =  1882 Batch =  0 Loss =  [9.21780696] Gradient_max =  3.40655497440606 learning rate =  0.0001\n",
      "Epoch =  1883 Batch =  0 Loss =  [9.21780696] Gradient_max =  3.4065549719922954 learning rate =  0.0001\n",
      "Epoch =  1884 Batch =  0 Loss =  [9.21780695] Gradient_max =  3.406554969593705 learning rate =  0.0001\n",
      "Epoch =  1885 Batch =  0 Loss =  [9.21780695] Gradient_max =  3.4065549672101922 learning rate =  0.0001\n",
      "Epoch =  1886 Batch =  0 Loss =  [9.21780694] Gradient_max =  3.4065549648416615 learning rate =  0.0001\n",
      "Epoch =  1887 Batch =  0 Loss =  [9.21780694] Gradient_max =  3.4065549624880194 learning rate =  0.0001\n",
      "Epoch =  1888 Batch =  0 Loss =  [9.21780693] Gradient_max =  3.4065549601491703 learning rate =  0.0001\n",
      "Epoch =  1889 Batch =  0 Loss =  [9.21780693] Gradient_max =  3.406554957825023 learning rate =  0.0001\n",
      "Epoch =  1890 Batch =  0 Loss =  [9.21780692] Gradient_max =  3.4065549555154835 learning rate =  0.0001\n",
      "Epoch =  1891 Batch =  0 Loss =  [9.21780692] Gradient_max =  3.406554953220462 learning rate =  0.0001\n",
      "Epoch =  1892 Batch =  0 Loss =  [9.21780691] Gradient_max =  3.4065549509398645 learning rate =  0.0001\n",
      "Epoch =  1893 Batch =  0 Loss =  [9.21780691] Gradient_max =  3.4065549486735973 learning rate =  0.0001\n",
      "Epoch =  1894 Batch =  0 Loss =  [9.2178069] Gradient_max =  3.406554946421577 learning rate =  0.0001\n",
      "Epoch =  1895 Batch =  0 Loss =  [9.2178069] Gradient_max =  3.4065549441837093 learning rate =  0.0001\n",
      "Epoch =  1896 Batch =  0 Loss =  [9.2178069] Gradient_max =  3.406554941959907 learning rate =  0.0001\n",
      "Epoch =  1897 Batch =  0 Loss =  [9.21780689] Gradient_max =  3.406554939750076 learning rate =  0.0001\n",
      "Epoch =  1898 Batch =  0 Loss =  [9.21780689] Gradient_max =  3.406554937554136 learning rate =  0.0001\n",
      "Epoch =  1899 Batch =  0 Loss =  [9.21780688] Gradient_max =  3.406554935371993 learning rate =  0.0001\n",
      "Epoch =  1900 Batch =  0 Loss =  [9.21780688] Gradient_max =  3.4065549332035654 learning rate =  0.0001\n",
      "Epoch =  1901 Batch =  0 Loss =  [9.21780687] Gradient_max =  3.4065549310487624 learning rate =  0.0001\n",
      "Epoch =  1902 Batch =  0 Loss =  [9.21780687] Gradient_max =  3.4065549289075014 learning rate =  0.0001\n",
      "Epoch =  1903 Batch =  0 Loss =  [9.21780686] Gradient_max =  3.4065549267796946 learning rate =  0.0001\n",
      "Epoch =  1904 Batch =  0 Loss =  [9.21780686] Gradient_max =  3.4065549246652593 learning rate =  0.0001\n",
      "Epoch =  1905 Batch =  0 Loss =  [9.21780686] Gradient_max =  3.4065549225641076 learning rate =  0.0001\n",
      "Epoch =  1906 Batch =  0 Loss =  [9.21780685] Gradient_max =  3.40655492047616 learning rate =  0.0001\n",
      "Epoch =  1907 Batch =  0 Loss =  [9.21780685] Gradient_max =  3.406554918401331 learning rate =  0.0001\n",
      "Epoch =  1908 Batch =  0 Loss =  [9.21780684] Gradient_max =  3.4065549163395388 learning rate =  0.0001\n",
      "Epoch =  1909 Batch =  0 Loss =  [9.21780684] Gradient_max =  3.4065549142907017 learning rate =  0.0001\n",
      "Epoch =  1910 Batch =  0 Loss =  [9.21780684] Gradient_max =  3.4065549122547356 learning rate =  0.0001\n",
      "Epoch =  1911 Batch =  0 Loss =  [9.21780683] Gradient_max =  3.406554910231561 learning rate =  0.0001\n",
      "Epoch =  1912 Batch =  0 Loss =  [9.21780683] Gradient_max =  3.4065549082210977 learning rate =  0.0001\n",
      "Epoch =  1913 Batch =  0 Loss =  [9.21780682] Gradient_max =  3.4065549062232656 learning rate =  0.0001\n",
      "Epoch =  1914 Batch =  0 Loss =  [9.21780682] Gradient_max =  3.406554904237984 learning rate =  0.0001\n",
      "Epoch =  1915 Batch =  0 Loss =  [9.21780682] Gradient_max =  3.406554902265175 learning rate =  0.0001\n",
      "Epoch =  1916 Batch =  0 Loss =  [9.21780681] Gradient_max =  3.4065549003047577 learning rate =  0.0001\n",
      "Epoch =  1917 Batch =  0 Loss =  [9.21780681] Gradient_max =  3.406554898356656 learning rate =  0.0001\n",
      "Epoch =  1918 Batch =  0 Loss =  [9.2178068] Gradient_max =  3.406554896420792 learning rate =  0.0001\n",
      "Epoch =  1919 Batch =  0 Loss =  [9.2178068] Gradient_max =  3.4065548944970883 learning rate =  0.0001\n",
      "Epoch =  1920 Batch =  0 Loss =  [9.2178068] Gradient_max =  3.40655489258547 learning rate =  0.0001\n",
      "Epoch =  1921 Batch =  0 Loss =  [9.21780679] Gradient_max =  3.406554890685858 learning rate =  0.0001\n",
      "Epoch =  1922 Batch =  0 Loss =  [9.21780679] Gradient_max =  3.406554888798177 learning rate =  0.0001\n",
      "Epoch =  1923 Batch =  0 Loss =  [9.21780678] Gradient_max =  3.406554886922356 learning rate =  0.0001\n",
      "Epoch =  1924 Batch =  0 Loss =  [9.21780678] Gradient_max =  3.406554885058315 learning rate =  0.0001\n",
      "Epoch =  1925 Batch =  0 Loss =  [9.21780678] Gradient_max =  3.406554883205983 learning rate =  0.0001\n",
      "Epoch =  1926 Batch =  0 Loss =  [9.21780677] Gradient_max =  3.4065548813652833 learning rate =  0.0001\n",
      "Epoch =  1927 Batch =  0 Loss =  [9.21780677] Gradient_max =  3.4065548795361478 learning rate =  0.0001\n",
      "Epoch =  1928 Batch =  0 Loss =  [9.21780677] Gradient_max =  3.406554877718497 learning rate =  0.0001\n",
      "Epoch =  1929 Batch =  0 Loss =  [9.21780676] Gradient_max =  3.406554875912263 learning rate =  0.0001\n",
      "Epoch =  1930 Batch =  0 Loss =  [9.21780676] Gradient_max =  3.406554874117375 learning rate =  0.0001\n",
      "Epoch =  1931 Batch =  0 Loss =  [9.21780676] Gradient_max =  3.406554872333757 learning rate =  0.0001\n",
      "Epoch =  1932 Batch =  0 Loss =  [9.21780675] Gradient_max =  3.4065548705613393 learning rate =  0.0001\n",
      "Epoch =  1933 Batch =  0 Loss =  [9.21780675] Gradient_max =  3.406554868800054 learning rate =  0.0001\n",
      "Epoch =  1934 Batch =  0 Loss =  [9.21780674] Gradient_max =  3.4065548670498305 learning rate =  0.0001\n",
      "Epoch =  1935 Batch =  0 Loss =  [9.21780674] Gradient_max =  3.406554865310594 learning rate =  0.0001\n",
      "Epoch =  1936 Batch =  0 Loss =  [9.21780674] Gradient_max =  3.406554863582283 learning rate =  0.0001\n",
      "Epoch =  1937 Batch =  0 Loss =  [9.21780673] Gradient_max =  3.406554861864823 learning rate =  0.0001\n",
      "Epoch =  1938 Batch =  0 Loss =  [9.21780673] Gradient_max =  3.4065548601581495 learning rate =  0.0001\n",
      "Epoch =  1939 Batch =  0 Loss =  [9.21780673] Gradient_max =  3.40655485846219 learning rate =  0.0001\n",
      "Epoch =  1940 Batch =  0 Loss =  [9.21780672] Gradient_max =  3.4065548567768804 learning rate =  0.0001\n",
      "Epoch =  1941 Batch =  0 Loss =  [9.21780672] Gradient_max =  3.4065548551021534 learning rate =  0.0001\n",
      "Epoch =  1942 Batch =  0 Loss =  [9.21780672] Gradient_max =  3.406554853437942 learning rate =  0.0001\n",
      "Epoch =  1943 Batch =  0 Loss =  [9.21780671] Gradient_max =  3.40655485178418 learning rate =  0.0001\n",
      "Epoch =  1944 Batch =  0 Loss =  [9.21780671] Gradient_max =  3.4065548501408 learning rate =  0.0001\n",
      "Epoch =  1945 Batch =  0 Loss =  [9.21780671] Gradient_max =  3.40655484850774 learning rate =  0.0001\n",
      "Epoch =  1946 Batch =  0 Loss =  [9.2178067] Gradient_max =  3.406554846884931 learning rate =  0.0001\n",
      "Epoch =  1947 Batch =  0 Loss =  [9.2178067] Gradient_max =  3.406554845272312 learning rate =  0.0001\n",
      "Epoch =  1948 Batch =  0 Loss =  [9.2178067] Gradient_max =  3.4065548436698165 learning rate =  0.0001\n",
      "Epoch =  1949 Batch =  0 Loss =  [9.21780669] Gradient_max =  3.40655484207738 learning rate =  0.0001\n",
      "Epoch =  1950 Batch =  0 Loss =  [9.21780669] Gradient_max =  3.406554840494944 learning rate =  0.0001\n",
      "Epoch =  1951 Batch =  0 Loss =  [9.21780669] Gradient_max =  3.40655483892244 learning rate =  0.0001\n",
      "Epoch =  1952 Batch =  0 Loss =  [9.21780668] Gradient_max =  3.4065548373598094 learning rate =  0.0001\n",
      "Epoch =  1953 Batch =  0 Loss =  [9.21780668] Gradient_max =  3.406554835806987 learning rate =  0.0001\n",
      "Epoch =  1954 Batch =  0 Loss =  [9.21780668] Gradient_max =  3.4065548342639134 learning rate =  0.0001\n",
      "Epoch =  1955 Batch =  0 Loss =  [9.21780668] Gradient_max =  3.406554832730524 learning rate =  0.0001\n",
      "Epoch =  1956 Batch =  0 Loss =  [9.21780667] Gradient_max =  3.406554831206762 learning rate =  0.0001\n",
      "Epoch =  1957 Batch =  0 Loss =  [9.21780667] Gradient_max =  3.4065548296925647 learning rate =  0.0001\n",
      "Epoch =  1958 Batch =  0 Loss =  [9.21780667] Gradient_max =  3.40655482818787 learning rate =  0.0001\n",
      "Epoch =  1959 Batch =  0 Loss =  [9.21780666] Gradient_max =  3.406554826692621 learning rate =  0.0001\n",
      "Epoch =  1960 Batch =  0 Loss =  [9.21780666] Gradient_max =  3.4065548252067575 learning rate =  0.0001\n",
      "Epoch =  1961 Batch =  0 Loss =  [9.21780666] Gradient_max =  3.406554823730221 learning rate =  0.0001\n",
      "Epoch =  1962 Batch =  0 Loss =  [9.21780665] Gradient_max =  3.4065548222629505 learning rate =  0.0001\n",
      "Epoch =  1963 Batch =  0 Loss =  [9.21780665] Gradient_max =  3.40655482080489 learning rate =  0.0001\n",
      "Epoch =  1964 Batch =  0 Loss =  [9.21780665] Gradient_max =  3.406554819355982 learning rate =  0.0001\n",
      "Epoch =  1965 Batch =  0 Loss =  [9.21780665] Gradient_max =  3.4065548179161658 learning rate =  0.0001\n",
      "Epoch =  1966 Batch =  0 Loss =  [9.21780664] Gradient_max =  3.406554816485388 learning rate =  0.0001\n",
      "Epoch =  1967 Batch =  0 Loss =  [9.21780664] Gradient_max =  3.406554815063588 learning rate =  0.0001\n",
      "Epoch =  1968 Batch =  0 Loss =  [9.21780664] Gradient_max =  3.406554813650712 learning rate =  0.0001\n",
      "Epoch =  1969 Batch =  0 Loss =  [9.21780663] Gradient_max =  3.4065548122467018 learning rate =  0.0001\n",
      "Epoch =  1970 Batch =  0 Loss =  [9.21780663] Gradient_max =  3.4065548108515027 learning rate =  0.0001\n",
      "Epoch =  1971 Batch =  0 Loss =  [9.21780663] Gradient_max =  3.4065548094650584 learning rate =  0.0001\n",
      "Epoch =  1972 Batch =  0 Loss =  [9.21780663] Gradient_max =  3.406554808087316 learning rate =  0.0001\n",
      "Epoch =  1973 Batch =  0 Loss =  [9.21780662] Gradient_max =  3.406554806718219 learning rate =  0.0001\n",
      "Epoch =  1974 Batch =  0 Loss =  [9.21780662] Gradient_max =  3.406554805357711 learning rate =  0.0001\n",
      "Epoch =  1975 Batch =  0 Loss =  [9.21780662] Gradient_max =  3.406554804005743 learning rate =  0.0001\n",
      "Epoch =  1976 Batch =  0 Loss =  [9.21780661] Gradient_max =  3.4065548026622574 learning rate =  0.0001\n",
      "Epoch =  1977 Batch =  0 Loss =  [9.21780661] Gradient_max =  3.406554801327202 learning rate =  0.0001\n",
      "Epoch =  1978 Batch =  0 Loss =  [9.21780661] Gradient_max =  3.4065548000005226 learning rate =  0.0001\n",
      "Epoch =  1979 Batch =  0 Loss =  [9.21780661] Gradient_max =  3.4065547986821687 learning rate =  0.0001\n",
      "Epoch =  1980 Batch =  0 Loss =  [9.2178066] Gradient_max =  3.406554797372087 learning rate =  0.0001\n",
      "Epoch =  1981 Batch =  0 Loss =  [9.2178066] Gradient_max =  3.4065547960702256 learning rate =  0.0001\n",
      "Epoch =  1982 Batch =  0 Loss =  [9.2178066] Gradient_max =  3.406554794776532 learning rate =  0.0001\n",
      "Epoch =  1983 Batch =  0 Loss =  [9.2178066] Gradient_max =  3.406554793490954 learning rate =  0.0001\n",
      "Epoch =  1984 Batch =  0 Loss =  [9.21780659] Gradient_max =  3.4065547922134436 learning rate =  0.0001\n",
      "Epoch =  1985 Batch =  0 Loss =  [9.21780659] Gradient_max =  3.4065547909439475 learning rate =  0.0001\n",
      "Epoch =  1986 Batch =  0 Loss =  [9.21780659] Gradient_max =  3.406554789682415 learning rate =  0.0001\n",
      "Epoch =  1987 Batch =  0 Loss =  [9.21780659] Gradient_max =  3.4065547884287986 learning rate =  0.0001\n",
      "Epoch =  1988 Batch =  0 Loss =  [9.21780658] Gradient_max =  3.4065547871830457 learning rate =  0.0001\n",
      "Epoch =  1989 Batch =  0 Loss =  [9.21780658] Gradient_max =  3.406554785945106 learning rate =  0.0001\n",
      "Epoch =  1990 Batch =  0 Loss =  [9.21780658] Gradient_max =  3.406554784714936 learning rate =  0.0001\n",
      "Epoch =  1991 Batch =  0 Loss =  [9.21780658] Gradient_max =  3.4065547834924805 learning rate =  0.0001\n",
      "Epoch =  1992 Batch =  0 Loss =  [9.21780657] Gradient_max =  3.4065547822776954 learning rate =  0.0001\n",
      "Epoch =  1993 Batch =  0 Loss =  [9.21780657] Gradient_max =  3.406554781070529 learning rate =  0.0001\n",
      "Epoch =  1994 Batch =  0 Loss =  [9.21780657] Gradient_max =  3.4065547798709352 learning rate =  0.0001\n",
      "Epoch =  1995 Batch =  0 Loss =  [9.21780657] Gradient_max =  3.4065547786788666 learning rate =  0.0001\n",
      "Epoch =  1996 Batch =  0 Loss =  [9.21780656] Gradient_max =  3.406554777494275 learning rate =  0.0001\n",
      "Epoch =  1997 Batch =  0 Loss =  [9.21780656] Gradient_max =  3.406554776317115 learning rate =  0.0001\n",
      "Epoch =  1998 Batch =  0 Loss =  [9.21780656] Gradient_max =  3.406554775147336 learning rate =  0.0001\n",
      "Epoch =  1999 Batch =  0 Loss =  [9.21780656] Gradient_max =  3.4065547739848983 learning rate =  0.0001\n",
      "Epoch =  2000 Batch =  0 Loss =  [9.21780655] Gradient_max =  3.406554772829747 learning rate =  0.0001\n",
      "Epoch =  2001 Batch =  0 Loss =  [9.21780655] Gradient_max =  3.406554771681844 learning rate =  0.0001\n",
      "Epoch =  2002 Batch =  0 Loss =  [9.21780655] Gradient_max =  3.406554770541139 learning rate =  0.0001\n",
      "Epoch =  2003 Batch =  0 Loss =  [9.21780655] Gradient_max =  3.406554769407589 learning rate =  0.0001\n",
      "Epoch =  2004 Batch =  0 Loss =  [9.21780655] Gradient_max =  3.4065547682811483 learning rate =  0.0001\n",
      "Epoch =  2005 Batch =  0 Loss =  [9.21780654] Gradient_max =  3.406554767161773 learning rate =  0.0001\n",
      "Epoch =  2006 Batch =  0 Loss =  [9.21780654] Gradient_max =  3.406554766049417 learning rate =  0.0001\n",
      "Epoch =  2007 Batch =  0 Loss =  [9.21780654] Gradient_max =  3.406554764944037 learning rate =  0.0001\n",
      "Epoch =  2008 Batch =  0 Loss =  [9.21780654] Gradient_max =  3.40655476384559 learning rate =  0.0001\n",
      "Epoch =  2009 Batch =  0 Loss =  [9.21780653] Gradient_max =  3.406554762754032 learning rate =  0.0001\n",
      "Epoch =  2010 Batch =  0 Loss =  [9.21780653] Gradient_max =  3.4065547616693164 learning rate =  0.0001\n",
      "Epoch =  2011 Batch =  0 Loss =  [9.21780653] Gradient_max =  3.4065547605914057 learning rate =  0.0001\n",
      "Epoch =  2012 Batch =  0 Loss =  [9.21780653] Gradient_max =  3.4065547595202545 learning rate =  0.0001\n",
      "Epoch =  2013 Batch =  0 Loss =  [9.21780653] Gradient_max =  3.4065547584558193 learning rate =  0.0001\n",
      "Epoch =  2014 Batch =  0 Loss =  [9.21780652] Gradient_max =  3.40655475739806 learning rate =  0.0001\n",
      "Epoch =  2015 Batch =  0 Loss =  [9.21780652] Gradient_max =  3.406554756346933 learning rate =  0.0001\n",
      "Epoch =  2016 Batch =  0 Loss =  [9.21780652] Gradient_max =  3.4065547553023965 learning rate =  0.0001\n",
      "Epoch =  2017 Batch =  0 Loss =  [9.21780652] Gradient_max =  3.4065547542644086 learning rate =  0.0001\n",
      "Epoch =  2018 Batch =  0 Loss =  [9.21780652] Gradient_max =  3.4065547532329297 learning rate =  0.0001\n",
      "Epoch =  2019 Batch =  0 Loss =  [9.21780651] Gradient_max =  3.40655475220792 learning rate =  0.0001\n",
      "Epoch =  2020 Batch =  0 Loss =  [9.21780651] Gradient_max =  3.4065547511893355 learning rate =  0.0001\n",
      "Epoch =  2021 Batch =  0 Loss =  [9.21780651] Gradient_max =  3.40655475017714 learning rate =  0.0001\n",
      "Epoch =  2022 Batch =  0 Loss =  [9.21780651] Gradient_max =  3.406554749171289 learning rate =  0.0001\n",
      "Epoch =  2023 Batch =  0 Loss =  [9.21780651] Gradient_max =  3.406554748171744 learning rate =  0.0001\n",
      "Epoch =  2024 Batch =  0 Loss =  [9.2178065] Gradient_max =  3.406554747178468 learning rate =  0.0001\n",
      "Epoch =  2025 Batch =  0 Loss =  [9.2178065] Gradient_max =  3.406554746191417 learning rate =  0.0001\n",
      "Epoch =  2026 Batch =  0 Loss =  [9.2178065] Gradient_max =  3.406554745210555 learning rate =  0.0001\n",
      "Epoch =  2027 Batch =  0 Loss =  [9.2178065] Gradient_max =  3.406554744235843 learning rate =  0.0001\n",
      "Epoch =  2028 Batch =  0 Loss =  [9.2178065] Gradient_max =  3.406554743267239 learning rate =  0.0001\n",
      "Epoch =  2029 Batch =  0 Loss =  [9.21780649] Gradient_max =  3.406554742304709 learning rate =  0.0001\n",
      "Epoch =  2030 Batch =  0 Loss =  [9.21780649] Gradient_max =  3.406554741348213 learning rate =  0.0001\n",
      "Epoch =  2031 Batch =  0 Loss =  [9.21780649] Gradient_max =  3.4065547403977128 learning rate =  0.0001\n",
      "Epoch =  2032 Batch =  0 Loss =  [9.21780649] Gradient_max =  3.406554739453173 learning rate =  0.0001\n",
      "Epoch =  2033 Batch =  0 Loss =  [9.21780649] Gradient_max =  3.4065547385145516 learning rate =  0.0001\n",
      "Epoch =  2034 Batch =  0 Loss =  [9.21780648] Gradient_max =  3.4065547375818155 learning rate =  0.0001\n",
      "Epoch =  2035 Batch =  0 Loss =  [9.21780648] Gradient_max =  3.406554736654926 learning rate =  0.0001\n",
      "Epoch =  2036 Batch =  0 Loss =  [9.21780648] Gradient_max =  3.4065547357338466 learning rate =  0.0001\n",
      "Epoch =  2037 Batch =  0 Loss =  [9.21780648] Gradient_max =  3.4065547348185397 learning rate =  0.0001\n",
      "Epoch =  2038 Batch =  0 Loss =  [9.21780648] Gradient_max =  3.4065547339089703 learning rate =  0.0001\n",
      "Epoch =  2039 Batch =  0 Loss =  [9.21780647] Gradient_max =  3.4065547330051036 learning rate =  0.0001\n",
      "Epoch =  2040 Batch =  0 Loss =  [9.21780647] Gradient_max =  3.406554732106902 learning rate =  0.0001\n",
      "Epoch =  2041 Batch =  0 Loss =  [9.21780647] Gradient_max =  3.4065547312143303 learning rate =  0.0001\n",
      "Epoch =  2042 Batch =  0 Loss =  [9.21780647] Gradient_max =  3.4065547303273522 learning rate =  0.0001\n",
      "Epoch =  2043 Batch =  0 Loss =  [9.21780647] Gradient_max =  3.406554729445932 learning rate =  0.0001\n",
      "Epoch =  2044 Batch =  0 Loss =  [9.21780647] Gradient_max =  3.406554728570039 learning rate =  0.0001\n",
      "Epoch =  2045 Batch =  0 Loss =  [9.21780646] Gradient_max =  3.4065547276996355 learning rate =  0.0001\n",
      "Epoch =  2046 Batch =  0 Loss =  [9.21780646] Gradient_max =  3.406554726834684 learning rate =  0.0001\n",
      "Epoch =  2047 Batch =  0 Loss =  [9.21780646] Gradient_max =  3.4065547259751563 learning rate =  0.0001\n",
      "Epoch =  2048 Batch =  0 Loss =  [9.21780646] Gradient_max =  3.406554725121014 learning rate =  0.0001\n",
      "Epoch =  2049 Batch =  0 Loss =  [9.21780646] Gradient_max =  3.4065547242722256 learning rate =  0.0001\n",
      "Epoch =  2050 Batch =  0 Loss =  [9.21780646] Gradient_max =  3.406554723428756 learning rate =  0.0001\n",
      "Epoch =  2051 Batch =  0 Loss =  [9.21780645] Gradient_max =  3.4065547225905712 learning rate =  0.0001\n",
      "Epoch =  2052 Batch =  0 Loss =  [9.21780645] Gradient_max =  3.406554721757639 learning rate =  0.0001\n",
      "Epoch =  2053 Batch =  0 Loss =  [9.21780645] Gradient_max =  3.406554720929929 learning rate =  0.0001\n",
      "Epoch =  2054 Batch =  0 Loss =  [9.21780645] Gradient_max =  3.4065547201074042 learning rate =  0.0001\n",
      "Epoch =  2055 Batch =  0 Loss =  [9.21780645] Gradient_max =  3.4065547192900336 learning rate =  0.0001\n",
      "Epoch =  2056 Batch =  0 Loss =  [9.21780645] Gradient_max =  3.406554718477786 learning rate =  0.0001\n",
      "Epoch =  2057 Batch =  0 Loss =  [9.21780644] Gradient_max =  3.4065547176706255 learning rate =  0.0001\n",
      "Epoch =  2058 Batch =  0 Loss =  [9.21780644] Gradient_max =  3.4065547168685244 learning rate =  0.0001\n",
      "Epoch =  2059 Batch =  0 Loss =  [9.21780644] Gradient_max =  3.406554716071449 learning rate =  0.0001\n",
      "Epoch =  2060 Batch =  0 Loss =  [9.21780644] Gradient_max =  3.406554715279369 learning rate =  0.0001\n",
      "Epoch =  2061 Batch =  0 Loss =  [9.21780644] Gradient_max =  3.4065547144922506 learning rate =  0.0001\n",
      "Epoch =  2062 Batch =  0 Loss =  [9.21780644] Gradient_max =  3.4065547137100642 learning rate =  0.0001\n",
      "Epoch =  2063 Batch =  0 Loss =  [9.21780643] Gradient_max =  3.4065547129327793 learning rate =  0.0001\n",
      "Epoch =  2064 Batch =  0 Loss =  [9.21780643] Gradient_max =  3.406554712160365 learning rate =  0.0001\n",
      "Epoch =  2065 Batch =  0 Loss =  [9.21780643] Gradient_max =  3.4065547113927876 learning rate =  0.0001\n",
      "Epoch =  2066 Batch =  0 Loss =  [9.21780643] Gradient_max =  3.406554710630024 learning rate =  0.0001\n",
      "Epoch =  2067 Batch =  0 Loss =  [9.21780643] Gradient_max =  3.4065547098720366 learning rate =  0.0001\n",
      "Epoch =  2068 Batch =  0 Loss =  [9.21780643] Gradient_max =  3.4065547091187978 learning rate =  0.0001\n",
      "Epoch =  2069 Batch =  0 Loss =  [9.21780642] Gradient_max =  3.4065547083702783 learning rate =  0.0001\n",
      "Epoch =  2070 Batch =  0 Loss =  [9.21780642] Gradient_max =  3.406554707626447 learning rate =  0.0001\n",
      "Epoch =  2071 Batch =  0 Loss =  [9.21780642] Gradient_max =  3.406554706887276 learning rate =  0.0001\n",
      "Epoch =  2072 Batch =  0 Loss =  [9.21780642] Gradient_max =  3.406554706152736 learning rate =  0.0001\n",
      "Epoch =  2073 Batch =  0 Loss =  [9.21780642] Gradient_max =  3.406554705422798 learning rate =  0.0001\n",
      "Epoch =  2074 Batch =  0 Loss =  [9.21780642] Gradient_max =  3.4065547046974345 learning rate =  0.0001\n",
      "Epoch =  2075 Batch =  0 Loss =  [9.21780642] Gradient_max =  3.4065547039766133 learning rate =  0.0001\n",
      "Epoch =  2076 Batch =  0 Loss =  [9.21780641] Gradient_max =  3.406554703260307 learning rate =  0.0001\n",
      "Epoch =  2077 Batch =  0 Loss =  [9.21780641] Gradient_max =  3.4065547025484904 learning rate =  0.0001\n",
      "Epoch =  2078 Batch =  0 Loss =  [9.21780641] Gradient_max =  3.4065547018411286 learning rate =  0.0001\n",
      "Epoch =  2079 Batch =  0 Loss =  [9.21780641] Gradient_max =  3.4065547011382002 learning rate =  0.0001\n",
      "Epoch =  2080 Batch =  0 Loss =  [9.21780641] Gradient_max =  3.406554700439675 learning rate =  0.0001\n",
      "Epoch =  2081 Batch =  0 Loss =  [9.21780641] Gradient_max =  3.4065546997455236 learning rate =  0.0001\n",
      "Epoch =  2082 Batch =  0 Loss =  [9.21780641] Gradient_max =  3.406554699055721 learning rate =  0.0001\n",
      "Epoch =  2083 Batch =  0 Loss =  [9.2178064] Gradient_max =  3.406554698370239 learning rate =  0.0001\n",
      "Epoch =  2084 Batch =  0 Loss =  [9.2178064] Gradient_max =  3.4065546976890513 learning rate =  0.0001\n",
      "Epoch =  2085 Batch =  0 Loss =  [9.2178064] Gradient_max =  3.40655469701213 learning rate =  0.0001\n",
      "Epoch =  2086 Batch =  0 Loss =  [9.2178064] Gradient_max =  3.406554696339447 learning rate =  0.0001\n",
      "Epoch =  2087 Batch =  0 Loss =  [9.2178064] Gradient_max =  3.4065546956709785 learning rate =  0.0001\n",
      "Epoch =  2088 Batch =  0 Loss =  [9.2178064] Gradient_max =  3.4065546950066974 learning rate =  0.0001\n",
      "Epoch =  2089 Batch =  0 Loss =  [9.2178064] Gradient_max =  3.406554694346575 learning rate =  0.0001\n",
      "Epoch =  2090 Batch =  0 Loss =  [9.21780639] Gradient_max =  3.4065546936905884 learning rate =  0.0001\n",
      "Epoch =  2091 Batch =  0 Loss =  [9.21780639] Gradient_max =  3.4065546930387107 learning rate =  0.0001\n",
      "Epoch =  2092 Batch =  0 Loss =  [9.21780639] Gradient_max =  3.4065546923909147 learning rate =  0.0001\n",
      "Epoch =  2093 Batch =  0 Loss =  [9.21780639] Gradient_max =  3.406554691747175 learning rate =  0.0001\n",
      "Epoch =  2094 Batch =  0 Loss =  [9.21780639] Gradient_max =  3.4065546911074667 learning rate =  0.0001\n",
      "Epoch =  2095 Batch =  0 Loss =  [9.21780639] Gradient_max =  3.4065546904717667 learning rate =  0.0001\n",
      "Epoch =  2096 Batch =  0 Loss =  [9.21780639] Gradient_max =  3.406554689840046 learning rate =  0.0001\n",
      "Epoch =  2097 Batch =  0 Loss =  [9.21780639] Gradient_max =  3.4065546892122813 learning rate =  0.0001\n",
      "Epoch =  2098 Batch =  0 Loss =  [9.21780638] Gradient_max =  3.406554688588448 learning rate =  0.0001\n",
      "Epoch =  2099 Batch =  0 Loss =  [9.21780638] Gradient_max =  3.406554687968522 learning rate =  0.0001\n",
      "Epoch =  2100 Batch =  0 Loss =  [9.21780638] Gradient_max =  3.406554687352478 learning rate =  0.0001\n",
      "Epoch =  2101 Batch =  0 Loss =  [9.21780638] Gradient_max =  3.4065546867402894 learning rate =  0.0001\n",
      "Epoch =  2102 Batch =  0 Loss =  [9.21780638] Gradient_max =  3.4065546861319356 learning rate =  0.0001\n",
      "Epoch =  2103 Batch =  0 Loss =  [9.21780638] Gradient_max =  3.4065546855273916 learning rate =  0.0001\n",
      "Epoch =  2104 Batch =  0 Loss =  [9.21780638] Gradient_max =  3.406554684926631 learning rate =  0.0001\n",
      "Epoch =  2105 Batch =  0 Loss =  [9.21780638] Gradient_max =  3.4065546843296355 learning rate =  0.0001\n",
      "Epoch =  2106 Batch =  0 Loss =  [9.21780637] Gradient_max =  3.406554683736376 learning rate =  0.0001\n",
      "Epoch =  2107 Batch =  0 Loss =  [9.21780637] Gradient_max =  3.40655468314683 learning rate =  0.0001\n",
      "Epoch =  2108 Batch =  0 Loss =  [9.21780637] Gradient_max =  3.4065546825609765 learning rate =  0.0001\n",
      "Epoch =  2109 Batch =  0 Loss =  [9.21780637] Gradient_max =  3.406554681978792 learning rate =  0.0001\n",
      "Epoch =  2110 Batch =  0 Loss =  [9.21780637] Gradient_max =  3.4065546814002516 learning rate =  0.0001\n",
      "Epoch =  2111 Batch =  0 Loss =  [9.21780637] Gradient_max =  3.4065546808253337 learning rate =  0.0001\n",
      "Epoch =  2112 Batch =  0 Loss =  [9.21780637] Gradient_max =  3.4065546802540156 learning rate =  0.0001\n",
      "Epoch =  2113 Batch =  0 Loss =  [9.21780637] Gradient_max =  3.4065546796862725 learning rate =  0.0001\n",
      "Epoch =  2114 Batch =  0 Loss =  [9.21780637] Gradient_max =  3.406554679122086 learning rate =  0.0001\n",
      "Epoch =  2115 Batch =  0 Loss =  [9.21780636] Gradient_max =  3.4065546785614313 learning rate =  0.0001\n",
      "Epoch =  2116 Batch =  0 Loss =  [9.21780636] Gradient_max =  3.406554678004287 learning rate =  0.0001\n",
      "Epoch =  2117 Batch =  0 Loss =  [9.21780636] Gradient_max =  3.40655467745063 learning rate =  0.0001\n",
      "Epoch =  2118 Batch =  0 Loss =  [9.21780636] Gradient_max =  3.40655467690044 learning rate =  0.0001\n",
      "Epoch =  2119 Batch =  0 Loss =  [9.21780636] Gradient_max =  3.406554676353693 learning rate =  0.0001\n",
      "Epoch =  2120 Batch =  0 Loss =  [9.21780636] Gradient_max =  3.4065546758103697 learning rate =  0.0001\n",
      "Epoch =  2121 Batch =  0 Loss =  [9.21780636] Gradient_max =  3.406554675270446 learning rate =  0.0001\n",
      "Epoch =  2122 Batch =  0 Loss =  [9.21780636] Gradient_max =  3.4065546747339033 learning rate =  0.0001\n",
      "Epoch =  2123 Batch =  0 Loss =  [9.21780636] Gradient_max =  3.406554674200721 learning rate =  0.0001\n",
      "Epoch =  2124 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.4065546736708736 learning rate =  0.0001\n",
      "Epoch =  2125 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.406554673144344 learning rate =  0.0001\n",
      "Epoch =  2126 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.4065546726211102 learning rate =  0.0001\n",
      "Epoch =  2127 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.406554672101151 learning rate =  0.0001\n",
      "Epoch =  2128 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.4065546715844466 learning rate =  0.0001\n",
      "Epoch =  2129 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.4065546710709778 learning rate =  0.0001\n",
      "Epoch =  2130 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.406554670560723 learning rate =  0.0001\n",
      "Epoch =  2131 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.4065546700536595 learning rate =  0.0001\n",
      "Epoch =  2132 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.406554669549772 learning rate =  0.0001\n",
      "Epoch =  2133 Batch =  0 Loss =  [9.21780635] Gradient_max =  3.4065546690490374 learning rate =  0.0001\n",
      "Epoch =  2134 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.406554668551437 learning rate =  0.0001\n",
      "Epoch =  2135 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.406554668056951 learning rate =  0.0001\n",
      "Epoch =  2136 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.4065546675655582 learning rate =  0.0001\n",
      "Epoch =  2137 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.4065546670772426 learning rate =  0.0001\n",
      "Epoch =  2138 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.4065546665919832 learning rate =  0.0001\n",
      "Epoch =  2139 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.40655466610976 learning rate =  0.0001\n",
      "Epoch =  2140 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.4065546656305563 learning rate =  0.0001\n",
      "Epoch =  2141 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.4065546651543492 learning rate =  0.0001\n",
      "Epoch =  2142 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.406554664681124 learning rate =  0.0001\n",
      "Epoch =  2143 Batch =  0 Loss =  [9.21780634] Gradient_max =  3.40655466421086 learning rate =  0.0001\n",
      "Epoch =  2144 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.4065546637435395 learning rate =  0.0001\n",
      "Epoch =  2145 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.406554663279141 learning rate =  0.0001\n",
      "Epoch =  2146 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.40655466281765 learning rate =  0.0001\n",
      "Epoch =  2147 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.4065546623590506 learning rate =  0.0001\n",
      "Epoch =  2148 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.4065546619033173 learning rate =  0.0001\n",
      "Epoch =  2149 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.4065546614504356 learning rate =  0.0001\n",
      "Epoch =  2150 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.4065546610003876 learning rate =  0.0001\n",
      "Epoch =  2151 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.4065546605531574 learning rate =  0.0001\n",
      "Epoch =  2152 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.406554660108724 learning rate =  0.0001\n",
      "Epoch =  2153 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.406554659667072 learning rate =  0.0001\n",
      "Epoch =  2154 Batch =  0 Loss =  [9.21780633] Gradient_max =  3.406554659228185 learning rate =  0.0001\n",
      "Epoch =  2155 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.4065546587920417 learning rate =  0.0001\n",
      "Epoch =  2156 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.406554658358626 learning rate =  0.0001\n",
      "Epoch =  2157 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.406554657927925 learning rate =  0.0001\n",
      "Epoch =  2158 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.406554657499919 learning rate =  0.0001\n",
      "Epoch =  2159 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.40655465707459 learning rate =  0.0001\n",
      "Epoch =  2160 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.406554656651922 learning rate =  0.0001\n",
      "Epoch =  2161 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.4065546562318985 learning rate =  0.0001\n",
      "Epoch =  2162 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.4065546558145026 learning rate =  0.0001\n",
      "Epoch =  2163 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.4065546553997184 learning rate =  0.0001\n",
      "Epoch =  2164 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.40655465498753 learning rate =  0.0001\n",
      "Epoch =  2165 Batch =  0 Loss =  [9.21780632] Gradient_max =  3.406554654577919 learning rate =  0.0001\n",
      "Epoch =  2166 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.4065546541708707 learning rate =  0.0001\n",
      "Epoch =  2167 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.4065546537663685 learning rate =  0.0001\n",
      "Epoch =  2168 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.4065546533643976 learning rate =  0.0001\n",
      "Epoch =  2169 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.4065546529649433 learning rate =  0.0001\n",
      "Epoch =  2170 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.4065546525679826 learning rate =  0.0001\n",
      "Epoch =  2171 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.406554652173509 learning rate =  0.0001\n",
      "Epoch =  2172 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.4065546517815037 learning rate =  0.0001\n",
      "Epoch =  2173 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.406554651391948 learning rate =  0.0001\n",
      "Epoch =  2174 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.406554651004831 learning rate =  0.0001\n",
      "Epoch =  2175 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.406554650620135 learning rate =  0.0001\n",
      "Epoch =  2176 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.4065546502378443 learning rate =  0.0001\n",
      "Epoch =  2177 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.4065546498579464 learning rate =  0.0001\n",
      "Epoch =  2178 Batch =  0 Loss =  [9.21780631] Gradient_max =  3.4065546494804235 learning rate =  0.0001\n",
      "Epoch =  2179 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.4065546491052623 learning rate =  0.0001\n",
      "Epoch =  2180 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.4065546487324476 learning rate =  0.0001\n",
      "Epoch =  2181 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.406554648361966 learning rate =  0.0001\n",
      "Epoch =  2182 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.406554647993801 learning rate =  0.0001\n",
      "Epoch =  2183 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.4065546476279382 learning rate =  0.0001\n",
      "Epoch =  2184 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.4065546472643655 learning rate =  0.0001\n",
      "Epoch =  2185 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.4065546469030648 learning rate =  0.0001\n",
      "Epoch =  2186 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.406554646544024 learning rate =  0.0001\n",
      "Epoch =  2187 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.4065546461872285 learning rate =  0.0001\n",
      "Epoch =  2188 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.406554645832666 learning rate =  0.0001\n",
      "Epoch =  2189 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.40655464548032 learning rate =  0.0001\n",
      "Epoch =  2190 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.406554645130177 learning rate =  0.0001\n",
      "Epoch =  2191 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.406554644782227 learning rate =  0.0001\n",
      "Epoch =  2192 Batch =  0 Loss =  [9.2178063] Gradient_max =  3.406554644436449 learning rate =  0.0001\n",
      "Epoch =  2193 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.4065546440928336 learning rate =  0.0001\n",
      "Epoch =  2194 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.406554643751371 learning rate =  0.0001\n",
      "Epoch =  2195 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.4065546434120395 learning rate =  0.0001\n",
      "Epoch =  2196 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.4065546430748306 learning rate =  0.0001\n",
      "Epoch =  2197 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.406554642739733 learning rate =  0.0001\n",
      "Epoch =  2198 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.4065546424067294 learning rate =  0.0001\n",
      "Epoch =  2199 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.406554642075808 learning rate =  0.0001\n",
      "Epoch =  2200 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.406554641746956 learning rate =  0.0001\n",
      "Epoch =  2201 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.4065546414201604 learning rate =  0.0001\n",
      "Epoch =  2202 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.406554641095408 learning rate =  0.0001\n",
      "Epoch =  2203 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.4065546407726854 learning rate =  0.0001\n",
      "Epoch =  2204 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.4065546404519838 learning rate =  0.0001\n",
      "Epoch =  2205 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.4065546401332853 learning rate =  0.0001\n",
      "Epoch =  2206 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.406554639816579 learning rate =  0.0001\n",
      "Epoch =  2207 Batch =  0 Loss =  [9.21780629] Gradient_max =  3.4065546395018553 learning rate =  0.0001\n",
      "Epoch =  2208 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.406554639189098 learning rate =  0.0001\n",
      "Epoch =  2209 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.4065546388782955 learning rate =  0.0001\n",
      "Epoch =  2210 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.406554638569436 learning rate =  0.0001\n",
      "Epoch =  2211 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.40655463826251 learning rate =  0.0001\n",
      "Epoch =  2212 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.4065546379575005 learning rate =  0.0001\n",
      "Epoch =  2213 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.4065546376544 learning rate =  0.0001\n",
      "Epoch =  2214 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.406554637353194 learning rate =  0.0001\n",
      "Epoch =  2215 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.4065546370538713 learning rate =  0.0001\n",
      "Epoch =  2216 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.406554636756419 learning rate =  0.0001\n",
      "Epoch =  2217 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.406554636460825 learning rate =  0.0001\n",
      "Epoch =  2218 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.4065546361670815 learning rate =  0.0001\n",
      "Epoch =  2219 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.4065546358751737 learning rate =  0.0001\n",
      "Epoch =  2220 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.4065546355850906 learning rate =  0.0001\n",
      "Epoch =  2221 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.4065546352968212 learning rate =  0.0001\n",
      "Epoch =  2222 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.406554635010354 learning rate =  0.0001\n",
      "Epoch =  2223 Batch =  0 Loss =  [9.21780628] Gradient_max =  3.406554634725676 learning rate =  0.0001\n",
      "Epoch =  2224 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.40655463444278 learning rate =  0.0001\n",
      "Epoch =  2225 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546341616506 learning rate =  0.0001\n",
      "Epoch =  2226 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.40655463388228 learning rate =  0.0001\n",
      "Epoch =  2227 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.406554633604654 learning rate =  0.0001\n",
      "Epoch =  2228 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.406554633328764 learning rate =  0.0001\n",
      "Epoch =  2229 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546330546006 learning rate =  0.0001\n",
      "Epoch =  2230 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546327821488 learning rate =  0.0001\n",
      "Epoch =  2231 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546325114004 learning rate =  0.0001\n",
      "Epoch =  2232 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.406554632242345 learning rate =  0.0001\n",
      "Epoch =  2233 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.40655463197497 learning rate =  0.0001\n",
      "Epoch =  2234 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546317092688 learning rate =  0.0001\n",
      "Epoch =  2235 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546314452266 learning rate =  0.0001\n",
      "Epoch =  2236 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546311828343 learning rate =  0.0001\n",
      "Epoch =  2237 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546309220833 learning rate =  0.0001\n",
      "Epoch =  2238 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.406554630662961 learning rate =  0.0001\n",
      "Epoch =  2239 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546304054584 learning rate =  0.0001\n",
      "Epoch =  2240 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.4065546301495657 learning rate =  0.0001\n",
      "Epoch =  2241 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.406554629895274 learning rate =  0.0001\n",
      "Epoch =  2242 Batch =  0 Loss =  [9.21780627] Gradient_max =  3.406554629642569 learning rate =  0.0001\n",
      "Epoch =  2243 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546293914437 learning rate =  0.0001\n",
      "Epoch =  2244 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.406554629141887 learning rate =  0.0001\n",
      "Epoch =  2245 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.406554628893892 learning rate =  0.0001\n",
      "Epoch =  2246 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.406554628647446 learning rate =  0.0001\n",
      "Epoch =  2247 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546284025388 learning rate =  0.0001\n",
      "Epoch =  2248 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.406554628159166 learning rate =  0.0001\n",
      "Epoch =  2249 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546279173096 learning rate =  0.0001\n",
      "Epoch =  2250 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546276769667 learning rate =  0.0001\n",
      "Epoch =  2251 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546274381258 learning rate =  0.0001\n",
      "Epoch =  2252 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546272007783 learning rate =  0.0001\n",
      "Epoch =  2253 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546269649123 learning rate =  0.0001\n",
      "Epoch =  2254 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.40655462673052 learning rate =  0.0001\n",
      "Epoch =  2255 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546264975928 learning rate =  0.0001\n",
      "Epoch =  2256 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.406554626266122 learning rate =  0.0001\n",
      "Epoch =  2257 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546260360975 learning rate =  0.0001\n",
      "Epoch =  2258 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.406554625807509 learning rate =  0.0001\n",
      "Epoch =  2259 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546255803514 learning rate =  0.0001\n",
      "Epoch =  2260 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546253546106 learning rate =  0.0001\n",
      "Epoch =  2261 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.4065546251302803 learning rate =  0.0001\n",
      "Epoch =  2262 Batch =  0 Loss =  [9.21780626] Gradient_max =  3.406554624907351 learning rate =  0.0001\n",
      "Epoch =  2263 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546246858167 learning rate =  0.0001\n",
      "Epoch =  2264 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546244656657 learning rate =  0.0001\n",
      "Epoch =  2265 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546242468923 learning rate =  0.0001\n",
      "Epoch =  2266 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.406554624029483 learning rate =  0.0001\n",
      "Epoch =  2267 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546238134337 learning rate =  0.0001\n",
      "Epoch =  2268 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.406554623598733 learning rate =  0.0001\n",
      "Epoch =  2269 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.406554623385375 learning rate =  0.0001\n",
      "Epoch =  2270 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.40655462317335 learning rate =  0.0001\n",
      "Epoch =  2271 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.406554622962648 learning rate =  0.0001\n",
      "Epoch =  2272 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.406554622753265 learning rate =  0.0001\n",
      "Epoch =  2273 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.40655462254519 learning rate =  0.0001\n",
      "Epoch =  2274 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546223384136 learning rate =  0.0001\n",
      "Epoch =  2275 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546221329286 learning rate =  0.0001\n",
      "Epoch =  2276 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546219287275 learning rate =  0.0001\n",
      "Epoch =  2277 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546217258023 learning rate =  0.0001\n",
      "Epoch =  2278 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.406554621524144 learning rate =  0.0001\n",
      "Epoch =  2279 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546213237456 learning rate =  0.0001\n",
      "Epoch =  2280 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.406554621124601 learning rate =  0.0001\n",
      "Epoch =  2281 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546209266993 learning rate =  0.0001\n",
      "Epoch =  2282 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546207300326 learning rate =  0.0001\n",
      "Epoch =  2283 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.406554620534598 learning rate =  0.0001\n",
      "Epoch =  2284 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.406554620340382 learning rate =  0.0001\n",
      "Epoch =  2285 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546201473786 learning rate =  0.0001\n",
      "Epoch =  2286 Batch =  0 Loss =  [9.21780625] Gradient_max =  3.4065546199555827 learning rate =  0.0001\n",
      "Epoch =  2287 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546197649845 learning rate =  0.0001\n",
      "Epoch =  2288 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554619575576 learning rate =  0.0001\n",
      "Epoch =  2289 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.40655461938735 learning rate =  0.0001\n",
      "Epoch =  2290 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546192002993 learning rate =  0.0001\n",
      "Epoch =  2291 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554619014419 learning rate =  0.0001\n",
      "Epoch =  2292 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554618829699 learning rate =  0.0001\n",
      "Epoch =  2293 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554618646132 learning rate =  0.0001\n",
      "Epoch =  2294 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546184637117 learning rate =  0.0001\n",
      "Epoch =  2295 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554618282432 learning rate =  0.0001\n",
      "Epoch =  2296 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546181022843 learning rate =  0.0001\n",
      "Epoch =  2297 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.40655461792326 learning rate =  0.0001\n",
      "Epoch =  2298 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554617745355 learning rate =  0.0001\n",
      "Epoch =  2299 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554617568561 learning rate =  0.0001\n",
      "Epoch =  2300 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546173928714 learning rate =  0.0001\n",
      "Epoch =  2301 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546172182795 learning rate =  0.0001\n",
      "Epoch =  2302 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554617044777 learning rate =  0.0001\n",
      "Epoch =  2303 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546168723584 learning rate =  0.0001\n",
      "Epoch =  2304 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546167010163 learning rate =  0.0001\n",
      "Epoch =  2305 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546165307454 learning rate =  0.0001\n",
      "Epoch =  2306 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554616361537 learning rate =  0.0001\n",
      "Epoch =  2307 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554616193387 learning rate =  0.0001\n",
      "Epoch =  2308 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554616026284 learning rate =  0.0001\n",
      "Epoch =  2309 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554615860227 learning rate =  0.0001\n",
      "Epoch =  2310 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554615695206 learning rate =  0.0001\n",
      "Epoch =  2311 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554615531217 learning rate =  0.0001\n",
      "Epoch =  2312 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.406554615368251 learning rate =  0.0001\n",
      "Epoch =  2313 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546152063026 learning rate =  0.0001\n",
      "Epoch =  2314 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546150453656 learning rate =  0.0001\n",
      "Epoch =  2315 Batch =  0 Loss =  [9.21780624] Gradient_max =  3.4065546148854335 learning rate =  0.0001\n",
      "Epoch =  2316 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546147265 learning rate =  0.0001\n",
      "Epoch =  2317 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554614568559 learning rate =  0.0001\n",
      "Epoch =  2318 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546144116032 learning rate =  0.0001\n",
      "Epoch =  2319 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546142556298 learning rate =  0.0001\n",
      "Epoch =  2320 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554614100628 learning rate =  0.0001\n",
      "Epoch =  2321 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546139465956 learning rate =  0.0001\n",
      "Epoch =  2322 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546137935248 learning rate =  0.0001\n",
      "Epoch =  2323 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546136414087 learning rate =  0.0001\n",
      "Epoch =  2324 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546134902416 learning rate =  0.0001\n",
      "Epoch =  2325 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546133400204 learning rate =  0.0001\n",
      "Epoch =  2326 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546131907367 learning rate =  0.0001\n",
      "Epoch =  2327 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554613042385 learning rate =  0.0001\n",
      "Epoch =  2328 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554612894959 learning rate =  0.0001\n",
      "Epoch =  2329 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554612748455 learning rate =  0.0001\n",
      "Epoch =  2330 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546126028643 learning rate =  0.0001\n",
      "Epoch =  2331 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546124581823 learning rate =  0.0001\n",
      "Epoch =  2332 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546123144044 learning rate =  0.0001\n",
      "Epoch =  2333 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554612171524 learning rate =  0.0001\n",
      "Epoch =  2334 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546120295367 learning rate =  0.0001\n",
      "Epoch =  2335 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546118884344 learning rate =  0.0001\n",
      "Epoch =  2336 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546117482133 learning rate =  0.0001\n",
      "Epoch =  2337 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546116088683 learning rate =  0.0001\n",
      "Epoch =  2338 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546114703924 learning rate =  0.0001\n",
      "Epoch =  2339 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546113327816 learning rate =  0.0001\n",
      "Epoch =  2340 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546111960296 learning rate =  0.0001\n",
      "Epoch =  2341 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546110601317 learning rate =  0.0001\n",
      "Epoch =  2342 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546109250824 learning rate =  0.0001\n",
      "Epoch =  2343 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546107908733 learning rate =  0.0001\n",
      "Epoch =  2344 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554610657505 learning rate =  0.0001\n",
      "Epoch =  2345 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.4065546105249673 learning rate =  0.0001\n",
      "Epoch =  2346 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554610393259 learning rate =  0.0001\n",
      "Epoch =  2347 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554610262373 learning rate =  0.0001\n",
      "Epoch =  2348 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554610132301 learning rate =  0.0001\n",
      "Epoch =  2349 Batch =  0 Loss =  [9.21780623] Gradient_max =  3.406554610003044 learning rate =  0.0001\n",
      "Epoch =  2350 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546098745934 learning rate =  0.0001\n",
      "Epoch =  2351 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554609746943 learning rate =  0.0001\n",
      "Epoch =  2352 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554609620091 learning rate =  0.0001\n",
      "Epoch =  2353 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.40655460949403 learning rate =  0.0001\n",
      "Epoch =  2354 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546093687553 learning rate =  0.0001\n",
      "Epoch =  2355 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554609244262 learning rate =  0.0001\n",
      "Epoch =  2356 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554609120548 learning rate =  0.0001\n",
      "Epoch =  2357 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554608997607 learning rate =  0.0001\n",
      "Epoch =  2358 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546088754313 learning rate =  0.0001\n",
      "Epoch =  2359 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554608754017 learning rate =  0.0001\n",
      "Epoch =  2360 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546086333613 learning rate =  0.0001\n",
      "Epoch =  2361 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546085134595 learning rate =  0.0001\n",
      "Epoch =  2362 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554608394306 learning rate =  0.0001\n",
      "Epoch =  2363 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546082758966 learning rate =  0.0001\n",
      "Epoch =  2364 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546081582263 learning rate =  0.0001\n",
      "Epoch =  2365 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546080412887 learning rate =  0.0001\n",
      "Epoch =  2366 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546079250812 learning rate =  0.0001\n",
      "Epoch =  2367 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546078095985 learning rate =  0.0001\n",
      "Epoch =  2368 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554607694839 learning rate =  0.0001\n",
      "Epoch =  2369 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554607580796 learning rate =  0.0001\n",
      "Epoch =  2370 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554607467463 learning rate =  0.0001\n",
      "Epoch =  2371 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554607354839 learning rate =  0.0001\n",
      "Epoch =  2372 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546072429154 learning rate =  0.0001\n",
      "Epoch =  2373 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546071316897 learning rate =  0.0001\n",
      "Epoch =  2374 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546070211594 learning rate =  0.0001\n",
      "Epoch =  2375 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546069113197 learning rate =  0.0001\n",
      "Epoch =  2376 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546068021644 learning rate =  0.0001\n",
      "Epoch =  2377 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546066936907 learning rate =  0.0001\n",
      "Epoch =  2378 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554606585894 learning rate =  0.0001\n",
      "Epoch =  2379 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546064787706 learning rate =  0.0001\n",
      "Epoch =  2380 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554606372315 learning rate =  0.0001\n",
      "Epoch =  2381 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546062665244 learning rate =  0.0001\n",
      "Epoch =  2382 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546061613916 learning rate =  0.0001\n",
      "Epoch =  2383 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554606056917 learning rate =  0.0001\n",
      "Epoch =  2384 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546059530933 learning rate =  0.0001\n",
      "Epoch =  2385 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.40655460584992 learning rate =  0.0001\n",
      "Epoch =  2386 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554605747387 learning rate =  0.0001\n",
      "Epoch =  2387 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554605645495 learning rate =  0.0001\n",
      "Epoch =  2388 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546055442386 learning rate =  0.0001\n",
      "Epoch =  2389 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554605443614 learning rate =  0.0001\n",
      "Epoch =  2390 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554605343617 learning rate =  0.0001\n",
      "Epoch =  2391 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.406554605244244 learning rate =  0.0001\n",
      "Epoch =  2392 Batch =  0 Loss =  [9.21780622] Gradient_max =  3.4065546051454922 learning rate =  0.0001\n",
      "Epoch =  2393 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554605047355 learning rate =  0.0001\n",
      "Epoch =  2394 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.40655460494983 learning rate =  0.0001\n",
      "Epoch =  2395 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554604852916 learning rate =  0.0001\n",
      "Epoch =  2396 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554604756605 learning rate =  0.0001\n",
      "Epoch =  2397 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546046608945 learning rate =  0.0001\n",
      "Epoch =  2398 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546045657795 learning rate =  0.0001\n",
      "Epoch =  2399 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546044712613 learning rate =  0.0001\n",
      "Epoch =  2400 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546043773303 learning rate =  0.0001\n",
      "Epoch =  2401 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546042839863 learning rate =  0.0001\n",
      "Epoch =  2402 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546041912267 learning rate =  0.0001\n",
      "Epoch =  2403 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554604099045 learning rate =  0.0001\n",
      "Epoch =  2404 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546040074355 learning rate =  0.0001\n",
      "Epoch =  2405 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546039163994 learning rate =  0.0001\n",
      "Epoch =  2406 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554603825931 learning rate =  0.0001\n",
      "Epoch =  2407 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554603736028 learning rate =  0.0001\n",
      "Epoch =  2408 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546036466845 learning rate =  0.0001\n",
      "Epoch =  2409 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546035578995 learning rate =  0.0001\n",
      "Epoch =  2410 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546034696657 learning rate =  0.0001\n",
      "Epoch =  2411 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546033819856 learning rate =  0.0001\n",
      "Epoch =  2412 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554603294852 learning rate =  0.0001\n",
      "Epoch =  2413 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546032082596 learning rate =  0.0001\n",
      "Epoch =  2414 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.40655460312221 learning rate =  0.0001\n",
      "Epoch =  2415 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546030366978 learning rate =  0.0001\n",
      "Epoch =  2416 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554602951716 learning rate =  0.0001\n",
      "Epoch =  2417 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546028672657 learning rate =  0.0001\n",
      "Epoch =  2418 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546027833435 learning rate =  0.0001\n",
      "Epoch =  2419 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554602699942 learning rate =  0.0001\n",
      "Epoch =  2420 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554602617062 learning rate =  0.0001\n",
      "Epoch =  2421 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554602534699 learning rate =  0.0001\n",
      "Epoch =  2422 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.40655460245285 learning rate =  0.0001\n",
      "Epoch =  2423 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546023715108 learning rate =  0.0001\n",
      "Epoch =  2424 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546022906807 learning rate =  0.0001\n",
      "Epoch =  2425 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554602210354 learning rate =  0.0001\n",
      "Epoch =  2426 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546021305266 learning rate =  0.0001\n",
      "Epoch =  2427 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546020512 learning rate =  0.0001\n",
      "Epoch =  2428 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554601972365 learning rate =  0.0001\n",
      "Epoch =  2429 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546018940247 learning rate =  0.0001\n",
      "Epoch =  2430 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546018161714 learning rate =  0.0001\n",
      "Epoch =  2431 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554601738804 learning rate =  0.0001\n",
      "Epoch =  2432 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546016619197 learning rate =  0.0001\n",
      "Epoch =  2433 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554601585516 learning rate =  0.0001\n",
      "Epoch =  2434 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546015095864 learning rate =  0.0001\n",
      "Epoch =  2435 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546014341317 learning rate =  0.0001\n",
      "Epoch =  2436 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546013591477 learning rate =  0.0001\n",
      "Epoch =  2437 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546012846313 learning rate =  0.0001\n",
      "Epoch =  2438 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554601210578 learning rate =  0.0001\n",
      "Epoch =  2439 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546011369876 learning rate =  0.0001\n",
      "Epoch =  2440 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554601063856 learning rate =  0.0001\n",
      "Epoch =  2441 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546009911807 learning rate =  0.0001\n",
      "Epoch =  2442 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554600918958 learning rate =  0.0001\n",
      "Epoch =  2443 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554600847186 learning rate =  0.0001\n",
      "Epoch =  2444 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554600775863 learning rate =  0.0001\n",
      "Epoch =  2445 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554600704983 learning rate =  0.0001\n",
      "Epoch =  2446 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546006345473 learning rate =  0.0001\n",
      "Epoch =  2447 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554600564548 learning rate =  0.0001\n",
      "Epoch =  2448 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554600494987 learning rate =  0.0001\n",
      "Epoch =  2449 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546004258587 learning rate =  0.0001\n",
      "Epoch =  2450 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546003571616 learning rate =  0.0001\n",
      "Epoch =  2451 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.406554600288895 learning rate =  0.0001\n",
      "Epoch =  2452 Batch =  0 Loss =  [9.21780621] Gradient_max =  3.4065546002210514 learning rate =  0.0001\n",
      "Epoch =  2453 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065546001536315 learning rate =  0.0001\n",
      "Epoch =  2454 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065546000866336 learning rate =  0.0001\n",
      "Epoch =  2455 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554600020051 learning rate =  0.0001\n",
      "Epoch =  2456 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545999538855 learning rate =  0.0001\n",
      "Epoch =  2457 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554599888132 learning rate =  0.0001\n",
      "Epoch =  2458 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545998227877 learning rate =  0.0001\n",
      "Epoch =  2459 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545997578526 learning rate =  0.0001\n",
      "Epoch =  2460 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545996933198 learning rate =  0.0001\n",
      "Epoch =  2461 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545996291933 learning rate =  0.0001\n",
      "Epoch =  2462 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545995654625 learning rate =  0.0001\n",
      "Epoch =  2463 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545995021296 learning rate =  0.0001\n",
      "Epoch =  2464 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554599439195 learning rate =  0.0001\n",
      "Epoch =  2465 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554599376651 learning rate =  0.0001\n",
      "Epoch =  2466 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554599314498 learning rate =  0.0001\n",
      "Epoch =  2467 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545992527286 learning rate =  0.0001\n",
      "Epoch =  2468 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554599191348 learning rate =  0.0001\n",
      "Epoch =  2469 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554599130349 learning rate =  0.0001\n",
      "Epoch =  2470 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545990697306 learning rate =  0.0001\n",
      "Epoch =  2471 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545990094885 learning rate =  0.0001\n",
      "Epoch =  2472 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554598949623 learning rate =  0.0001\n",
      "Epoch =  2473 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545988901307 learning rate =  0.0001\n",
      "Epoch =  2474 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545988310104 learning rate =  0.0001\n",
      "Epoch =  2475 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554598772258 learning rate =  0.0001\n",
      "Epoch =  2476 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554598713872 learning rate =  0.0001\n",
      "Epoch =  2477 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.40655459865585 learning rate =  0.0001\n",
      "Epoch =  2478 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.40655459859819 learning rate =  0.0001\n",
      "Epoch =  2479 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545985408883 learning rate =  0.0001\n",
      "Epoch =  2480 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554598483946 learning rate =  0.0001\n",
      "Epoch =  2481 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554598427357 learning rate =  0.0001\n",
      "Epoch =  2482 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545983711227 learning rate =  0.0001\n",
      "Epoch =  2483 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545983152363 learning rate =  0.0001\n",
      "Epoch =  2484 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554598259697 learning rate =  0.0001\n",
      "Epoch =  2485 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554598204507 learning rate =  0.0001\n",
      "Epoch =  2486 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545981496603 learning rate =  0.0001\n",
      "Epoch =  2487 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545980951573 learning rate =  0.0001\n",
      "Epoch =  2488 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545980409904 learning rate =  0.0001\n",
      "Epoch =  2489 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545979871636 learning rate =  0.0001\n",
      "Epoch =  2490 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545979336704 learning rate =  0.0001\n",
      "Epoch =  2491 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545978805125 learning rate =  0.0001\n",
      "Epoch =  2492 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554597827687 learning rate =  0.0001\n",
      "Epoch =  2493 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545977751883 learning rate =  0.0001\n",
      "Epoch =  2494 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545977230194 learning rate =  0.0001\n",
      "Epoch =  2495 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554597671173 learning rate =  0.0001\n",
      "Epoch =  2496 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545976196505 learning rate =  0.0001\n",
      "Epoch =  2497 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.40655459756845 learning rate =  0.0001\n",
      "Epoch =  2498 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554597517569 learning rate =  0.0001\n",
      "Epoch =  2499 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554597467003 learning rate =  0.0001\n",
      "Epoch =  2500 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545974167537 learning rate =  0.0001\n",
      "Epoch =  2501 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554597366817 learning rate =  0.0001\n",
      "Epoch =  2502 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545973171925 learning rate =  0.0001\n",
      "Epoch =  2503 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545972678772 learning rate =  0.0001\n",
      "Epoch =  2504 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545972188676 learning rate =  0.0001\n",
      "Epoch =  2505 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554597170165 learning rate =  0.0001\n",
      "Epoch =  2506 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545971217666 learning rate =  0.0001\n",
      "Epoch =  2507 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554597073668 learning rate =  0.0001\n",
      "Epoch =  2508 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554597025871 learning rate =  0.0001\n",
      "Epoch =  2509 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596978371 learning rate =  0.0001\n",
      "Epoch =  2510 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545969311657 learning rate =  0.0001\n",
      "Epoch =  2511 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596884256 learning rate =  0.0001\n",
      "Epoch =  2512 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545968376396 learning rate =  0.0001\n",
      "Epoch =  2513 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596791311 learning rate =  0.0001\n",
      "Epoch =  2514 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596745274 learning rate =  0.0001\n",
      "Epoch =  2515 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596699522 learning rate =  0.0001\n",
      "Epoch =  2516 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545966540576 learning rate =  0.0001\n",
      "Epoch =  2517 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545966088746 learning rate =  0.0001\n",
      "Epoch =  2518 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545965639736 learning rate =  0.0001\n",
      "Epoch =  2519 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596519351 learning rate =  0.0001\n",
      "Epoch =  2520 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545964750106 learning rate =  0.0001\n",
      "Epoch =  2521 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596430943 learning rate =  0.0001\n",
      "Epoch =  2522 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596387148 learning rate =  0.0001\n",
      "Epoch =  2523 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545963436294 learning rate =  0.0001\n",
      "Epoch =  2524 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545963003814 learning rate =  0.0001\n",
      "Epoch =  2525 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596257403 learning rate =  0.0001\n",
      "Epoch =  2526 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545962146926 learning rate =  0.0001\n",
      "Epoch =  2527 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554596172249 learning rate =  0.0001\n",
      "Epoch =  2528 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545961300674 learning rate =  0.0001\n",
      "Epoch =  2529 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545960881503 learning rate =  0.0001\n",
      "Epoch =  2530 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545960464947 learning rate =  0.0001\n",
      "Epoch =  2531 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545960050985 learning rate =  0.0001\n",
      "Epoch =  2532 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.40655459596396 learning rate =  0.0001\n",
      "Epoch =  2533 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545959230787 learning rate =  0.0001\n",
      "Epoch =  2534 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545958824495 learning rate =  0.0001\n",
      "Epoch =  2535 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554595842078 learning rate =  0.0001\n",
      "Epoch =  2536 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545958019534 learning rate =  0.0001\n",
      "Epoch =  2537 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554595762082 learning rate =  0.0001\n",
      "Epoch =  2538 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545957224574 learning rate =  0.0001\n",
      "Epoch =  2539 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545956830783 learning rate =  0.0001\n",
      "Epoch =  2540 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545956439465 learning rate =  0.0001\n",
      "Epoch =  2541 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554595605057 learning rate =  0.0001\n",
      "Epoch =  2542 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545955664116 learning rate =  0.0001\n",
      "Epoch =  2543 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545955280063 learning rate =  0.0001\n",
      "Epoch =  2544 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554595489842 learning rate =  0.0001\n",
      "Epoch =  2545 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545954519134 learning rate =  0.0001\n",
      "Epoch =  2546 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554595414222 learning rate =  0.0001\n",
      "Epoch =  2547 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.406554595376766 learning rate =  0.0001\n",
      "Epoch =  2548 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545953395433 learning rate =  0.0001\n",
      "Epoch =  2549 Batch =  0 Loss =  [9.2178062] Gradient_max =  3.4065545953025502 learning rate =  0.0001\n",
      "Epoch =  2550 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554595265791 learning rate =  0.0001\n",
      "Epoch =  2551 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545952292586 learning rate =  0.0001\n",
      "Epoch =  2552 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545951929534 learning rate =  0.0001\n",
      "Epoch =  2553 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545951568756 learning rate =  0.0001\n",
      "Epoch =  2554 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545951210234 learning rate =  0.0001\n",
      "Epoch =  2555 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554595085393 learning rate =  0.0001\n",
      "Epoch =  2556 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545950499847 learning rate =  0.0001\n",
      "Epoch =  2557 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554595014796 learning rate =  0.0001\n",
      "Epoch =  2558 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545949798297 learning rate =  0.0001\n",
      "Epoch =  2559 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545949450793 learning rate =  0.0001\n",
      "Epoch =  2560 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545949105442 learning rate =  0.0001\n",
      "Epoch =  2561 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545948762277 learning rate =  0.0001\n",
      "Epoch =  2562 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594842123 learning rate =  0.0001\n",
      "Epoch =  2563 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545948082288 learning rate =  0.0001\n",
      "Epoch =  2564 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545947745477 learning rate =  0.0001\n",
      "Epoch =  2565 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594741076 learning rate =  0.0001\n",
      "Epoch =  2566 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594707812 learning rate =  0.0001\n",
      "Epoch =  2567 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594674757 learning rate =  0.0001\n",
      "Epoch =  2568 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545946419076 learning rate =  0.0001\n",
      "Epoch =  2569 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594609262 learning rate =  0.0001\n",
      "Epoch =  2570 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594576821 learning rate =  0.0001\n",
      "Epoch =  2571 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545945445814 learning rate =  0.0001\n",
      "Epoch =  2572 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545945125413 learning rate =  0.0001\n",
      "Epoch =  2573 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594480702 learning rate =  0.0001\n",
      "Epoch =  2574 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594449062 learning rate =  0.0001\n",
      "Epoch =  2575 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594417619 learning rate =  0.0001\n",
      "Epoch =  2576 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459438637 learning rate =  0.0001\n",
      "Epoch =  2577 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594355317 learning rate =  0.0001\n",
      "Epoch =  2578 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545943244575 learning rate =  0.0001\n",
      "Epoch =  2579 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594293789 learning rate =  0.0001\n",
      "Epoch =  2580 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545942633113 learning rate =  0.0001\n",
      "Epoch =  2581 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594233025 learning rate =  0.0001\n",
      "Epoch =  2582 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545942029267 learning rate =  0.0001\n",
      "Epoch =  2583 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545941730155 learning rate =  0.0001\n",
      "Epoch =  2584 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594143291 learning rate =  0.0001\n",
      "Epoch =  2585 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554594113752 learning rate =  0.0001\n",
      "Epoch =  2586 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545940843966 learning rate =  0.0001\n",
      "Epoch =  2587 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545940552235 learning rate =  0.0001\n",
      "Epoch =  2588 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545940262334 learning rate =  0.0001\n",
      "Epoch =  2589 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545939974218 learning rate =  0.0001\n",
      "Epoch =  2590 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545939687922 learning rate =  0.0001\n",
      "Epoch =  2591 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545939403385 learning rate =  0.0001\n",
      "Epoch =  2592 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593912065 learning rate =  0.0001\n",
      "Epoch =  2593 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545938839645 learning rate =  0.0001\n",
      "Epoch =  2594 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545938560406 learning rate =  0.0001\n",
      "Epoch =  2595 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545938282904 learning rate =  0.0001\n",
      "Epoch =  2596 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545938007147 learning rate =  0.0001\n",
      "Epoch =  2597 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545937733086 learning rate =  0.0001\n",
      "Epoch =  2598 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593746073 learning rate =  0.0001\n",
      "Epoch =  2599 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545937190085 learning rate =  0.0001\n",
      "Epoch =  2600 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593692111 learning rate =  0.0001\n",
      "Epoch =  2601 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545936653807 learning rate =  0.0001\n",
      "Epoch =  2602 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545936388184 learning rate =  0.0001\n",
      "Epoch =  2603 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545936124226 learning rate =  0.0001\n",
      "Epoch =  2604 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593586189 learning rate =  0.0001\n",
      "Epoch =  2605 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545935601205 learning rate =  0.0001\n",
      "Epoch =  2606 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593534213 learning rate =  0.0001\n",
      "Epoch =  2607 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545935084685 learning rate =  0.0001\n",
      "Epoch =  2608 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545934828827 learning rate =  0.0001\n",
      "Epoch =  2609 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593457456 learning rate =  0.0001\n",
      "Epoch =  2610 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593432189 learning rate =  0.0001\n",
      "Epoch =  2611 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593407078 learning rate =  0.0001\n",
      "Epoch =  2612 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593382125 learning rate =  0.0001\n",
      "Epoch =  2613 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593357327 learning rate =  0.0001\n",
      "Epoch =  2614 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545933326833 learning rate =  0.0001\n",
      "Epoch =  2615 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545933081918 learning rate =  0.0001\n",
      "Epoch =  2616 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593283854 learning rate =  0.0001\n",
      "Epoch =  2617 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593259667 learning rate =  0.0001\n",
      "Epoch =  2618 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545932356316 learning rate =  0.0001\n",
      "Epoch =  2619 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593211744 learning rate =  0.0001\n",
      "Epoch =  2620 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545931880075 learning rate =  0.0001\n",
      "Epoch =  2621 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554593164417 learning rate =  0.0001\n",
      "Epoch =  2622 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545931409735 learning rate =  0.0001\n",
      "Epoch =  2623 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545931176775 learning rate =  0.0001\n",
      "Epoch =  2624 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545930945254 learning rate =  0.0001\n",
      "Epoch =  2625 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545930715166 learning rate =  0.0001\n",
      "Epoch =  2626 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545930486514 learning rate =  0.0001\n",
      "Epoch =  2627 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545930259318 learning rate =  0.0001\n",
      "Epoch =  2628 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545930033507 learning rate =  0.0001\n",
      "Epoch =  2629 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545929809105 learning rate =  0.0001\n",
      "Epoch =  2630 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545929586087 learning rate =  0.0001\n",
      "Epoch =  2631 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545929364487 learning rate =  0.0001\n",
      "Epoch =  2632 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545929144228 learning rate =  0.0001\n",
      "Epoch =  2633 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592892538 learning rate =  0.0001\n",
      "Epoch =  2634 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545928707874 learning rate =  0.0001\n",
      "Epoch =  2635 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592849172 learning rate =  0.0001\n",
      "Epoch =  2636 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545928276926 learning rate =  0.0001\n",
      "Epoch =  2637 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545928063443 learning rate =  0.0001\n",
      "Epoch =  2638 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545927851333 learning rate =  0.0001\n",
      "Epoch =  2639 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545927640515 learning rate =  0.0001\n",
      "Epoch =  2640 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545927431007 learning rate =  0.0001\n",
      "Epoch =  2641 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592722282 learning rate =  0.0001\n",
      "Epoch =  2642 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592701591 learning rate =  0.0001\n",
      "Epoch =  2643 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545926810294 learning rate =  0.0001\n",
      "Epoch =  2644 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592660597 learning rate =  0.0001\n",
      "Epoch =  2645 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545926402927 learning rate =  0.0001\n",
      "Epoch =  2646 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545926201115 learning rate =  0.0001\n",
      "Epoch =  2647 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592600057 learning rate =  0.0001\n",
      "Epoch =  2648 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592580128 learning rate =  0.0001\n",
      "Epoch =  2649 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545925603233 learning rate =  0.0001\n",
      "Epoch =  2650 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545925406417 learning rate =  0.0001\n",
      "Epoch =  2651 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592521082 learning rate =  0.0001\n",
      "Epoch =  2652 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545925016445 learning rate =  0.0001\n",
      "Epoch =  2653 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545924823266 learning rate =  0.0001\n",
      "Epoch =  2654 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592463132 learning rate =  0.0001\n",
      "Epoch =  2655 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592444055 learning rate =  0.0001\n",
      "Epoch =  2656 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545924250995 learning rate =  0.0001\n",
      "Epoch =  2657 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592406259 learning rate =  0.0001\n",
      "Epoch =  2658 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545923875344 learning rate =  0.0001\n",
      "Epoch =  2659 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459236893 learning rate =  0.0001\n",
      "Epoch =  2660 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592350439 learning rate =  0.0001\n",
      "Epoch =  2661 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592332065 learning rate =  0.0001\n",
      "Epoch =  2662 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545923138036 learning rate =  0.0001\n",
      "Epoch =  2663 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592295656 learning rate =  0.0001\n",
      "Epoch =  2664 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545922776237 learning rate =  0.0001\n",
      "Epoch =  2665 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545922597007 learning rate =  0.0001\n",
      "Epoch =  2666 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592241891 learning rate =  0.0001\n",
      "Epoch =  2667 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545922241918 learning rate =  0.0001\n",
      "Epoch =  2668 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592206601 learning rate =  0.0001\n",
      "Epoch =  2669 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592189121 learning rate =  0.0001\n",
      "Epoch =  2670 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545921717493 learning rate =  0.0001\n",
      "Epoch =  2671 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545921544866 learning rate =  0.0001\n",
      "Epoch =  2672 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545921373337 learning rate =  0.0001\n",
      "Epoch =  2673 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545921202856 learning rate =  0.0001\n",
      "Epoch =  2674 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545921033413 learning rate =  0.0001\n",
      "Epoch =  2675 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592086506 learning rate =  0.0001\n",
      "Epoch =  2676 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545920697753 learning rate =  0.0001\n",
      "Epoch =  2677 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545920531477 learning rate =  0.0001\n",
      "Epoch =  2678 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592036623 learning rate =  0.0001\n",
      "Epoch =  2679 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554592020201 learning rate =  0.0001\n",
      "Epoch =  2680 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459200388 learning rate =  0.0001\n",
      "Epoch =  2681 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591987664 learning rate =  0.0001\n",
      "Epoch =  2682 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545919715454 learning rate =  0.0001\n",
      "Epoch =  2683 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545919555302 learning rate =  0.0001\n",
      "Epoch =  2684 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591939613 learning rate =  0.0001\n",
      "Epoch =  2685 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591923795 learning rate =  0.0001\n",
      "Epoch =  2686 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591908076 learning rate =  0.0001\n",
      "Epoch =  2687 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591892455 learning rate =  0.0001\n",
      "Epoch =  2688 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591876931 learning rate =  0.0001\n",
      "Epoch =  2689 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545918615032 learning rate =  0.0001\n",
      "Epoch =  2690 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545918461706 learning rate =  0.0001\n",
      "Epoch =  2691 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591830935 learning rate =  0.0001\n",
      "Epoch =  2692 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591815792 learning rate =  0.0001\n",
      "Epoch =  2693 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545918007456 learning rate =  0.0001\n",
      "Epoch =  2694 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591785791 learning rate =  0.0001\n",
      "Epoch =  2695 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545917709312 learning rate =  0.0001\n",
      "Epoch =  2696 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545917561653 learning rate =  0.0001\n",
      "Epoch =  2697 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545917414863 learning rate =  0.0001\n",
      "Epoch =  2698 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591726903 learning rate =  0.0001\n",
      "Epoch =  2699 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545917124087 learning rate =  0.0001\n",
      "Epoch =  2700 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545916980056 learning rate =  0.0001\n",
      "Epoch =  2701 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591683691 learning rate =  0.0001\n",
      "Epoch =  2702 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591669465 learning rate =  0.0001\n",
      "Epoch =  2703 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545916553286 learning rate =  0.0001\n",
      "Epoch =  2704 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545916412807 learning rate =  0.0001\n",
      "Epoch =  2705 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545916273208 learning rate =  0.0001\n",
      "Epoch =  2706 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591613446 learning rate =  0.0001\n",
      "Epoch =  2707 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545915996585 learning rate =  0.0001\n",
      "Epoch =  2708 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545915859556 learning rate =  0.0001\n",
      "Epoch =  2709 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545915723368 learning rate =  0.0001\n",
      "Epoch =  2710 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591558807 learning rate =  0.0001\n",
      "Epoch =  2711 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545915453592 learning rate =  0.0001\n",
      "Epoch =  2712 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545915319952 learning rate =  0.0001\n",
      "Epoch =  2713 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545915187134 learning rate =  0.0001\n",
      "Epoch =  2714 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591505517 learning rate =  0.0001\n",
      "Epoch =  2715 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591492399 learning rate =  0.0001\n",
      "Epoch =  2716 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545914793645 learning rate =  0.0001\n",
      "Epoch =  2717 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591466411 learning rate =  0.0001\n",
      "Epoch =  2718 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545914535384 learning rate =  0.0001\n",
      "Epoch =  2719 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591440746 learning rate =  0.0001\n",
      "Epoch =  2720 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591428032 learning rate =  0.0001\n",
      "Epoch =  2721 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545914153983 learning rate =  0.0001\n",
      "Epoch =  2722 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545914028426 learning rate =  0.0001\n",
      "Epoch =  2723 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545913903654 learning rate =  0.0001\n",
      "Epoch =  2724 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591377966 learning rate =  0.0001\n",
      "Epoch =  2725 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591365644 learning rate =  0.0001\n",
      "Epoch =  2726 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545913533968 learning rate =  0.0001\n",
      "Epoch =  2727 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591341228 learning rate =  0.0001\n",
      "Epoch =  2728 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545913291335 learning rate =  0.0001\n",
      "Epoch =  2729 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591317115 learning rate =  0.0001\n",
      "Epoch =  2730 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591305171 learning rate =  0.0001\n",
      "Epoch =  2731 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545912933026 learning rate =  0.0001\n",
      "Epoch =  2732 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591281506 learning rate =  0.0001\n",
      "Epoch =  2733 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545912697837 learning rate =  0.0001\n",
      "Epoch =  2734 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545912581356 learning rate =  0.0001\n",
      "Epoch =  2735 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545912465587 learning rate =  0.0001\n",
      "Epoch =  2736 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591235053 learning rate =  0.0001\n",
      "Epoch =  2737 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545912236197 learning rate =  0.0001\n",
      "Epoch =  2738 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545912122577 learning rate =  0.0001\n",
      "Epoch =  2739 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545912009663 learning rate =  0.0001\n",
      "Epoch =  2740 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545911897455 learning rate =  0.0001\n",
      "Epoch =  2741 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591178595 learning rate =  0.0001\n",
      "Epoch =  2742 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545911675117 learning rate =  0.0001\n",
      "Epoch =  2743 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545911564996 learning rate =  0.0001\n",
      "Epoch =  2744 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545911455573 learning rate =  0.0001\n",
      "Epoch =  2745 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545911346793 learning rate =  0.0001\n",
      "Epoch =  2746 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545911238693 learning rate =  0.0001\n",
      "Epoch =  2747 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459111313 learning rate =  0.0001\n",
      "Epoch =  2748 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545911024553 learning rate =  0.0001\n",
      "Epoch =  2749 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545910918464 learning rate =  0.0001\n",
      "Epoch =  2750 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591081304 learning rate =  0.0001\n",
      "Epoch =  2751 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591070828 learning rate =  0.0001\n",
      "Epoch =  2752 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591060417 learning rate =  0.0001\n",
      "Epoch =  2753 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545910500696 learning rate =  0.0001\n",
      "Epoch =  2754 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591039786 learning rate =  0.0001\n",
      "Epoch =  2755 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545910295674 learning rate =  0.0001\n",
      "Epoch =  2756 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554591019414 learning rate =  0.0001\n",
      "Epoch =  2757 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545910093222 learning rate =  0.0001\n",
      "Epoch =  2758 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545909992934 learning rate =  0.0001\n",
      "Epoch =  2759 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590989327 learning rate =  0.0001\n",
      "Epoch =  2760 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545909794226 learning rate =  0.0001\n",
      "Epoch =  2761 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545909695785 learning rate =  0.0001\n",
      "Epoch =  2762 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545909597983 learning rate =  0.0001\n",
      "Epoch =  2763 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590950078 learning rate =  0.0001\n",
      "Epoch =  2764 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545909404165 learning rate =  0.0001\n",
      "Epoch =  2765 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545909308166 learning rate =  0.0001\n",
      "Epoch =  2766 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545909212758 learning rate =  0.0001\n",
      "Epoch =  2767 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545909117962 learning rate =  0.0001\n",
      "Epoch =  2768 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545909023736 learning rate =  0.0001\n",
      "Epoch =  2769 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459089301 learning rate =  0.0001\n",
      "Epoch =  2770 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545908837063 learning rate =  0.0001\n",
      "Epoch =  2771 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590874457 learning rate =  0.0001\n",
      "Epoch =  2772 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545908652677 learning rate =  0.0001\n",
      "Epoch =  2773 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545908561354 learning rate =  0.0001\n",
      "Epoch =  2774 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545908470583 learning rate =  0.0001\n",
      "Epoch =  2775 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590838038 learning rate =  0.0001\n",
      "Epoch =  2776 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590829076 learning rate =  0.0001\n",
      "Epoch =  2777 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545908201695 learning rate =  0.0001\n",
      "Epoch =  2778 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545908113184 learning rate =  0.0001\n",
      "Epoch =  2779 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459080252 learning rate =  0.0001\n",
      "Epoch =  2780 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459079378 learning rate =  0.0001\n",
      "Epoch =  2781 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545907850923 learning rate =  0.0001\n",
      "Epoch =  2782 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590776457 learning rate =  0.0001\n",
      "Epoch =  2783 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590767878 learning rate =  0.0001\n",
      "Epoch =  2784 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459075935 learning rate =  0.0001\n",
      "Epoch =  2785 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545907508756 learning rate =  0.0001\n",
      "Epoch =  2786 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590742455 learning rate =  0.0001\n",
      "Epoch =  2787 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590734087 learning rate =  0.0001\n",
      "Epoch =  2788 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590725769 learning rate =  0.0001\n",
      "Epoch =  2789 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590717504 learning rate =  0.0001\n",
      "Epoch =  2790 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545907092916 learning rate =  0.0001\n",
      "Epoch =  2791 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545907011288 learning rate =  0.0001\n",
      "Epoch =  2792 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545906930175 learning rate =  0.0001\n",
      "Epoch =  2793 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545906849555 learning rate =  0.0001\n",
      "Epoch =  2794 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545906769445 learning rate =  0.0001\n",
      "Epoch =  2795 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545906689834 learning rate =  0.0001\n",
      "Epoch =  2796 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545906610706 learning rate =  0.0001\n",
      "Epoch =  2797 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545906532075 learning rate =  0.0001\n",
      "Epoch =  2798 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590645395 learning rate =  0.0001\n",
      "Epoch =  2799 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590637629 learning rate =  0.0001\n",
      "Epoch =  2800 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545906299115 learning rate =  0.0001\n",
      "Epoch =  2801 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590622244 learning rate =  0.0001\n",
      "Epoch =  2802 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590614623 learning rate =  0.0001\n",
      "Epoch =  2803 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590607048 learning rate =  0.0001\n",
      "Epoch =  2804 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545905995216 learning rate =  0.0001\n",
      "Epoch =  2805 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545905920422 learning rate =  0.0001\n",
      "Epoch =  2806 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459058461 learning rate =  0.0001\n",
      "Epoch =  2807 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545905772234 learning rate =  0.0001\n",
      "Epoch =  2808 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590569882 learning rate =  0.0001\n",
      "Epoch =  2809 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590562587 learning rate =  0.0001\n",
      "Epoch =  2810 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590555337 learning rate =  0.0001\n",
      "Epoch =  2811 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590548133 learning rate =  0.0001\n",
      "Epoch =  2812 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545905409733 learning rate =  0.0001\n",
      "Epoch =  2813 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545905338577 learning rate =  0.0001\n",
      "Epoch =  2814 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545905267864 learning rate =  0.0001\n",
      "Epoch =  2815 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590519759 learning rate =  0.0001\n",
      "Epoch =  2816 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590512774 learning rate =  0.0001\n",
      "Epoch =  2817 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545905058343 learning rate =  0.0001\n",
      "Epoch =  2818 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590498937 learning rate =  0.0001\n",
      "Epoch =  2819 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545904920835 learning rate =  0.0001\n",
      "Epoch =  2820 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.40655459048527 learning rate =  0.0001\n",
      "Epoch =  2821 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545904785015 learning rate =  0.0001\n",
      "Epoch =  2822 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545904717736 learning rate =  0.0001\n",
      "Epoch =  2823 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545904650887 learning rate =  0.0001\n",
      "Epoch =  2824 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545904584447 learning rate =  0.0001\n",
      "Epoch =  2825 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590451842 learning rate =  0.0001\n",
      "Epoch =  2826 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545904452796 learning rate =  0.0001\n",
      "Epoch =  2827 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.406554590438761 learning rate =  0.0001\n",
      "Epoch =  2828 Batch =  0 Loss =  [9.21780619] Gradient_max =  3.4065545904322807 learning rate =  0.0001\n",
      "Epoch =  2829 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590425841 learning rate =  0.0001\n",
      "Epoch =  2830 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545904194403 learning rate =  0.0001\n",
      "Epoch =  2831 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590413082 learning rate =  0.0001\n",
      "Epoch =  2832 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590406761 learning rate =  0.0001\n",
      "Epoch =  2833 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545904004773 learning rate =  0.0001\n",
      "Epoch =  2834 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590394237 learning rate =  0.0001\n",
      "Epoch =  2835 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903880343 learning rate =  0.0001\n",
      "Epoch =  2836 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590381869 learning rate =  0.0001\n",
      "Epoch =  2837 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590375743 learning rate =  0.0001\n",
      "Epoch =  2838 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903696552 learning rate =  0.0001\n",
      "Epoch =  2839 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903636045 learning rate =  0.0001\n",
      "Epoch =  2840 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903575933 learning rate =  0.0001\n",
      "Epoch =  2841 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903516177 learning rate =  0.0001\n",
      "Epoch =  2842 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590345679 learning rate =  0.0001\n",
      "Epoch =  2843 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903397783 learning rate =  0.0001\n",
      "Epoch =  2844 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590333913 learning rate =  0.0001\n",
      "Epoch =  2845 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903280845 learning rate =  0.0001\n",
      "Epoch =  2846 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903222927 learning rate =  0.0001\n",
      "Epoch =  2847 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903165364 learning rate =  0.0001\n",
      "Epoch =  2848 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590310818 learning rate =  0.0001\n",
      "Epoch =  2849 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545903051335 learning rate =  0.0001\n",
      "Epoch =  2850 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590299485 learning rate =  0.0001\n",
      "Epoch =  2851 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590293871 learning rate =  0.0001\n",
      "Epoch =  2852 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590288292 learning rate =  0.0001\n",
      "Epoch =  2853 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902827474 learning rate =  0.0001\n",
      "Epoch =  2854 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902772376 learning rate =  0.0001\n",
      "Epoch =  2855 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902717616 learning rate =  0.0001\n",
      "Epoch =  2856 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902663197 learning rate =  0.0001\n",
      "Epoch =  2857 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590260911 learning rate =  0.0001\n",
      "Epoch =  2858 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590255537 learning rate =  0.0001\n",
      "Epoch =  2859 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590250196 learning rate =  0.0001\n",
      "Epoch =  2860 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902448884 learning rate =  0.0001\n",
      "Epoch =  2861 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590239615 learning rate =  0.0001\n",
      "Epoch =  2862 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902343732 learning rate =  0.0001\n",
      "Epoch =  2863 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902291636 learning rate =  0.0001\n",
      "Epoch =  2864 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590223985 learning rate =  0.0001\n",
      "Epoch =  2865 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902188417 learning rate =  0.0001\n",
      "Epoch =  2866 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902137297 learning rate =  0.0001\n",
      "Epoch =  2867 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590208649 learning rate =  0.0001\n",
      "Epoch =  2868 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545902035996 learning rate =  0.0001\n",
      "Epoch =  2869 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590198582 learning rate =  0.0001\n",
      "Epoch =  2870 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901935956 learning rate =  0.0001\n",
      "Epoch =  2871 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655459018864 learning rate =  0.0001\n",
      "Epoch =  2872 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590183715 learning rate =  0.0001\n",
      "Epoch =  2873 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901788212 learning rate =  0.0001\n",
      "Epoch =  2874 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901739562 learning rate =  0.0001\n",
      "Epoch =  2875 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590169123 learning rate =  0.0001\n",
      "Epoch =  2876 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901643213 learning rate =  0.0001\n",
      "Epoch =  2877 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901595486 learning rate =  0.0001\n",
      "Epoch =  2878 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901548044 learning rate =  0.0001\n",
      "Epoch =  2879 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655459015009 learning rate =  0.0001\n",
      "Epoch =  2880 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901454057 learning rate =  0.0001\n",
      "Epoch =  2881 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901407486 learning rate =  0.0001\n",
      "Epoch =  2882 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901361234 learning rate =  0.0001\n",
      "Epoch =  2883 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590131524 learning rate =  0.0001\n",
      "Epoch =  2884 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901269534 learning rate =  0.0001\n",
      "Epoch =  2885 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901224126 learning rate =  0.0001\n",
      "Epoch =  2886 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901178997 learning rate =  0.0001\n",
      "Epoch =  2887 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901134153 learning rate =  0.0001\n",
      "Epoch =  2888 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590108957 learning rate =  0.0001\n",
      "Epoch =  2889 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590104528 learning rate =  0.0001\n",
      "Epoch =  2890 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545901001264 learning rate =  0.0001\n",
      "Epoch =  2891 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900957512 learning rate =  0.0001\n",
      "Epoch =  2892 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900914054 learning rate =  0.0001\n",
      "Epoch =  2893 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900870857 learning rate =  0.0001\n",
      "Epoch =  2894 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590082794 learning rate =  0.0001\n",
      "Epoch =  2895 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590078527 learning rate =  0.0001\n",
      "Epoch =  2896 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590074285 learning rate =  0.0001\n",
      "Epoch =  2897 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590070072 learning rate =  0.0001\n",
      "Epoch =  2898 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900658836 learning rate =  0.0001\n",
      "Epoch =  2899 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590061723 learning rate =  0.0001\n",
      "Epoch =  2900 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900575858 learning rate =  0.0001\n",
      "Epoch =  2901 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900534766 learning rate =  0.0001\n",
      "Epoch =  2902 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900493914 learning rate =  0.0001\n",
      "Epoch =  2903 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590045333 learning rate =  0.0001\n",
      "Epoch =  2904 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900412992 learning rate =  0.0001\n",
      "Epoch =  2905 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590037288 learning rate =  0.0001\n",
      "Epoch =  2906 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900333065 learning rate =  0.0001\n",
      "Epoch =  2907 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590029346 learning rate =  0.0001\n",
      "Epoch =  2908 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590025413 learning rate =  0.0001\n",
      "Epoch =  2909 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590021502 learning rate =  0.0001\n",
      "Epoch =  2910 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900176164 learning rate =  0.0001\n",
      "Epoch =  2911 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590013754 learning rate =  0.0001\n",
      "Epoch =  2912 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545900099177 learning rate =  0.0001\n",
      "Epoch =  2913 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590006104 learning rate =  0.0001\n",
      "Epoch =  2914 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554590002313 learning rate =  0.0001\n",
      "Epoch =  2915 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589998547 learning rate =  0.0001\n",
      "Epoch =  2916 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899948035 learning rate =  0.0001\n",
      "Epoch =  2917 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899910834 learning rate =  0.0001\n",
      "Epoch =  2918 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589987385 learning rate =  0.0001\n",
      "Epoch =  2919 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899837137 learning rate =  0.0001\n",
      "Epoch =  2920 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589980063 learning rate =  0.0001\n",
      "Epoch =  2921 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589976435 learning rate =  0.0001\n",
      "Epoch =  2922 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589972828 learning rate =  0.0001\n",
      "Epoch =  2923 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899692458 learning rate =  0.0001\n",
      "Epoch =  2924 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589965685 learning rate =  0.0001\n",
      "Epoch =  2925 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899621457 learning rate =  0.0001\n",
      "Epoch =  2926 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589958629 learning rate =  0.0001\n",
      "Epoch =  2927 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899551335 learning rate =  0.0001\n",
      "Epoch =  2928 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899516607 learning rate =  0.0001\n",
      "Epoch =  2929 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899482084 learning rate =  0.0001\n",
      "Epoch =  2930 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899447787 learning rate =  0.0001\n",
      "Epoch =  2931 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899413694 learning rate =  0.0001\n",
      "Epoch =  2932 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899379828 learning rate =  0.0001\n",
      "Epoch =  2933 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899346166 learning rate =  0.0001\n",
      "Epoch =  2934 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589931271 learning rate =  0.0001\n",
      "Epoch =  2935 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899279446 learning rate =  0.0001\n",
      "Epoch =  2936 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589924642 learning rate =  0.0001\n",
      "Epoch =  2937 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899213574 learning rate =  0.0001\n",
      "Epoch =  2938 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899180947 learning rate =  0.0001\n",
      "Epoch =  2939 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899148515 learning rate =  0.0001\n",
      "Epoch =  2940 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899116287 learning rate =  0.0001\n",
      "Epoch =  2941 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589908425 learning rate =  0.0001\n",
      "Epoch =  2942 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545899052423 learning rate =  0.0001\n",
      "Epoch =  2943 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458990208 learning rate =  0.0001\n",
      "Epoch =  2944 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589898938 learning rate =  0.0001\n",
      "Epoch =  2945 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898958134 learning rate =  0.0001\n",
      "Epoch =  2946 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898927083 learning rate =  0.0001\n",
      "Epoch =  2947 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898896223 learning rate =  0.0001\n",
      "Epoch =  2948 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898865577 learning rate =  0.0001\n",
      "Epoch =  2949 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898835103 learning rate =  0.0001\n",
      "Epoch =  2950 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589880482 learning rate =  0.0001\n",
      "Epoch =  2951 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898774725 learning rate =  0.0001\n",
      "Epoch =  2952 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898744825 learning rate =  0.0001\n",
      "Epoch =  2953 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898715106 learning rate =  0.0001\n",
      "Epoch =  2954 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589868556 learning rate =  0.0001\n",
      "Epoch =  2955 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898656207 learning rate =  0.0001\n",
      "Epoch =  2956 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898627043 learning rate =  0.0001\n",
      "Epoch =  2957 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898598057 learning rate =  0.0001\n",
      "Epoch =  2958 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589856925 learning rate =  0.0001\n",
      "Epoch =  2959 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898540623 learning rate =  0.0001\n",
      "Epoch =  2960 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589851216 learning rate =  0.0001\n",
      "Epoch =  2961 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898483887 learning rate =  0.0001\n",
      "Epoch =  2962 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589845581 learning rate =  0.0001\n",
      "Epoch =  2963 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589842787 learning rate =  0.0001\n",
      "Epoch =  2964 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898400127 learning rate =  0.0001\n",
      "Epoch =  2965 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589837253 learning rate =  0.0001\n",
      "Epoch =  2966 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898345153 learning rate =  0.0001\n",
      "Epoch =  2967 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898317913 learning rate =  0.0001\n",
      "Epoch =  2968 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589829084 learning rate =  0.0001\n",
      "Epoch =  2969 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898263925 learning rate =  0.0001\n",
      "Epoch =  2970 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898237186 learning rate =  0.0001\n",
      "Epoch =  2971 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898210625 learning rate =  0.0001\n",
      "Epoch =  2972 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898184215 learning rate =  0.0001\n",
      "Epoch =  2973 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898157983 learning rate =  0.0001\n",
      "Epoch =  2974 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898131915 learning rate =  0.0001\n",
      "Epoch =  2975 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898106 learning rate =  0.0001\n",
      "Epoch =  2976 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589808025 learning rate =  0.0001\n",
      "Epoch =  2977 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898054657 learning rate =  0.0001\n",
      "Epoch =  2978 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898029224 learning rate =  0.0001\n",
      "Epoch =  2979 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545898003964 learning rate =  0.0001\n",
      "Epoch =  2980 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897978846 learning rate =  0.0001\n",
      "Epoch =  2981 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897953866 learning rate =  0.0001\n",
      "Epoch =  2982 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897929086 learning rate =  0.0001\n",
      "Epoch =  2983 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589790444 learning rate =  0.0001\n",
      "Epoch =  2984 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897879943 learning rate =  0.0001\n",
      "Epoch =  2985 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589785558 learning rate =  0.0001\n",
      "Epoch =  2986 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589783139 learning rate =  0.0001\n",
      "Epoch =  2987 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897807357 learning rate =  0.0001\n",
      "Epoch =  2988 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897783456 learning rate =  0.0001\n",
      "Epoch =  2989 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458977597 learning rate =  0.0001\n",
      "Epoch =  2990 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458977361 learning rate =  0.0001\n",
      "Epoch =  2991 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897712655 learning rate =  0.0001\n",
      "Epoch =  2992 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589768934 learning rate =  0.0001\n",
      "Epoch =  2993 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897666176 learning rate =  0.0001\n",
      "Epoch =  2994 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897643172 learning rate =  0.0001\n",
      "Epoch =  2995 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458976203 learning rate =  0.0001\n",
      "Epoch =  2996 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589759756 learning rate =  0.0001\n",
      "Epoch =  2997 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589757497 learning rate =  0.0001\n",
      "Epoch =  2998 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589755251 learning rate =  0.0001\n",
      "Epoch =  2999 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589753019 learning rate =  0.0001\n",
      "Epoch =  3000 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897508023 learning rate =  0.0001\n",
      "Epoch =  3001 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897485983 learning rate =  0.0001\n",
      "Epoch =  3002 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897464093 learning rate =  0.0001\n",
      "Epoch =  3003 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897442333 learning rate =  0.0001\n",
      "Epoch =  3004 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897420724 learning rate =  0.0001\n",
      "Epoch =  3005 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897399216 learning rate =  0.0001\n",
      "Epoch =  3006 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897377856 learning rate =  0.0001\n",
      "Epoch =  3007 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589735663 learning rate =  0.0001\n",
      "Epoch =  3008 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897335547 learning rate =  0.0001\n",
      "Epoch =  3009 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897314564 learning rate =  0.0001\n",
      "Epoch =  3010 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897293723 learning rate =  0.0001\n",
      "Epoch =  3011 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897273024 learning rate =  0.0001\n",
      "Epoch =  3012 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897252454 learning rate =  0.0001\n",
      "Epoch =  3013 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589723199 learning rate =  0.0001\n",
      "Epoch =  3014 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897211686 learning rate =  0.0001\n",
      "Epoch =  3015 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589719148 learning rate =  0.0001\n",
      "Epoch =  3016 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458971714 learning rate =  0.0001\n",
      "Epoch =  3017 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897151464 learning rate =  0.0001\n",
      "Epoch =  3018 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589713164 learning rate =  0.0001\n",
      "Epoch =  3019 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897111953 learning rate =  0.0001\n",
      "Epoch =  3020 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589709237 learning rate =  0.0001\n",
      "Epoch =  3021 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897072926 learning rate =  0.0001\n",
      "Epoch =  3022 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589705359 learning rate =  0.0001\n",
      "Epoch =  3023 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545897034375 learning rate =  0.0001\n",
      "Epoch =  3024 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589701528 learning rate =  0.0001\n",
      "Epoch =  3025 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896996317 learning rate =  0.0001\n",
      "Epoch =  3026 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589697745 learning rate =  0.0001\n",
      "Epoch =  3027 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896958707 learning rate =  0.0001\n",
      "Epoch =  3028 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896940086 learning rate =  0.0001\n",
      "Epoch =  3029 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589692157 learning rate =  0.0001\n",
      "Epoch =  3030 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896903187 learning rate =  0.0001\n",
      "Epoch =  3031 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896884912 learning rate =  0.0001\n",
      "Epoch =  3032 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589686674 learning rate =  0.0001\n",
      "Epoch =  3033 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589684869 learning rate =  0.0001\n",
      "Epoch =  3034 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896830742 learning rate =  0.0001\n",
      "Epoch =  3035 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589681293 learning rate =  0.0001\n",
      "Epoch =  3036 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896795215 learning rate =  0.0001\n",
      "Epoch =  3037 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458967776 learning rate =  0.0001\n",
      "Epoch =  3038 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896760105 learning rate =  0.0001\n",
      "Epoch =  3039 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896742715 learning rate =  0.0001\n",
      "Epoch =  3040 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589672543 learning rate =  0.0001\n",
      "Epoch =  3041 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589670826 learning rate =  0.0001\n",
      "Epoch =  3042 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896691174 learning rate =  0.0001\n",
      "Epoch =  3043 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589667424 learning rate =  0.0001\n",
      "Epoch =  3044 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589665737 learning rate =  0.0001\n",
      "Epoch =  3045 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896640637 learning rate =  0.0001\n",
      "Epoch =  3046 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896623965 learning rate =  0.0001\n",
      "Epoch =  3047 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896607423 learning rate =  0.0001\n",
      "Epoch =  3048 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896590987 learning rate =  0.0001\n",
      "Epoch =  3049 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589657464 learning rate =  0.0001\n",
      "Epoch =  3050 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896558422 learning rate =  0.0001\n",
      "Epoch =  3051 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896542275 learning rate =  0.0001\n",
      "Epoch =  3052 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896526253 learning rate =  0.0001\n",
      "Epoch =  3053 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896510305 learning rate =  0.0001\n",
      "Epoch =  3054 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589649448 learning rate =  0.0001\n",
      "Epoch =  3055 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896478726 learning rate =  0.0001\n",
      "Epoch =  3056 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896463085 learning rate =  0.0001\n",
      "Epoch =  3057 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589644755 learning rate =  0.0001\n",
      "Epoch =  3058 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896432106 learning rate =  0.0001\n",
      "Epoch =  3059 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589641674 learning rate =  0.0001\n",
      "Epoch =  3060 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896401477 learning rate =  0.0001\n",
      "Epoch =  3061 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896386316 learning rate =  0.0001\n",
      "Epoch =  3062 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589637125 learning rate =  0.0001\n",
      "Epoch =  3063 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589635631 learning rate =  0.0001\n",
      "Epoch =  3064 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458963414 learning rate =  0.0001\n",
      "Epoch =  3065 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589632662 learning rate =  0.0001\n",
      "Epoch =  3066 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896311935 learning rate =  0.0001\n",
      "Epoch =  3067 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896297333 learning rate =  0.0001\n",
      "Epoch =  3068 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896282825 learning rate =  0.0001\n",
      "Epoch =  3069 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896268383 learning rate =  0.0001\n",
      "Epoch =  3070 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589625406 learning rate =  0.0001\n",
      "Epoch =  3071 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896239815 learning rate =  0.0001\n",
      "Epoch =  3072 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896225666 learning rate =  0.0001\n",
      "Epoch =  3073 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589621161 learning rate =  0.0001\n",
      "Epoch =  3074 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589619762 learning rate =  0.0001\n",
      "Epoch =  3075 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589618371 learning rate =  0.0001\n",
      "Epoch =  3076 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896169906 learning rate =  0.0001\n",
      "Epoch =  3077 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589615619 learning rate =  0.0001\n",
      "Epoch =  3078 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896142564 learning rate =  0.0001\n",
      "Epoch =  3079 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589612901 learning rate =  0.0001\n",
      "Epoch =  3080 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589611552 learning rate =  0.0001\n",
      "Epoch =  3081 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896102125 learning rate =  0.0001\n",
      "Epoch =  3082 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896088834 learning rate =  0.0001\n",
      "Epoch =  3083 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896075613 learning rate =  0.0001\n",
      "Epoch =  3084 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896062477 learning rate =  0.0001\n",
      "Epoch =  3085 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896049425 learning rate =  0.0001\n",
      "Epoch =  3086 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545896036458 learning rate =  0.0001\n",
      "Epoch =  3087 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589602356 learning rate =  0.0001\n",
      "Epoch =  3088 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589601074 learning rate =  0.0001\n",
      "Epoch =  3089 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895998013 learning rate =  0.0001\n",
      "Epoch =  3090 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589598536 learning rate =  0.0001\n",
      "Epoch =  3091 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895972784 learning rate =  0.0001\n",
      "Epoch =  3092 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895960274 learning rate =  0.0001\n",
      "Epoch =  3093 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895947857 learning rate =  0.0001\n",
      "Epoch =  3094 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895935516 learning rate =  0.0001\n",
      "Epoch =  3095 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895923255 learning rate =  0.0001\n",
      "Epoch =  3096 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895911042 learning rate =  0.0001\n",
      "Epoch =  3097 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589589894 learning rate =  0.0001\n",
      "Epoch =  3098 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895886906 learning rate =  0.0001\n",
      "Epoch =  3099 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589587494 learning rate =  0.0001\n",
      "Epoch =  3100 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895863045 learning rate =  0.0001\n",
      "Epoch =  3101 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895851232 learning rate =  0.0001\n",
      "Epoch =  3102 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895839486 learning rate =  0.0001\n",
      "Epoch =  3103 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895827816 learning rate =  0.0001\n",
      "Epoch =  3104 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895816207 learning rate =  0.0001\n",
      "Epoch =  3105 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589580469 learning rate =  0.0001\n",
      "Epoch =  3106 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589579324 learning rate =  0.0001\n",
      "Epoch =  3107 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589578185 learning rate =  0.0001\n",
      "Epoch =  3108 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895770546 learning rate =  0.0001\n",
      "Epoch =  3109 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895759306 learning rate =  0.0001\n",
      "Epoch =  3110 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895748124 learning rate =  0.0001\n",
      "Epoch =  3111 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895737017 learning rate =  0.0001\n",
      "Epoch =  3112 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895725986 learning rate =  0.0001\n",
      "Epoch =  3113 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895715026 learning rate =  0.0001\n",
      "Epoch =  3114 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589570412 learning rate =  0.0001\n",
      "Epoch =  3115 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589569328 learning rate =  0.0001\n",
      "Epoch =  3116 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895682536 learning rate =  0.0001\n",
      "Epoch =  3117 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895671834 learning rate =  0.0001\n",
      "Epoch =  3118 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895661216 learning rate =  0.0001\n",
      "Epoch =  3119 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589565064 learning rate =  0.0001\n",
      "Epoch =  3120 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589564013 learning rate =  0.0001\n",
      "Epoch =  3121 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895629716 learning rate =  0.0001\n",
      "Epoch =  3122 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895619334 learning rate =  0.0001\n",
      "Epoch =  3123 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895609044 learning rate =  0.0001\n",
      "Epoch =  3124 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895598808 learning rate =  0.0001\n",
      "Epoch =  3125 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895588625 learning rate =  0.0001\n",
      "Epoch =  3126 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895578517 learning rate =  0.0001\n",
      "Epoch =  3127 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895568485 learning rate =  0.0001\n",
      "Epoch =  3128 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589555848 learning rate =  0.0001\n",
      "Epoch =  3129 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589554857 learning rate =  0.0001\n",
      "Epoch =  3130 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895538705 learning rate =  0.0001\n",
      "Epoch =  3131 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589552889 learning rate =  0.0001\n",
      "Epoch =  3132 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895519147 learning rate =  0.0001\n",
      "Epoch =  3133 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895509457 learning rate =  0.0001\n",
      "Epoch =  3134 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895499847 learning rate =  0.0001\n",
      "Epoch =  3135 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895490263 learning rate =  0.0001\n",
      "Epoch =  3136 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589548079 learning rate =  0.0001\n",
      "Epoch =  3137 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895471345 learning rate =  0.0001\n",
      "Epoch =  3138 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895461953 learning rate =  0.0001\n",
      "Epoch =  3139 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895452627 learning rate =  0.0001\n",
      "Epoch =  3140 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589544335 learning rate =  0.0001\n",
      "Epoch =  3141 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895434153 learning rate =  0.0001\n",
      "Epoch =  3142 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895424987 learning rate =  0.0001\n",
      "Epoch =  3143 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458954159 learning rate =  0.0001\n",
      "Epoch =  3144 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895406855 learning rate =  0.0001\n",
      "Epoch =  3145 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895397857 learning rate =  0.0001\n",
      "Epoch =  3146 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895388936 learning rate =  0.0001\n",
      "Epoch =  3147 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589538005 learning rate =  0.0001\n",
      "Epoch =  3148 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895371234 learning rate =  0.0001\n",
      "Epoch =  3149 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895362472 learning rate =  0.0001\n",
      "Epoch =  3150 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589535376 learning rate =  0.0001\n",
      "Epoch =  3151 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895345104 learning rate =  0.0001\n",
      "Epoch =  3152 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895336498 learning rate =  0.0001\n",
      "Epoch =  3153 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895327953 learning rate =  0.0001\n",
      "Epoch =  3154 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895319476 learning rate =  0.0001\n",
      "Epoch =  3155 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589531103 learning rate =  0.0001\n",
      "Epoch =  3156 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895302645 learning rate =  0.0001\n",
      "Epoch =  3157 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895294314 learning rate =  0.0001\n",
      "Epoch =  3158 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895286014 learning rate =  0.0001\n",
      "Epoch =  3159 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589527778 learning rate =  0.0001\n",
      "Epoch =  3160 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589526961 learning rate =  0.0001\n",
      "Epoch =  3161 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895261486 learning rate =  0.0001\n",
      "Epoch =  3162 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458952534 learning rate =  0.0001\n",
      "Epoch =  3163 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895245384 learning rate =  0.0001\n",
      "Epoch =  3164 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895237395 learning rate =  0.0001\n",
      "Epoch =  3165 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895229454 learning rate =  0.0001\n",
      "Epoch =  3166 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895221576 learning rate =  0.0001\n",
      "Epoch =  3167 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895213742 learning rate =  0.0001\n",
      "Epoch =  3168 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589520597 learning rate =  0.0001\n",
      "Epoch =  3169 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589519823 learning rate =  0.0001\n",
      "Epoch =  3170 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589519054 learning rate =  0.0001\n",
      "Epoch =  3171 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458951829 learning rate =  0.0001\n",
      "Epoch =  3172 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589517531 learning rate =  0.0001\n",
      "Epoch =  3173 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589516775 learning rate =  0.0001\n",
      "Epoch =  3174 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895160256 learning rate =  0.0001\n",
      "Epoch =  3175 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895152796 learning rate =  0.0001\n",
      "Epoch =  3176 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895145397 learning rate =  0.0001\n",
      "Epoch =  3177 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589513803 learning rate =  0.0001\n",
      "Epoch =  3178 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895130716 learning rate =  0.0001\n",
      "Epoch =  3179 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895123464 learning rate =  0.0001\n",
      "Epoch =  3180 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589511624 learning rate =  0.0001\n",
      "Epoch =  3181 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895109057 learning rate =  0.0001\n",
      "Epoch =  3182 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589510191 learning rate =  0.0001\n",
      "Epoch =  3183 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589509483 learning rate =  0.0001\n",
      "Epoch =  3184 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895087777 learning rate =  0.0001\n",
      "Epoch =  3185 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589508076 learning rate =  0.0001\n",
      "Epoch =  3186 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589507381 learning rate =  0.0001\n",
      "Epoch =  3187 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895066895 learning rate =  0.0001\n",
      "Epoch =  3188 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895060008 learning rate =  0.0001\n",
      "Epoch =  3189 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895053186 learning rate =  0.0001\n",
      "Epoch =  3190 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895046405 learning rate =  0.0001\n",
      "Epoch =  3191 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589503967 learning rate =  0.0001\n",
      "Epoch =  3192 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895032954 learning rate =  0.0001\n",
      "Epoch =  3193 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895026297 learning rate =  0.0001\n",
      "Epoch =  3194 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895019675 learning rate =  0.0001\n",
      "Epoch =  3195 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895013076 learning rate =  0.0001\n",
      "Epoch =  3196 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895006544 learning rate =  0.0001\n",
      "Epoch =  3197 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545895000047 learning rate =  0.0001\n",
      "Epoch =  3198 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458949936 learning rate =  0.0001\n",
      "Epoch =  3199 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589498717 learning rate =  0.0001\n",
      "Epoch =  3200 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589498077 learning rate =  0.0001\n",
      "Epoch =  3201 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589497445 learning rate =  0.0001\n",
      "Epoch =  3202 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589496816 learning rate =  0.0001\n",
      "Epoch =  3203 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894961886 learning rate =  0.0001\n",
      "Epoch =  3204 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589495567 learning rate =  0.0001\n",
      "Epoch =  3205 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589494949 learning rate =  0.0001\n",
      "Epoch =  3206 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589494333 learning rate =  0.0001\n",
      "Epoch =  3207 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589493723 learning rate =  0.0001\n",
      "Epoch =  3208 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894931137 learning rate =  0.0001\n",
      "Epoch =  3209 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589492513 learning rate =  0.0001\n",
      "Epoch =  3210 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894919134 learning rate =  0.0001\n",
      "Epoch =  3211 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894913183 learning rate =  0.0001\n",
      "Epoch =  3212 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894907268 learning rate =  0.0001\n",
      "Epoch =  3213 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589490139 learning rate =  0.0001\n",
      "Epoch =  3214 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894895535 learning rate =  0.0001\n",
      "Epoch =  3215 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589488972 learning rate =  0.0001\n",
      "Epoch =  3216 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894883953 learning rate =  0.0001\n",
      "Epoch =  3217 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894878215 learning rate =  0.0001\n",
      "Epoch =  3218 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589487252 learning rate =  0.0001\n",
      "Epoch =  3219 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894866838 learning rate =  0.0001\n",
      "Epoch =  3220 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894861216 learning rate =  0.0001\n",
      "Epoch =  3221 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894855624 learning rate =  0.0001\n",
      "Epoch =  3222 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894850047 learning rate =  0.0001\n",
      "Epoch =  3223 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894844522 learning rate =  0.0001\n",
      "Epoch =  3224 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894839024 learning rate =  0.0001\n",
      "Epoch =  3225 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894833575 learning rate =  0.0001\n",
      "Epoch =  3226 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894828144 learning rate =  0.0001\n",
      "Epoch =  3227 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894822757 learning rate =  0.0001\n",
      "Epoch =  3228 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458948174 learning rate =  0.0001\n",
      "Epoch =  3229 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894812055 learning rate =  0.0001\n",
      "Epoch =  3230 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589480676 learning rate =  0.0001\n",
      "Epoch =  3231 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894801517 learning rate =  0.0001\n",
      "Epoch =  3232 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894796276 learning rate =  0.0001\n",
      "Epoch =  3233 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589479108 learning rate =  0.0001\n",
      "Epoch =  3234 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894785925 learning rate =  0.0001\n",
      "Epoch =  3235 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589478078 learning rate =  0.0001\n",
      "Epoch =  3236 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894775684 learning rate =  0.0001\n",
      "Epoch =  3237 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894770617 learning rate =  0.0001\n",
      "Epoch =  3238 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589476559 learning rate =  0.0001\n",
      "Epoch =  3239 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894760576 learning rate =  0.0001\n",
      "Epoch =  3240 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894755624 learning rate =  0.0001\n",
      "Epoch =  3241 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894750677 learning rate =  0.0001\n",
      "Epoch =  3242 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589474577 learning rate =  0.0001\n",
      "Epoch =  3243 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894740885 learning rate =  0.0001\n",
      "Epoch =  3244 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589473604 learning rate =  0.0001\n",
      "Epoch =  3245 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894731217 learning rate =  0.0001\n",
      "Epoch =  3246 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589472645 learning rate =  0.0001\n",
      "Epoch =  3247 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894721683 learning rate =  0.0001\n",
      "Epoch =  3248 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589471695 learning rate =  0.0001\n",
      "Epoch =  3249 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894712237 learning rate =  0.0001\n",
      "Epoch =  3250 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894707574 learning rate =  0.0001\n",
      "Epoch =  3251 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894702933 learning rate =  0.0001\n",
      "Epoch =  3252 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589469832 learning rate =  0.0001\n",
      "Epoch =  3253 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589469373 learning rate =  0.0001\n",
      "Epoch =  3254 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894689175 learning rate =  0.0001\n",
      "Epoch =  3255 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894684632 learning rate =  0.0001\n",
      "Epoch =  3256 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589468014 learning rate =  0.0001\n",
      "Epoch =  3257 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589467568 learning rate =  0.0001\n",
      "Epoch =  3258 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894671225 learning rate =  0.0001\n",
      "Epoch =  3259 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894666807 learning rate =  0.0001\n",
      "Epoch =  3260 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894662423 learning rate =  0.0001\n",
      "Epoch =  3261 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894658054 learning rate =  0.0001\n",
      "Epoch =  3262 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589465373 learning rate =  0.0001\n",
      "Epoch =  3263 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894649416 learning rate =  0.0001\n",
      "Epoch =  3264 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894645135 learning rate =  0.0001\n",
      "Epoch =  3265 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589464088 learning rate =  0.0001\n",
      "Epoch =  3266 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894636653 learning rate =  0.0001\n",
      "Epoch =  3267 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894632443 learning rate =  0.0001\n",
      "Epoch =  3268 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589462827 learning rate =  0.0001\n",
      "Epoch =  3269 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894624125 learning rate =  0.0001\n",
      "Epoch =  3270 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589462 learning rate =  0.0001\n",
      "Epoch =  3271 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894615887 learning rate =  0.0001\n",
      "Epoch =  3272 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894611824 learning rate =  0.0001\n",
      "Epoch =  3273 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894607774 learning rate =  0.0001\n",
      "Epoch =  3274 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589460376 learning rate =  0.0001\n",
      "Epoch =  3275 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589459975 learning rate =  0.0001\n",
      "Epoch =  3276 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894595775 learning rate =  0.0001\n",
      "Epoch =  3277 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894591827 learning rate =  0.0001\n",
      "Epoch =  3278 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589458791 learning rate =  0.0001\n",
      "Epoch =  3279 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894584 learning rate =  0.0001\n",
      "Epoch =  3280 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589458011 learning rate =  0.0001\n",
      "Epoch =  3281 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894576243 learning rate =  0.0001\n",
      "Epoch =  3282 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894572424 learning rate =  0.0001\n",
      "Epoch =  3283 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589456862 learning rate =  0.0001\n",
      "Epoch =  3284 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589456484 learning rate =  0.0001\n",
      "Epoch =  3285 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894561096 learning rate =  0.0001\n",
      "Epoch =  3286 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894557356 learning rate =  0.0001\n",
      "Epoch =  3287 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589455364 learning rate =  0.0001\n",
      "Epoch =  3288 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589454994 learning rate =  0.0001\n",
      "Epoch =  3289 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894546285 learning rate =  0.0001\n",
      "Epoch =  3290 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589454265 learning rate =  0.0001\n",
      "Epoch =  3291 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894539047 learning rate =  0.0001\n",
      "Epoch =  3292 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894535445 learning rate =  0.0001\n",
      "Epoch =  3293 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589453188 learning rate =  0.0001\n",
      "Epoch =  3294 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894528313 learning rate =  0.0001\n",
      "Epoch =  3295 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894524787 learning rate =  0.0001\n",
      "Epoch =  3296 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894521274 learning rate =  0.0001\n",
      "Epoch =  3297 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589451779 learning rate =  0.0001\n",
      "Epoch =  3298 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894514333 learning rate =  0.0001\n",
      "Epoch =  3299 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589451089 learning rate =  0.0001\n",
      "Epoch =  3300 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894507467 learning rate =  0.0001\n",
      "Epoch =  3301 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458945041 learning rate =  0.0001\n",
      "Epoch =  3302 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589450071 learning rate =  0.0001\n",
      "Epoch =  3303 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894497347 learning rate =  0.0001\n",
      "Epoch =  3304 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894494025 learning rate =  0.0001\n",
      "Epoch =  3305 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894490685 learning rate =  0.0001\n",
      "Epoch =  3306 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894487395 learning rate =  0.0001\n",
      "Epoch =  3307 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589448412 learning rate =  0.0001\n",
      "Epoch =  3308 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894480858 learning rate =  0.0001\n",
      "Epoch =  3309 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589447762 learning rate =  0.0001\n",
      "Epoch =  3310 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589447442 learning rate =  0.0001\n",
      "Epoch =  3311 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458944712 learning rate =  0.0001\n",
      "Epoch =  3312 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894468023 learning rate =  0.0001\n",
      "Epoch =  3313 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894464884 learning rate =  0.0001\n",
      "Epoch =  3314 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589446175 learning rate =  0.0001\n",
      "Epoch =  3315 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589445864 learning rate =  0.0001\n",
      "Epoch =  3316 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894455527 learning rate =  0.0001\n",
      "Epoch =  3317 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894452445 learning rate =  0.0001\n",
      "Epoch =  3318 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894449385 learning rate =  0.0001\n",
      "Epoch =  3319 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894446347 learning rate =  0.0001\n",
      "Epoch =  3320 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589444332 learning rate =  0.0001\n",
      "Epoch =  3321 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894440312 learning rate =  0.0001\n",
      "Epoch =  3322 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894437337 learning rate =  0.0001\n",
      "Epoch =  3323 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589443437 learning rate =  0.0001\n",
      "Epoch =  3324 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589443141 learning rate =  0.0001\n",
      "Epoch =  3325 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589442849 learning rate =  0.0001\n",
      "Epoch =  3326 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894425577 learning rate =  0.0001\n",
      "Epoch =  3327 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589442269 learning rate =  0.0001\n",
      "Epoch =  3328 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589441982 learning rate =  0.0001\n",
      "Epoch =  3329 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589441696 learning rate =  0.0001\n",
      "Epoch =  3330 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894414107 learning rate =  0.0001\n",
      "Epoch =  3331 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589441128 learning rate =  0.0001\n",
      "Epoch =  3332 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589440849 learning rate =  0.0001\n",
      "Epoch =  3333 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458944057 learning rate =  0.0001\n",
      "Epoch =  3334 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589440295 learning rate =  0.0001\n",
      "Epoch =  3335 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894400193 learning rate =  0.0001\n",
      "Epoch =  3336 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894397453 learning rate =  0.0001\n",
      "Epoch =  3337 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894394735 learning rate =  0.0001\n",
      "Epoch =  3338 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894392026 learning rate =  0.0001\n",
      "Epoch =  3339 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894389357 learning rate =  0.0001\n",
      "Epoch =  3340 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589438667 learning rate =  0.0001\n",
      "Epoch =  3341 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894384024 learning rate =  0.0001\n",
      "Epoch =  3342 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589438139 learning rate =  0.0001\n",
      "Epoch =  3343 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589437877 learning rate =  0.0001\n",
      "Epoch =  3344 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894376186 learning rate =  0.0001\n",
      "Epoch =  3345 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589437359 learning rate =  0.0001\n",
      "Epoch =  3346 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894371008 learning rate =  0.0001\n",
      "Epoch =  3347 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589436845 learning rate =  0.0001\n",
      "Epoch =  3348 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589436592 learning rate =  0.0001\n",
      "Epoch =  3349 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589436339 learning rate =  0.0001\n",
      "Epoch =  3350 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894360882 learning rate =  0.0001\n",
      "Epoch =  3351 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894358396 learning rate =  0.0001\n",
      "Epoch =  3352 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589435591 learning rate =  0.0001\n",
      "Epoch =  3353 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589435345 learning rate =  0.0001\n",
      "Epoch =  3354 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589435101 learning rate =  0.0001\n",
      "Epoch =  3355 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589434859 learning rate =  0.0001\n",
      "Epoch =  3356 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589434616 learning rate =  0.0001\n",
      "Epoch =  3357 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894343754 learning rate =  0.0001\n",
      "Epoch =  3358 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589434138 learning rate =  0.0001\n",
      "Epoch =  3359 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894338998 learning rate =  0.0001\n",
      "Epoch =  3360 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589433664 learning rate =  0.0001\n",
      "Epoch =  3361 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589433431 learning rate =  0.0001\n",
      "Epoch =  3362 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894331977 learning rate =  0.0001\n",
      "Epoch =  3363 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894329663 learning rate =  0.0001\n",
      "Epoch =  3364 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894327363 learning rate =  0.0001\n",
      "Epoch =  3365 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589432508 learning rate =  0.0001\n",
      "Epoch =  3366 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894322824 learning rate =  0.0001\n",
      "Epoch =  3367 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589432057 learning rate =  0.0001\n",
      "Epoch =  3368 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894318325 learning rate =  0.0001\n",
      "Epoch =  3369 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894316096 learning rate =  0.0001\n",
      "Epoch =  3370 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589431388 learning rate =  0.0001\n",
      "Epoch =  3371 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894311673 learning rate =  0.0001\n",
      "Epoch =  3372 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589430948 learning rate =  0.0001\n",
      "Epoch =  3373 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894307308 learning rate =  0.0001\n",
      "Epoch =  3374 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589430516 learning rate =  0.0001\n",
      "Epoch =  3375 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894303018 learning rate =  0.0001\n",
      "Epoch =  3376 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589430088 learning rate =  0.0001\n",
      "Epoch =  3377 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894298763 learning rate =  0.0001\n",
      "Epoch =  3378 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589429664 learning rate =  0.0001\n",
      "Epoch =  3379 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589429455 learning rate =  0.0001\n",
      "Epoch =  3380 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589429249 learning rate =  0.0001\n",
      "Epoch =  3381 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894290405 learning rate =  0.0001\n",
      "Epoch =  3382 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894288363 learning rate =  0.0001\n",
      "Epoch =  3383 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894286324 learning rate =  0.0001\n",
      "Epoch =  3384 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589428429 learning rate =  0.0001\n",
      "Epoch =  3385 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894282265 learning rate =  0.0001\n",
      "Epoch =  3386 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589428026 learning rate =  0.0001\n",
      "Epoch =  3387 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894278273 learning rate =  0.0001\n",
      "Epoch =  3388 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894276292 learning rate =  0.0001\n",
      "Epoch =  3389 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589427432 learning rate =  0.0001\n",
      "Epoch =  3390 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589427236 learning rate =  0.0001\n",
      "Epoch =  3391 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589427042 learning rate =  0.0001\n",
      "Epoch =  3392 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589426848 learning rate =  0.0001\n",
      "Epoch =  3393 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589426657 learning rate =  0.0001\n",
      "Epoch =  3394 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894264666 learning rate =  0.0001\n",
      "Epoch =  3395 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589426276 learning rate =  0.0001\n",
      "Epoch =  3396 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894260882 learning rate =  0.0001\n",
      "Epoch =  3397 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894259 learning rate =  0.0001\n",
      "Epoch =  3398 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894257134 learning rate =  0.0001\n",
      "Epoch =  3399 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589425531 learning rate =  0.0001\n",
      "Epoch =  3400 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589425347 learning rate =  0.0001\n",
      "Epoch =  3401 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589425166 learning rate =  0.0001\n",
      "Epoch =  3402 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589424983 learning rate =  0.0001\n",
      "Epoch =  3403 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589424803 learning rate =  0.0001\n",
      "Epoch =  3404 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589424625 learning rate =  0.0001\n",
      "Epoch =  3405 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589424445 learning rate =  0.0001\n",
      "Epoch =  3406 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894242684 learning rate =  0.0001\n",
      "Epoch =  3407 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894240916 learning rate =  0.0001\n",
      "Epoch =  3408 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589423916 learning rate =  0.0001\n",
      "Epoch =  3409 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894237435 learning rate =  0.0001\n",
      "Epoch =  3410 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894235725 learning rate =  0.0001\n",
      "Epoch =  3411 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589423401 learning rate =  0.0001\n",
      "Epoch =  3412 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589423232 learning rate =  0.0001\n",
      "Epoch =  3413 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894230613 learning rate =  0.0001\n",
      "Epoch =  3414 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589422894 learning rate =  0.0001\n",
      "Epoch =  3415 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589422727 learning rate =  0.0001\n",
      "Epoch =  3416 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894225604 learning rate =  0.0001\n",
      "Epoch =  3417 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894223956 learning rate =  0.0001\n",
      "Epoch =  3418 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894222296 learning rate =  0.0001\n",
      "Epoch =  3419 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894220666 learning rate =  0.0001\n",
      "Epoch =  3420 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894219045 learning rate =  0.0001\n",
      "Epoch =  3421 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894217433 learning rate =  0.0001\n",
      "Epoch =  3422 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894215834 learning rate =  0.0001\n",
      "Epoch =  3423 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589421423 learning rate =  0.0001\n",
      "Epoch =  3424 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589421269 learning rate =  0.0001\n",
      "Epoch =  3425 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458942111 learning rate =  0.0001\n",
      "Epoch =  3426 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894209546 learning rate =  0.0001\n",
      "Epoch =  3427 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894207996 learning rate =  0.0001\n",
      "Epoch =  3428 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894206455 learning rate =  0.0001\n",
      "Epoch =  3429 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589420489 learning rate =  0.0001\n",
      "Epoch =  3430 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589420337 learning rate =  0.0001\n",
      "Epoch =  3431 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589420184 learning rate =  0.0001\n",
      "Epoch =  3432 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589420033 learning rate =  0.0001\n",
      "Epoch =  3433 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589419885 learning rate =  0.0001\n",
      "Epoch =  3434 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589419736 learning rate =  0.0001\n",
      "Epoch =  3435 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894195886 learning rate =  0.0001\n",
      "Epoch =  3436 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894194424 learning rate =  0.0001\n",
      "Epoch =  3437 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589419297 learning rate =  0.0001\n",
      "Epoch =  3438 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589419154 learning rate =  0.0001\n",
      "Epoch =  3439 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894190086 learning rate =  0.0001\n",
      "Epoch =  3440 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894188656 learning rate =  0.0001\n",
      "Epoch =  3441 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589418724 learning rate =  0.0001\n",
      "Epoch =  3442 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894185836 learning rate =  0.0001\n",
      "Epoch =  3443 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894184432 learning rate =  0.0001\n",
      "Epoch =  3444 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894183034 learning rate =  0.0001\n",
      "Epoch =  3445 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589418165 learning rate =  0.0001\n",
      "Epoch =  3446 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894180267 learning rate =  0.0001\n",
      "Epoch =  3447 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894178895 learning rate =  0.0001\n",
      "Epoch =  3448 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589417753 learning rate =  0.0001\n",
      "Epoch =  3449 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894176186 learning rate =  0.0001\n",
      "Epoch =  3450 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589417484 learning rate =  0.0001\n",
      "Epoch =  3451 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458941735 learning rate =  0.0001\n",
      "Epoch =  3452 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894172162 learning rate =  0.0001\n",
      "Epoch =  3453 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894170857 learning rate =  0.0001\n",
      "Epoch =  3454 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589416953 learning rate =  0.0001\n",
      "Epoch =  3455 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894168223 learning rate =  0.0001\n",
      "Epoch =  3456 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894166926 learning rate =  0.0001\n",
      "Epoch =  3457 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894165643 learning rate =  0.0001\n",
      "Epoch =  3458 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894164355 learning rate =  0.0001\n",
      "Epoch =  3459 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589416307 learning rate =  0.0001\n",
      "Epoch =  3460 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894161793 learning rate =  0.0001\n",
      "Epoch =  3461 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589416055 learning rate =  0.0001\n",
      "Epoch =  3462 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589415929 learning rate =  0.0001\n",
      "Epoch =  3463 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894158036 learning rate =  0.0001\n",
      "Epoch =  3464 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894156806 learning rate =  0.0001\n",
      "Epoch =  3465 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894155593 learning rate =  0.0001\n",
      "Epoch =  3466 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894154363 learning rate =  0.0001\n",
      "Epoch =  3467 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589415316 learning rate =  0.0001\n",
      "Epoch =  3468 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589415196 learning rate =  0.0001\n",
      "Epoch =  3469 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589415077 learning rate =  0.0001\n",
      "Epoch =  3470 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894149585 learning rate =  0.0001\n",
      "Epoch =  3471 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589414841 learning rate =  0.0001\n",
      "Epoch =  3472 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589414723 learning rate =  0.0001\n",
      "Epoch =  3473 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589414607 learning rate =  0.0001\n",
      "Epoch =  3474 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589414491 learning rate =  0.0001\n",
      "Epoch =  3475 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589414374 learning rate =  0.0001\n",
      "Epoch =  3476 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589414259 learning rate =  0.0001\n",
      "Epoch =  3477 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589414146 learning rate =  0.0001\n",
      "Epoch =  3478 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894140326 learning rate =  0.0001\n",
      "Epoch =  3479 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589413921 learning rate =  0.0001\n",
      "Epoch =  3480 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894138087 learning rate =  0.0001\n",
      "Epoch =  3481 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894136977 learning rate =  0.0001\n",
      "Epoch =  3482 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894135885 learning rate =  0.0001\n",
      "Epoch =  3483 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894134783 learning rate =  0.0001\n",
      "Epoch =  3484 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589413369 learning rate =  0.0001\n",
      "Epoch =  3485 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894132616 learning rate =  0.0001\n",
      "Epoch =  3486 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589413154 learning rate =  0.0001\n",
      "Epoch =  3487 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894130467 learning rate =  0.0001\n",
      "Epoch =  3488 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589412941 learning rate =  0.0001\n",
      "Epoch =  3489 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894128357 learning rate =  0.0001\n",
      "Epoch =  3490 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589412731 learning rate =  0.0001\n",
      "Epoch =  3491 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894126257 learning rate =  0.0001\n",
      "Epoch =  3492 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589412523 learning rate =  0.0001\n",
      "Epoch =  3493 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894124205 learning rate =  0.0001\n",
      "Epoch =  3494 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589412317 learning rate =  0.0001\n",
      "Epoch =  3495 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589412216 learning rate =  0.0001\n",
      "Epoch =  3496 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589412115 learning rate =  0.0001\n",
      "Epoch =  3497 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894120146 learning rate =  0.0001\n",
      "Epoch =  3498 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894119147 learning rate =  0.0001\n",
      "Epoch =  3499 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894118143 learning rate =  0.0001\n",
      "Epoch =  3500 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894117157 learning rate =  0.0001\n",
      "Epoch =  3501 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894116176 learning rate =  0.0001\n",
      "Epoch =  3502 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894115212 learning rate =  0.0001\n",
      "Epoch =  3503 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894114244 learning rate =  0.0001\n",
      "Epoch =  3504 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894113285 learning rate =  0.0001\n",
      "Epoch =  3505 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894112326 learning rate =  0.0001\n",
      "Epoch =  3506 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589411138 learning rate =  0.0001\n",
      "Epoch =  3507 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894110443 learning rate =  0.0001\n",
      "Epoch =  3508 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458941095 learning rate =  0.0001\n",
      "Epoch =  3509 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894108595 learning rate =  0.0001\n",
      "Epoch =  3510 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894107663 learning rate =  0.0001\n",
      "Epoch =  3511 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894106752 learning rate =  0.0001\n",
      "Epoch =  3512 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894105846 learning rate =  0.0001\n",
      "Epoch =  3513 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894104945 learning rate =  0.0001\n",
      "Epoch =  3514 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894104035 learning rate =  0.0001\n",
      "Epoch =  3515 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589410313 learning rate =  0.0001\n",
      "Epoch =  3516 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894102227 learning rate =  0.0001\n",
      "Epoch =  3517 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894101343 learning rate =  0.0001\n",
      "Epoch =  3518 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894100455 learning rate =  0.0001\n",
      "Epoch =  3519 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894099576 learning rate =  0.0001\n",
      "Epoch =  3520 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458940987 learning rate =  0.0001\n",
      "Epoch =  3521 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894097826 learning rate =  0.0001\n",
      "Epoch =  3522 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589409698 learning rate =  0.0001\n",
      "Epoch =  3523 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894096116 learning rate =  0.0001\n",
      "Epoch =  3524 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894095264 learning rate =  0.0001\n",
      "Epoch =  3525 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894094416 learning rate =  0.0001\n",
      "Epoch =  3526 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894093576 learning rate =  0.0001\n",
      "Epoch =  3527 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894092746 learning rate =  0.0001\n",
      "Epoch =  3528 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458940919 learning rate =  0.0001\n",
      "Epoch =  3529 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894091085 learning rate =  0.0001\n",
      "Epoch =  3530 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894090263 learning rate =  0.0001\n",
      "Epoch =  3531 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894089455 learning rate =  0.0001\n",
      "Epoch =  3532 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894088656 learning rate =  0.0001\n",
      "Epoch =  3533 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589408785 learning rate =  0.0001\n",
      "Epoch =  3534 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894087044 learning rate =  0.0001\n",
      "Epoch =  3535 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894086258 learning rate =  0.0001\n",
      "Epoch =  3536 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589408547 learning rate =  0.0001\n",
      "Epoch =  3537 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894084694 learning rate =  0.0001\n",
      "Epoch =  3538 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589408391 learning rate =  0.0001\n",
      "Epoch =  3539 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589408315 learning rate =  0.0001\n",
      "Epoch =  3540 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589408236 learning rate =  0.0001\n",
      "Epoch =  3541 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458940816 learning rate =  0.0001\n",
      "Epoch =  3542 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589408083 learning rate =  0.0001\n",
      "Epoch =  3543 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589408007 learning rate =  0.0001\n",
      "Epoch =  3544 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589407932 learning rate =  0.0001\n",
      "Epoch =  3545 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894078575 learning rate =  0.0001\n",
      "Epoch =  3546 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894077847 learning rate =  0.0001\n",
      "Epoch =  3547 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589407709 learning rate =  0.0001\n",
      "Epoch =  3548 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589407637 learning rate =  0.0001\n",
      "Epoch =  3549 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894075626 learning rate =  0.0001\n",
      "Epoch =  3550 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894074893 learning rate =  0.0001\n",
      "Epoch =  3551 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894074174 learning rate =  0.0001\n",
      "Epoch =  3552 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589407346 learning rate =  0.0001\n",
      "Epoch =  3553 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894072757 learning rate =  0.0001\n",
      "Epoch =  3554 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894072047 learning rate =  0.0001\n",
      "Epoch =  3555 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894071354 learning rate =  0.0001\n",
      "Epoch =  3556 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894070652 learning rate =  0.0001\n",
      "Epoch =  3557 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589406998 learning rate =  0.0001\n",
      "Epoch =  3558 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589406928 learning rate =  0.0001\n",
      "Epoch =  3559 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894068614 learning rate =  0.0001\n",
      "Epoch =  3560 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589406793 learning rate =  0.0001\n",
      "Epoch =  3561 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894067264 learning rate =  0.0001\n",
      "Epoch =  3562 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894066585 learning rate =  0.0001\n",
      "Epoch =  3563 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894065927 learning rate =  0.0001\n",
      "Epoch =  3564 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589406527 learning rate =  0.0001\n",
      "Epoch =  3565 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894064613 learning rate =  0.0001\n",
      "Epoch =  3566 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894063956 learning rate =  0.0001\n",
      "Epoch =  3567 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589406331 learning rate =  0.0001\n",
      "Epoch =  3568 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894062663 learning rate =  0.0001\n",
      "Epoch =  3569 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894062015 learning rate =  0.0001\n",
      "Epoch =  3570 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589406137 learning rate =  0.0001\n",
      "Epoch =  3571 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894060745 learning rate =  0.0001\n",
      "Epoch =  3572 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894060123 learning rate =  0.0001\n",
      "Epoch =  3573 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894059492 learning rate =  0.0001\n",
      "Epoch =  3574 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894058866 learning rate =  0.0001\n",
      "Epoch =  3575 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894058245 learning rate =  0.0001\n",
      "Epoch =  3576 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894057623 learning rate =  0.0001\n",
      "Epoch =  3577 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894057023 learning rate =  0.0001\n",
      "Epoch =  3578 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894056415 learning rate =  0.0001\n",
      "Epoch =  3579 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894055806 learning rate =  0.0001\n",
      "Epoch =  3580 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894055207 learning rate =  0.0001\n",
      "Epoch =  3581 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894054607 learning rate =  0.0001\n",
      "Epoch =  3582 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894054012 learning rate =  0.0001\n",
      "Epoch =  3583 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589405343 learning rate =  0.0001\n",
      "Epoch =  3584 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894052844 learning rate =  0.0001\n",
      "Epoch =  3585 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589405225 learning rate =  0.0001\n",
      "Epoch =  3586 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894051685 learning rate =  0.0001\n",
      "Epoch =  3587 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894051117 learning rate =  0.0001\n",
      "Epoch =  3588 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589405055 learning rate =  0.0001\n",
      "Epoch =  3589 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894049984 learning rate =  0.0001\n",
      "Epoch =  3590 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589404943 learning rate =  0.0001\n",
      "Epoch =  3591 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894048865 learning rate =  0.0001\n",
      "Epoch =  3592 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894048315 learning rate =  0.0001\n",
      "Epoch =  3593 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894047773 learning rate =  0.0001\n",
      "Epoch =  3594 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894047213 learning rate =  0.0001\n",
      "Epoch =  3595 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894046676 learning rate =  0.0001\n",
      "Epoch =  3596 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589404612 learning rate =  0.0001\n",
      "Epoch =  3597 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589404559 learning rate =  0.0001\n",
      "Epoch =  3598 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589404507 learning rate =  0.0001\n",
      "Epoch =  3599 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894044544 learning rate =  0.0001\n",
      "Epoch =  3600 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589404401 learning rate =  0.0001\n",
      "Epoch =  3601 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589404349 learning rate =  0.0001\n",
      "Epoch =  3602 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894042955 learning rate =  0.0001\n",
      "Epoch =  3603 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894042435 learning rate =  0.0001\n",
      "Epoch =  3604 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589404194 learning rate =  0.0001\n",
      "Epoch =  3605 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894041427 learning rate =  0.0001\n",
      "Epoch =  3606 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894040916 learning rate =  0.0001\n",
      "Epoch =  3607 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894040397 learning rate =  0.0001\n",
      "Epoch =  3608 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458940399 learning rate =  0.0001\n",
      "Epoch =  3609 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894039397 learning rate =  0.0001\n",
      "Epoch =  3610 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894038896 learning rate =  0.0001\n",
      "Epoch =  3611 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894038403 learning rate =  0.0001\n",
      "Epoch =  3612 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894037905 learning rate =  0.0001\n",
      "Epoch =  3613 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589403743 learning rate =  0.0001\n",
      "Epoch =  3614 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894036933 learning rate =  0.0001\n",
      "Epoch =  3615 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894036466 learning rate =  0.0001\n",
      "Epoch =  3616 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589403599 learning rate =  0.0001\n",
      "Epoch =  3617 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894035516 learning rate =  0.0001\n",
      "Epoch =  3618 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589403504 learning rate =  0.0001\n",
      "Epoch =  3619 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894034552 learning rate =  0.0001\n",
      "Epoch =  3620 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458940341 learning rate =  0.0001\n",
      "Epoch =  3621 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589403363 learning rate =  0.0001\n",
      "Epoch =  3622 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589403317 learning rate =  0.0001\n",
      "Epoch =  3623 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894032727 learning rate =  0.0001\n",
      "Epoch =  3624 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894032265 learning rate =  0.0001\n",
      "Epoch =  3625 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894031812 learning rate =  0.0001\n",
      "Epoch =  3626 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589403136 learning rate =  0.0001\n",
      "Epoch =  3627 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589403093 learning rate =  0.0001\n",
      "Epoch =  3628 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894030476 learning rate =  0.0001\n",
      "Epoch =  3629 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589403004 learning rate =  0.0001\n",
      "Epoch =  3630 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894029596 learning rate =  0.0001\n",
      "Epoch =  3631 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894029157 learning rate =  0.0001\n",
      "Epoch =  3632 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894028726 learning rate =  0.0001\n",
      "Epoch =  3633 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894028295 learning rate =  0.0001\n",
      "Epoch =  3634 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589402787 learning rate =  0.0001\n",
      "Epoch =  3635 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894027447 learning rate =  0.0001\n",
      "Epoch =  3636 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894027034 learning rate =  0.0001\n",
      "Epoch =  3637 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589402661 learning rate =  0.0001\n",
      "Epoch =  3638 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589402621 learning rate =  0.0001\n",
      "Epoch =  3639 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894025786 learning rate =  0.0001\n",
      "Epoch =  3640 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894025355 learning rate =  0.0001\n",
      "Epoch =  3641 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589402495 learning rate =  0.0001\n",
      "Epoch =  3642 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894024543 learning rate =  0.0001\n",
      "Epoch =  3643 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589402414 learning rate =  0.0001\n",
      "Epoch =  3644 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894023743 learning rate =  0.0001\n",
      "Epoch =  3645 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894023352 learning rate =  0.0001\n",
      "Epoch =  3646 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894022957 learning rate =  0.0001\n",
      "Epoch =  3647 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894022566 learning rate =  0.0001\n",
      "Epoch =  3648 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589402218 learning rate =  0.0001\n",
      "Epoch =  3649 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894021776 learning rate =  0.0001\n",
      "Epoch =  3650 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894021394 learning rate =  0.0001\n",
      "Epoch =  3651 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894021008 learning rate =  0.0001\n",
      "Epoch =  3652 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589402062 learning rate =  0.0001\n",
      "Epoch =  3653 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894020257 learning rate =  0.0001\n",
      "Epoch =  3654 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589401987 learning rate =  0.0001\n",
      "Epoch =  3655 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894019467 learning rate =  0.0001\n",
      "Epoch =  3656 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589401909 learning rate =  0.0001\n",
      "Epoch =  3657 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589401872 learning rate =  0.0001\n",
      "Epoch =  3658 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894018343 learning rate =  0.0001\n",
      "Epoch =  3659 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894017975 learning rate =  0.0001\n",
      "Epoch =  3660 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458940176 learning rate =  0.0001\n",
      "Epoch =  3661 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894017233 learning rate =  0.0001\n",
      "Epoch =  3662 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589401686 learning rate =  0.0001\n",
      "Epoch =  3663 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894016513 learning rate =  0.0001\n",
      "Epoch =  3664 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894016145 learning rate =  0.0001\n",
      "Epoch =  3665 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894015794 learning rate =  0.0001\n",
      "Epoch =  3666 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894015448 learning rate =  0.0001\n",
      "Epoch =  3667 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458940151 learning rate =  0.0001\n",
      "Epoch =  3668 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894014755 learning rate =  0.0001\n",
      "Epoch =  3669 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894014413 learning rate =  0.0001\n",
      "Epoch =  3670 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894014075 learning rate =  0.0001\n",
      "Epoch =  3671 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894013742 learning rate =  0.0001\n",
      "Epoch =  3672 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589401341 learning rate =  0.0001\n",
      "Epoch =  3673 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589401307 learning rate =  0.0001\n",
      "Epoch =  3674 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589401276 learning rate =  0.0001\n",
      "Epoch =  3675 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894012432 learning rate =  0.0001\n",
      "Epoch =  3676 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894012095 learning rate =  0.0001\n",
      "Epoch =  3677 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894011775 learning rate =  0.0001\n",
      "Epoch =  3678 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894011464 learning rate =  0.0001\n",
      "Epoch =  3679 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589401113 learning rate =  0.0001\n",
      "Epoch =  3680 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894010802 learning rate =  0.0001\n",
      "Epoch =  3681 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894010487 learning rate =  0.0001\n",
      "Epoch =  3682 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589401018 learning rate =  0.0001\n",
      "Epoch =  3683 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400986 learning rate =  0.0001\n",
      "Epoch =  3684 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894009555 learning rate =  0.0001\n",
      "Epoch =  3685 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894009244 learning rate =  0.0001\n",
      "Epoch =  3686 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400893 learning rate =  0.0001\n",
      "Epoch =  3687 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400863 learning rate =  0.0001\n",
      "Epoch =  3688 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894008324 learning rate =  0.0001\n",
      "Epoch =  3689 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894008027 learning rate =  0.0001\n",
      "Epoch =  3690 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400771 learning rate =  0.0001\n",
      "Epoch =  3691 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400742 learning rate =  0.0001\n",
      "Epoch =  3692 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894007125 learning rate =  0.0001\n",
      "Epoch =  3693 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894006815 learning rate =  0.0001\n",
      "Epoch =  3694 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894006535 learning rate =  0.0001\n",
      "Epoch =  3695 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400624 learning rate =  0.0001\n",
      "Epoch =  3696 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400595 learning rate =  0.0001\n",
      "Epoch =  3697 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400567 learning rate =  0.0001\n",
      "Epoch =  3698 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400538 learning rate =  0.0001\n",
      "Epoch =  3699 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400509 learning rate =  0.0001\n",
      "Epoch =  3700 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894004803 learning rate =  0.0001\n",
      "Epoch =  3701 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894004505 learning rate =  0.0001\n",
      "Epoch =  3702 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894004234 learning rate =  0.0001\n",
      "Epoch =  3703 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894003946 learning rate =  0.0001\n",
      "Epoch =  3704 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894003675 learning rate =  0.0001\n",
      "Epoch =  3705 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400339 learning rate =  0.0001\n",
      "Epoch =  3706 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894003106 learning rate =  0.0001\n",
      "Epoch =  3707 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400284 learning rate =  0.0001\n",
      "Epoch =  3708 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400258 learning rate =  0.0001\n",
      "Epoch =  3709 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894002294 learning rate =  0.0001\n",
      "Epoch =  3710 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894002027 learning rate =  0.0001\n",
      "Epoch =  3711 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400176 learning rate =  0.0001\n",
      "Epoch =  3712 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894001494 learning rate =  0.0001\n",
      "Epoch =  3713 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400124 learning rate =  0.0001\n",
      "Epoch =  3714 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400097 learning rate =  0.0001\n",
      "Epoch =  3715 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894000713 learning rate =  0.0001\n",
      "Epoch =  3716 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589400045 learning rate =  0.0001\n",
      "Epoch =  3717 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545894000198 learning rate =  0.0001\n",
      "Epoch =  3718 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399994 learning rate =  0.0001\n",
      "Epoch =  3719 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399968 learning rate =  0.0001\n",
      "Epoch =  3720 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893999434 learning rate =  0.0001\n",
      "Epoch =  3721 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893999176 learning rate =  0.0001\n",
      "Epoch =  3722 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893998923 learning rate =  0.0001\n",
      "Epoch =  3723 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399868 learning rate =  0.0001\n",
      "Epoch =  3724 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399844 learning rate =  0.0001\n",
      "Epoch =  3725 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893998204 learning rate =  0.0001\n",
      "Epoch =  3726 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399796 learning rate =  0.0001\n",
      "Epoch =  3727 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893997724 learning rate =  0.0001\n",
      "Epoch =  3728 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893997475 learning rate =  0.0001\n",
      "Epoch =  3729 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399724 learning rate =  0.0001\n",
      "Epoch =  3730 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893996996 learning rate =  0.0001\n",
      "Epoch =  3731 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893996774 learning rate =  0.0001\n",
      "Epoch =  3732 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399653 learning rate =  0.0001\n",
      "Epoch =  3733 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893996294 learning rate =  0.0001\n",
      "Epoch =  3734 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893996076 learning rate =  0.0001\n",
      "Epoch =  3735 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893995846 learning rate =  0.0001\n",
      "Epoch =  3736 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939956 learning rate =  0.0001\n",
      "Epoch =  3737 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893995393 learning rate =  0.0001\n",
      "Epoch =  3738 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893995175 learning rate =  0.0001\n",
      "Epoch =  3739 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893994953 learning rate =  0.0001\n",
      "Epoch =  3740 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893994718 learning rate =  0.0001\n",
      "Epoch =  3741 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893994496 learning rate =  0.0001\n",
      "Epoch =  3742 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399426 learning rate =  0.0001\n",
      "Epoch =  3743 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399405 learning rate =  0.0001\n",
      "Epoch =  3744 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399382 learning rate =  0.0001\n",
      "Epoch =  3745 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939936 learning rate =  0.0001\n",
      "Epoch =  3746 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893993376 learning rate =  0.0001\n",
      "Epoch =  3747 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893993168 learning rate =  0.0001\n",
      "Epoch =  3748 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399294 learning rate =  0.0001\n",
      "Epoch =  3749 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893992737 learning rate =  0.0001\n",
      "Epoch =  3750 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893992524 learning rate =  0.0001\n",
      "Epoch =  3751 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399232 learning rate =  0.0001\n",
      "Epoch =  3752 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939921 learning rate =  0.0001\n",
      "Epoch =  3753 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893991906 learning rate =  0.0001\n",
      "Epoch =  3754 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893991707 learning rate =  0.0001\n",
      "Epoch =  3755 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893991507 learning rate =  0.0001\n",
      "Epoch =  3756 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939913 learning rate =  0.0001\n",
      "Epoch =  3757 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939911 learning rate =  0.0001\n",
      "Epoch =  3758 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893990903 learning rate =  0.0001\n",
      "Epoch =  3759 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399071 learning rate =  0.0001\n",
      "Epoch =  3760 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893990516 learning rate =  0.0001\n",
      "Epoch =  3761 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893990317 learning rate =  0.0001\n",
      "Epoch =  3762 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589399012 learning rate =  0.0001\n",
      "Epoch =  3763 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893989944 learning rate =  0.0001\n",
      "Epoch =  3764 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398973 learning rate =  0.0001\n",
      "Epoch =  3765 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893989544 learning rate =  0.0001\n",
      "Epoch =  3766 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398935 learning rate =  0.0001\n",
      "Epoch =  3767 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893989175 learning rate =  0.0001\n",
      "Epoch =  3768 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893988984 learning rate =  0.0001\n",
      "Epoch =  3769 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893988785 learning rate =  0.0001\n",
      "Epoch =  3770 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893988607 learning rate =  0.0001\n",
      "Epoch =  3771 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893988416 learning rate =  0.0001\n",
      "Epoch =  3772 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398822 learning rate =  0.0001\n",
      "Epoch =  3773 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398803 learning rate =  0.0001\n",
      "Epoch =  3774 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893987865 learning rate =  0.0001\n",
      "Epoch =  3775 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893987674 learning rate =  0.0001\n",
      "Epoch =  3776 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398749 learning rate =  0.0001\n",
      "Epoch =  3777 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398733 learning rate =  0.0001\n",
      "Epoch =  3778 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893987137 learning rate =  0.0001\n",
      "Epoch =  3779 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398696 learning rate =  0.0001\n",
      "Epoch =  3780 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398679 learning rate =  0.0001\n",
      "Epoch =  3781 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893986617 learning rate =  0.0001\n",
      "Epoch =  3782 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398645 learning rate =  0.0001\n",
      "Epoch =  3783 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398629 learning rate =  0.0001\n",
      "Epoch =  3784 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398611 learning rate =  0.0001\n",
      "Epoch =  3785 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893985956 learning rate =  0.0001\n",
      "Epoch =  3786 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398578 learning rate =  0.0001\n",
      "Epoch =  3787 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398562 learning rate =  0.0001\n",
      "Epoch =  3788 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398546 learning rate =  0.0001\n",
      "Epoch =  3789 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398529 learning rate =  0.0001\n",
      "Epoch =  3790 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398514 learning rate =  0.0001\n",
      "Epoch =  3791 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398497 learning rate =  0.0001\n",
      "Epoch =  3792 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893984823 learning rate =  0.0001\n",
      "Epoch =  3793 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398467 learning rate =  0.0001\n",
      "Epoch =  3794 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939845 learning rate =  0.0001\n",
      "Epoch =  3795 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893984344 learning rate =  0.0001\n",
      "Epoch =  3796 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398419 learning rate =  0.0001\n",
      "Epoch =  3797 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893984033 learning rate =  0.0001\n",
      "Epoch =  3798 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893983877 learning rate =  0.0001\n",
      "Epoch =  3799 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398373 learning rate =  0.0001\n",
      "Epoch =  3800 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893983566 learning rate =  0.0001\n",
      "Epoch =  3801 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893983433 learning rate =  0.0001\n",
      "Epoch =  3802 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893983287 learning rate =  0.0001\n",
      "Epoch =  3803 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893983136 learning rate =  0.0001\n",
      "Epoch =  3804 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893982994 learning rate =  0.0001\n",
      "Epoch =  3805 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398285 learning rate =  0.0001\n",
      "Epoch =  3806 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893982696 learning rate =  0.0001\n",
      "Epoch =  3807 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893982554 learning rate =  0.0001\n",
      "Epoch =  3808 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398242 learning rate =  0.0001\n",
      "Epoch =  3809 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398228 learning rate =  0.0001\n",
      "Epoch =  3810 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893982128 learning rate =  0.0001\n",
      "Epoch =  3811 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893981994 learning rate =  0.0001\n",
      "Epoch =  3812 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893981857 learning rate =  0.0001\n",
      "Epoch =  3813 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893981706 learning rate =  0.0001\n",
      "Epoch =  3814 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893981577 learning rate =  0.0001\n",
      "Epoch =  3815 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893981453 learning rate =  0.0001\n",
      "Epoch =  3816 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893981297 learning rate =  0.0001\n",
      "Epoch =  3817 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893981177 learning rate =  0.0001\n",
      "Epoch =  3818 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893981057 learning rate =  0.0001\n",
      "Epoch =  3819 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398092 learning rate =  0.0001\n",
      "Epoch =  3820 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893980778 learning rate =  0.0001\n",
      "Epoch =  3821 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893980644 learning rate =  0.0001\n",
      "Epoch =  3822 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893980524 learning rate =  0.0001\n",
      "Epoch =  3823 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398038 learning rate =  0.0001\n",
      "Epoch =  3824 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398026 learning rate =  0.0001\n",
      "Epoch =  3825 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398012 learning rate =  0.0001\n",
      "Epoch =  3826 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589398 learning rate =  0.0001\n",
      "Epoch =  3827 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893979876 learning rate =  0.0001\n",
      "Epoch =  3828 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893979743 learning rate =  0.0001\n",
      "Epoch =  3829 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893979623 learning rate =  0.0001\n",
      "Epoch =  3830 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397949 learning rate =  0.0001\n",
      "Epoch =  3831 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397936 learning rate =  0.0001\n",
      "Epoch =  3832 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893979245 learning rate =  0.0001\n",
      "Epoch =  3833 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397913 learning rate =  0.0001\n",
      "Epoch =  3834 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893979015 learning rate =  0.0001\n",
      "Epoch =  3835 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893978886 learning rate =  0.0001\n",
      "Epoch =  3836 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397877 learning rate =  0.0001\n",
      "Epoch =  3837 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397865 learning rate =  0.0001\n",
      "Epoch =  3838 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893978535 learning rate =  0.0001\n",
      "Epoch =  3839 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397842 learning rate =  0.0001\n",
      "Epoch =  3840 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397831 learning rate =  0.0001\n",
      "Epoch =  3841 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893978184 learning rate =  0.0001\n",
      "Epoch =  3842 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397808 learning rate =  0.0001\n",
      "Epoch =  3843 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397796 learning rate =  0.0001\n",
      "Epoch =  3844 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893977855 learning rate =  0.0001\n",
      "Epoch =  3845 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893977736 learning rate =  0.0001\n",
      "Epoch =  3846 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397762 learning rate =  0.0001\n",
      "Epoch =  3847 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893977522 learning rate =  0.0001\n",
      "Epoch =  3848 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893977407 learning rate =  0.0001\n",
      "Epoch =  3849 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939773 learning rate =  0.0001\n",
      "Epoch =  3850 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893977207 learning rate =  0.0001\n",
      "Epoch =  3851 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893977105 learning rate =  0.0001\n",
      "Epoch =  3852 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893976976 learning rate =  0.0001\n",
      "Epoch =  3853 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397686 learning rate =  0.0001\n",
      "Epoch =  3854 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397677 learning rate =  0.0001\n",
      "Epoch =  3855 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893976674 learning rate =  0.0001\n",
      "Epoch =  3856 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893976568 learning rate =  0.0001\n",
      "Epoch =  3857 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397646 learning rate =  0.0001\n",
      "Epoch =  3858 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893976354 learning rate =  0.0001\n",
      "Epoch =  3859 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893976243 learning rate =  0.0001\n",
      "Epoch =  3860 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893976146 learning rate =  0.0001\n",
      "Epoch =  3861 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397602 learning rate =  0.0001\n",
      "Epoch =  3862 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893975924 learning rate =  0.0001\n",
      "Epoch =  3863 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893975826 learning rate =  0.0001\n",
      "Epoch =  3864 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397572 learning rate =  0.0001\n",
      "Epoch =  3865 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893975617 learning rate =  0.0001\n",
      "Epoch =  3866 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939755 learning rate =  0.0001\n",
      "Epoch =  3867 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893975395 learning rate =  0.0001\n",
      "Epoch =  3868 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939753 learning rate =  0.0001\n",
      "Epoch =  3869 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893975204 learning rate =  0.0001\n",
      "Epoch =  3870 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893975093 learning rate =  0.0001\n",
      "Epoch =  3871 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893974982 learning rate =  0.0001\n",
      "Epoch =  3872 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893974885 learning rate =  0.0001\n",
      "Epoch =  3873 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893974782 learning rate =  0.0001\n",
      "Epoch =  3874 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397469 learning rate =  0.0001\n",
      "Epoch =  3875 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397459 learning rate =  0.0001\n",
      "Epoch =  3876 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397448 learning rate =  0.0001\n",
      "Epoch =  3877 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893974387 learning rate =  0.0001\n",
      "Epoch =  3878 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893974294 learning rate =  0.0001\n",
      "Epoch =  3879 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397418 learning rate =  0.0001\n",
      "Epoch =  3880 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397408 learning rate =  0.0001\n",
      "Epoch =  3881 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893973987 learning rate =  0.0001\n",
      "Epoch =  3882 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939739 learning rate =  0.0001\n",
      "Epoch =  3883 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893973797 learning rate =  0.0001\n",
      "Epoch =  3884 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939737 learning rate =  0.0001\n",
      "Epoch =  3885 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397361 learning rate =  0.0001\n",
      "Epoch =  3886 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397352 learning rate =  0.0001\n",
      "Epoch =  3887 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397344 learning rate =  0.0001\n",
      "Epoch =  3888 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397334 learning rate =  0.0001\n",
      "Epoch =  3889 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893973224 learning rate =  0.0001\n",
      "Epoch =  3890 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893973144 learning rate =  0.0001\n",
      "Epoch =  3891 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893973046 learning rate =  0.0001\n",
      "Epoch =  3892 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893972966 learning rate =  0.0001\n",
      "Epoch =  3893 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893972886 learning rate =  0.0001\n",
      "Epoch =  3894 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397279 learning rate =  0.0001\n",
      "Epoch =  3895 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939727 learning rate =  0.0001\n",
      "Epoch =  3896 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397262 learning rate =  0.0001\n",
      "Epoch =  3897 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397254 learning rate =  0.0001\n",
      "Epoch =  3898 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397245 learning rate =  0.0001\n",
      "Epoch =  3899 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397236 learning rate =  0.0001\n",
      "Epoch =  3900 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893972273 learning rate =  0.0001\n",
      "Epoch =  3901 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893972207 learning rate =  0.0001\n",
      "Epoch =  3902 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397212 learning rate =  0.0001\n",
      "Epoch =  3903 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397204 learning rate =  0.0001\n",
      "Epoch =  3904 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893971954 learning rate =  0.0001\n",
      "Epoch =  3905 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397187 learning rate =  0.0001\n",
      "Epoch =  3906 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893971776 learning rate =  0.0001\n",
      "Epoch =  3907 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939717 learning rate =  0.0001\n",
      "Epoch =  3908 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397162 learning rate =  0.0001\n",
      "Epoch =  3909 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397155 learning rate =  0.0001\n",
      "Epoch =  3910 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893971465 learning rate =  0.0001\n",
      "Epoch =  3911 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893971385 learning rate =  0.0001\n",
      "Epoch =  3912 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397131 learning rate =  0.0001\n",
      "Epoch =  3913 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893971234 learning rate =  0.0001\n",
      "Epoch =  3914 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893971168 learning rate =  0.0001\n",
      "Epoch =  3915 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397108 learning rate =  0.0001\n",
      "Epoch =  3916 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893970994 learning rate =  0.0001\n",
      "Epoch =  3917 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893970937 learning rate =  0.0001\n",
      "Epoch =  3918 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397086 learning rate =  0.0001\n",
      "Epoch =  3919 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893970786 learning rate =  0.0001\n",
      "Epoch =  3920 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397072 learning rate =  0.0001\n",
      "Epoch =  3921 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893970652 learning rate =  0.0001\n",
      "Epoch =  3922 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893970577 learning rate =  0.0001\n",
      "Epoch =  3923 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397051 learning rate =  0.0001\n",
      "Epoch =  3924 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893970435 learning rate =  0.0001\n",
      "Epoch =  3925 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397037 learning rate =  0.0001\n",
      "Epoch =  3926 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939703 learning rate =  0.0001\n",
      "Epoch =  3927 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397023 learning rate =  0.0001\n",
      "Epoch =  3928 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893970164 learning rate =  0.0001\n",
      "Epoch =  3929 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893970093 learning rate =  0.0001\n",
      "Epoch =  3930 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589397004 learning rate =  0.0001\n",
      "Epoch =  3931 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893969977 learning rate =  0.0001\n",
      "Epoch =  3932 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396991 learning rate =  0.0001\n",
      "Epoch =  3933 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396984 learning rate =  0.0001\n",
      "Epoch =  3934 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893969786 learning rate =  0.0001\n",
      "Epoch =  3935 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893969698 learning rate =  0.0001\n",
      "Epoch =  3936 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396963 learning rate =  0.0001\n",
      "Epoch =  3937 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396957 learning rate =  0.0001\n",
      "Epoch =  3938 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893969498 learning rate =  0.0001\n",
      "Epoch =  3939 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396945 learning rate =  0.0001\n",
      "Epoch =  3940 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396938 learning rate =  0.0001\n",
      "Epoch =  3941 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893969307 learning rate =  0.0001\n",
      "Epoch =  3942 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893969245 learning rate =  0.0001\n",
      "Epoch =  3943 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893969187 learning rate =  0.0001\n",
      "Epoch =  3944 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396911 learning rate =  0.0001\n",
      "Epoch =  3945 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893969045 learning rate =  0.0001\n",
      "Epoch =  3946 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893969 learning rate =  0.0001\n",
      "Epoch =  3947 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396894 learning rate =  0.0001\n",
      "Epoch =  3948 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968876 learning rate =  0.0001\n",
      "Epoch =  3949 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968814 learning rate =  0.0001\n",
      "Epoch =  3950 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968756 learning rate =  0.0001\n",
      "Epoch =  3951 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968707 learning rate =  0.0001\n",
      "Epoch =  3952 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396865 learning rate =  0.0001\n",
      "Epoch =  3953 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396858 learning rate =  0.0001\n",
      "Epoch =  3954 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968525 learning rate =  0.0001\n",
      "Epoch =  3955 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968463 learning rate =  0.0001\n",
      "Epoch =  3956 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939684 learning rate =  0.0001\n",
      "Epoch =  3957 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968334 learning rate =  0.0001\n",
      "Epoch =  3958 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968285 learning rate =  0.0001\n",
      "Epoch =  3959 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396823 learning rate =  0.0001\n",
      "Epoch =  3960 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968183 learning rate =  0.0001\n",
      "Epoch =  3961 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968134 learning rate =  0.0001\n",
      "Epoch =  3962 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893968077 learning rate =  0.0001\n",
      "Epoch =  3963 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396803 learning rate =  0.0001\n",
      "Epoch =  3964 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396797 learning rate =  0.0001\n",
      "Epoch =  3965 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396793 learning rate =  0.0001\n",
      "Epoch =  3966 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396787 learning rate =  0.0001\n",
      "Epoch =  3967 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939678 learning rate =  0.0001\n",
      "Epoch =  3968 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967744 learning rate =  0.0001\n",
      "Epoch =  3969 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967704 learning rate =  0.0001\n",
      "Epoch =  3970 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967655 learning rate =  0.0001\n",
      "Epoch =  3971 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967615 learning rate =  0.0001\n",
      "Epoch =  3972 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396754 learning rate =  0.0001\n",
      "Epoch =  3973 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396749 learning rate =  0.0001\n",
      "Epoch =  3974 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967437 learning rate =  0.0001\n",
      "Epoch =  3975 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967393 learning rate =  0.0001\n",
      "Epoch =  3976 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967344 learning rate =  0.0001\n",
      "Epoch =  3977 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396728 learning rate =  0.0001\n",
      "Epoch =  3978 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967237 learning rate =  0.0001\n",
      "Epoch =  3979 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967184 learning rate =  0.0001\n",
      "Epoch =  3980 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967135 learning rate =  0.0001\n",
      "Epoch =  3981 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967086 learning rate =  0.0001\n",
      "Epoch =  3982 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967037 learning rate =  0.0001\n",
      "Epoch =  3983 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893967 learning rate =  0.0001\n",
      "Epoch =  3984 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966944 learning rate =  0.0001\n",
      "Epoch =  3985 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966904 learning rate =  0.0001\n",
      "Epoch =  3986 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396685 learning rate =  0.0001\n",
      "Epoch =  3987 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396682 learning rate =  0.0001\n",
      "Epoch =  3988 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396676 learning rate =  0.0001\n",
      "Epoch =  3989 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966727 learning rate =  0.0001\n",
      "Epoch =  3990 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396669 learning rate =  0.0001\n",
      "Epoch =  3991 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966642 learning rate =  0.0001\n",
      "Epoch =  3992 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939666 learning rate =  0.0001\n",
      "Epoch =  3993 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396655 learning rate =  0.0001\n",
      "Epoch =  3994 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966513 learning rate =  0.0001\n",
      "Epoch =  3995 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396647 learning rate =  0.0001\n",
      "Epoch =  3996 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396643 learning rate =  0.0001\n",
      "Epoch =  3997 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396639 learning rate =  0.0001\n",
      "Epoch =  3998 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966336 learning rate =  0.0001\n",
      "Epoch =  3999 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396629 learning rate =  0.0001\n",
      "Epoch =  4000 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396624 learning rate =  0.0001\n",
      "Epoch =  4001 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966207 learning rate =  0.0001\n",
      "Epoch =  4002 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966163 learning rate =  0.0001\n",
      "Epoch =  4003 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966123 learning rate =  0.0001\n",
      "Epoch =  4004 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396609 learning rate =  0.0001\n",
      "Epoch =  4005 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396604 learning rate =  0.0001\n",
      "Epoch =  4006 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893966003 learning rate =  0.0001\n",
      "Epoch =  4007 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396595 learning rate =  0.0001\n",
      "Epoch =  4008 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965905 learning rate =  0.0001\n",
      "Epoch =  4009 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396586 learning rate =  0.0001\n",
      "Epoch =  4010 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965834 learning rate =  0.0001\n",
      "Epoch =  4011 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396579 learning rate =  0.0001\n",
      "Epoch =  4012 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965745 learning rate =  0.0001\n",
      "Epoch =  4013 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396571 learning rate =  0.0001\n",
      "Epoch =  4014 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965665 learning rate =  0.0001\n",
      "Epoch =  4015 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396563 learning rate =  0.0001\n",
      "Epoch =  4016 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396559 learning rate =  0.0001\n",
      "Epoch =  4017 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396556 learning rate =  0.0001\n",
      "Epoch =  4018 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965514 learning rate =  0.0001\n",
      "Epoch =  4019 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965474 learning rate =  0.0001\n",
      "Epoch =  4020 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396542 learning rate =  0.0001\n",
      "Epoch =  4021 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965385 learning rate =  0.0001\n",
      "Epoch =  4022 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396534 learning rate =  0.0001\n",
      "Epoch =  4023 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965314 learning rate =  0.0001\n",
      "Epoch =  4024 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965274 learning rate =  0.0001\n",
      "Epoch =  4025 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396524 learning rate =  0.0001\n",
      "Epoch =  4026 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965203 learning rate =  0.0001\n",
      "Epoch =  4027 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396517 learning rate =  0.0001\n",
      "Epoch =  4028 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396513 learning rate =  0.0001\n",
      "Epoch =  4029 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965083 learning rate =  0.0001\n",
      "Epoch =  4030 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965052 learning rate =  0.0001\n",
      "Epoch =  4031 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893965 learning rate =  0.0001\n",
      "Epoch =  4032 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964977 learning rate =  0.0001\n",
      "Epoch =  4033 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964955 learning rate =  0.0001\n",
      "Epoch =  4034 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396491 learning rate =  0.0001\n",
      "Epoch =  4035 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396488 learning rate =  0.0001\n",
      "Epoch =  4036 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964844 learning rate =  0.0001\n",
      "Epoch =  4037 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396482 learning rate =  0.0001\n",
      "Epoch =  4038 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964773 learning rate =  0.0001\n",
      "Epoch =  4039 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964737 learning rate =  0.0001\n",
      "Epoch =  4040 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964706 learning rate =  0.0001\n",
      "Epoch =  4041 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964675 learning rate =  0.0001\n",
      "Epoch =  4042 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396464 learning rate =  0.0001\n",
      "Epoch =  4043 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396461 learning rate =  0.0001\n",
      "Epoch =  4044 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964577 learning rate =  0.0001\n",
      "Epoch =  4045 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396451 learning rate =  0.0001\n",
      "Epoch =  4046 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964493 learning rate =  0.0001\n",
      "Epoch =  4047 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396446 learning rate =  0.0001\n",
      "Epoch =  4048 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396443 learning rate =  0.0001\n",
      "Epoch =  4049 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396441 learning rate =  0.0001\n",
      "Epoch =  4050 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964377 learning rate =  0.0001\n",
      "Epoch =  4051 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964333 learning rate =  0.0001\n",
      "Epoch =  4052 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396431 learning rate =  0.0001\n",
      "Epoch =  4053 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964275 learning rate =  0.0001\n",
      "Epoch =  4054 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396424 learning rate =  0.0001\n",
      "Epoch =  4055 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396421 learning rate =  0.0001\n",
      "Epoch =  4056 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396417 learning rate =  0.0001\n",
      "Epoch =  4057 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964138 learning rate =  0.0001\n",
      "Epoch =  4058 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964115 learning rate =  0.0001\n",
      "Epoch =  4059 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964075 learning rate =  0.0001\n",
      "Epoch =  4060 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893964035 learning rate =  0.0001\n",
      "Epoch =  4061 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396399 learning rate =  0.0001\n",
      "Epoch =  4062 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396397 learning rate =  0.0001\n",
      "Epoch =  4063 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396395 learning rate =  0.0001\n",
      "Epoch =  4064 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396391 learning rate =  0.0001\n",
      "Epoch =  4065 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396389 learning rate =  0.0001\n",
      "Epoch =  4066 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963867 learning rate =  0.0001\n",
      "Epoch =  4067 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963827 learning rate =  0.0001\n",
      "Epoch =  4068 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963796 learning rate =  0.0001\n",
      "Epoch =  4069 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963756 learning rate =  0.0001\n",
      "Epoch =  4070 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396374 learning rate =  0.0001\n",
      "Epoch =  4071 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963702 learning rate =  0.0001\n",
      "Epoch =  4072 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396367 learning rate =  0.0001\n",
      "Epoch =  4073 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396365 learning rate =  0.0001\n",
      "Epoch =  4074 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963614 learning rate =  0.0001\n",
      "Epoch =  4075 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963582 learning rate =  0.0001\n",
      "Epoch =  4076 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396356 learning rate =  0.0001\n",
      "Epoch =  4077 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963525 learning rate =  0.0001\n",
      "Epoch =  4078 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963494 learning rate =  0.0001\n",
      "Epoch =  4079 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396347 learning rate =  0.0001\n",
      "Epoch =  4080 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963436 learning rate =  0.0001\n",
      "Epoch =  4081 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963414 learning rate =  0.0001\n",
      "Epoch =  4082 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963387 learning rate =  0.0001\n",
      "Epoch =  4083 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963356 learning rate =  0.0001\n",
      "Epoch =  4084 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396333 learning rate =  0.0001\n",
      "Epoch =  4085 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396331 learning rate =  0.0001\n",
      "Epoch =  4086 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396327 learning rate =  0.0001\n",
      "Epoch =  4087 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963245 learning rate =  0.0001\n",
      "Epoch =  4088 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963236 learning rate =  0.0001\n",
      "Epoch =  4089 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963196 learning rate =  0.0001\n",
      "Epoch =  4090 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396317 learning rate =  0.0001\n",
      "Epoch =  4091 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963147 learning rate =  0.0001\n",
      "Epoch =  4092 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396312 learning rate =  0.0001\n",
      "Epoch =  4093 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963076 learning rate =  0.0001\n",
      "Epoch =  4094 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963054 learning rate =  0.0001\n",
      "Epoch =  4095 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963027 learning rate =  0.0001\n",
      "Epoch =  4096 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893963 learning rate =  0.0001\n",
      "Epoch =  4097 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396298 learning rate =  0.0001\n",
      "Epoch =  4098 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962956 learning rate =  0.0001\n",
      "Epoch =  4099 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396293 learning rate =  0.0001\n",
      "Epoch =  4100 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396291 learning rate =  0.0001\n",
      "Epoch =  4101 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962894 learning rate =  0.0001\n",
      "Epoch =  4102 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962863 learning rate =  0.0001\n",
      "Epoch =  4103 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962823 learning rate =  0.0001\n",
      "Epoch =  4104 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962805 learning rate =  0.0001\n",
      "Epoch =  4105 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962774 learning rate =  0.0001\n",
      "Epoch =  4106 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962756 learning rate =  0.0001\n",
      "Epoch =  4107 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396274 learning rate =  0.0001\n",
      "Epoch =  4108 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396272 learning rate =  0.0001\n",
      "Epoch =  4109 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396268 learning rate =  0.0001\n",
      "Epoch =  4110 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962663 learning rate =  0.0001\n",
      "Epoch =  4111 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962637 learning rate =  0.0001\n",
      "Epoch =  4112 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396262 learning rate =  0.0001\n",
      "Epoch =  4113 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962588 learning rate =  0.0001\n",
      "Epoch =  4114 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962565 learning rate =  0.0001\n",
      "Epoch =  4115 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962548 learning rate =  0.0001\n",
      "Epoch =  4116 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962526 learning rate =  0.0001\n",
      "Epoch =  4117 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939625 learning rate =  0.0001\n",
      "Epoch =  4118 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396248 learning rate =  0.0001\n",
      "Epoch =  4119 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962454 learning rate =  0.0001\n",
      "Epoch =  4120 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962446 learning rate =  0.0001\n",
      "Epoch =  4121 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396242 learning rate =  0.0001\n",
      "Epoch =  4122 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939624 learning rate =  0.0001\n",
      "Epoch =  4123 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962383 learning rate =  0.0001\n",
      "Epoch =  4124 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962366 learning rate =  0.0001\n",
      "Epoch =  4125 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396235 learning rate =  0.0001\n",
      "Epoch =  4126 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962312 learning rate =  0.0001\n",
      "Epoch =  4127 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962286 learning rate =  0.0001\n",
      "Epoch =  4128 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962277 learning rate =  0.0001\n",
      "Epoch =  4129 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396226 learning rate =  0.0001\n",
      "Epoch =  4130 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962232 learning rate =  0.0001\n",
      "Epoch =  4131 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396221 learning rate =  0.0001\n",
      "Epoch =  4132 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962192 learning rate =  0.0001\n",
      "Epoch =  4133 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962175 learning rate =  0.0001\n",
      "Epoch =  4134 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396215 learning rate =  0.0001\n",
      "Epoch =  4135 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396213 learning rate =  0.0001\n",
      "Epoch =  4136 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962113 learning rate =  0.0001\n",
      "Epoch =  4137 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962077 learning rate =  0.0001\n",
      "Epoch =  4138 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396206 learning rate =  0.0001\n",
      "Epoch =  4139 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396204 learning rate =  0.0001\n",
      "Epoch =  4140 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962024 learning rate =  0.0001\n",
      "Epoch =  4141 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893962 learning rate =  0.0001\n",
      "Epoch =  4142 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961993 learning rate =  0.0001\n",
      "Epoch =  4143 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396197 learning rate =  0.0001\n",
      "Epoch =  4144 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396196 learning rate =  0.0001\n",
      "Epoch =  4145 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396194 learning rate =  0.0001\n",
      "Epoch =  4146 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396192 learning rate =  0.0001\n",
      "Epoch =  4147 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961904 learning rate =  0.0001\n",
      "Epoch =  4148 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961886 learning rate =  0.0001\n",
      "Epoch =  4149 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961864 learning rate =  0.0001\n",
      "Epoch =  4150 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961837 learning rate =  0.0001\n",
      "Epoch =  4151 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961815 learning rate =  0.0001\n",
      "Epoch =  4152 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961797 learning rate =  0.0001\n",
      "Epoch =  4153 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396178 learning rate =  0.0001\n",
      "Epoch =  4154 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396176 learning rate =  0.0001\n",
      "Epoch =  4155 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961744 learning rate =  0.0001\n",
      "Epoch =  4156 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961726 learning rate =  0.0001\n",
      "Epoch =  4157 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961704 learning rate =  0.0001\n",
      "Epoch =  4158 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396169 learning rate =  0.0001\n",
      "Epoch =  4159 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961673 learning rate =  0.0001\n",
      "Epoch =  4160 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396166 learning rate =  0.0001\n",
      "Epoch =  4161 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961624 learning rate =  0.0001\n",
      "Epoch =  4162 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396162 learning rate =  0.0001\n",
      "Epoch =  4163 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939616 learning rate =  0.0001\n",
      "Epoch =  4164 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396159 learning rate =  0.0001\n",
      "Epoch =  4165 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961584 learning rate =  0.0001\n",
      "Epoch =  4166 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396155 learning rate =  0.0001\n",
      "Epoch =  4167 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961544 learning rate =  0.0001\n",
      "Epoch =  4168 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396154 learning rate =  0.0001\n",
      "Epoch =  4169 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396153 learning rate =  0.0001\n",
      "Epoch =  4170 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961517 learning rate =  0.0001\n",
      "Epoch =  4171 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961495 learning rate =  0.0001\n",
      "Epoch =  4172 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961486 learning rate =  0.0001\n",
      "Epoch =  4173 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396147 learning rate =  0.0001\n",
      "Epoch =  4174 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961455 learning rate =  0.0001\n",
      "Epoch =  4175 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396145 learning rate =  0.0001\n",
      "Epoch =  4176 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961424 learning rate =  0.0001\n",
      "Epoch =  4177 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961406 learning rate =  0.0001\n",
      "Epoch =  4178 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939614 learning rate =  0.0001\n",
      "Epoch =  4179 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396139 learning rate =  0.0001\n",
      "Epoch =  4180 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396137 learning rate =  0.0001\n",
      "Epoch =  4181 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396135 learning rate =  0.0001\n",
      "Epoch =  4182 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396133 learning rate =  0.0001\n",
      "Epoch =  4183 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961326 learning rate =  0.0001\n",
      "Epoch =  4184 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396131 learning rate =  0.0001\n",
      "Epoch =  4185 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939613 learning rate =  0.0001\n",
      "Epoch =  4186 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396128 learning rate =  0.0001\n",
      "Epoch =  4187 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961273 learning rate =  0.0001\n",
      "Epoch =  4188 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961273 learning rate =  0.0001\n",
      "Epoch =  4189 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961255 learning rate =  0.0001\n",
      "Epoch =  4190 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961255 learning rate =  0.0001\n",
      "Epoch =  4191 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961238 learning rate =  0.0001\n",
      "Epoch =  4192 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961224 learning rate =  0.0001\n",
      "Epoch =  4193 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396123 learning rate =  0.0001\n",
      "Epoch =  4194 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961207 learning rate =  0.0001\n",
      "Epoch =  4195 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939612 learning rate =  0.0001\n",
      "Epoch =  4196 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396119 learning rate =  0.0001\n",
      "Epoch =  4197 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961184 learning rate =  0.0001\n",
      "Epoch =  4198 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961184 learning rate =  0.0001\n",
      "Epoch =  4199 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961175 learning rate =  0.0001\n",
      "Epoch =  4200 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961158 learning rate =  0.0001\n",
      "Epoch =  4201 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961144 learning rate =  0.0001\n",
      "Epoch =  4202 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396113 learning rate =  0.0001\n",
      "Epoch =  4203 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396112 learning rate =  0.0001\n",
      "Epoch =  4204 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961118 learning rate =  0.0001\n",
      "Epoch =  4205 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939611 learning rate =  0.0001\n",
      "Epoch =  4206 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396108 learning rate =  0.0001\n",
      "Epoch =  4207 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961073 learning rate =  0.0001\n",
      "Epoch =  4208 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396107 learning rate =  0.0001\n",
      "Epoch =  4209 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961064 learning rate =  0.0001\n",
      "Epoch =  4210 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396105 learning rate =  0.0001\n",
      "Epoch =  4211 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961047 learning rate =  0.0001\n",
      "Epoch =  4212 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396103 learning rate =  0.0001\n",
      "Epoch =  4213 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961016 learning rate =  0.0001\n",
      "Epoch =  4214 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893961002 learning rate =  0.0001\n",
      "Epoch =  4215 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396099 learning rate =  0.0001\n",
      "Epoch =  4216 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960976 learning rate =  0.0001\n",
      "Epoch =  4217 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396097 learning rate =  0.0001\n",
      "Epoch =  4218 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960967 learning rate =  0.0001\n",
      "Epoch =  4219 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960962 learning rate =  0.0001\n",
      "Epoch =  4220 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396094 learning rate =  0.0001\n",
      "Epoch =  4221 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396094 learning rate =  0.0001\n",
      "Epoch =  4222 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960927 learning rate =  0.0001\n",
      "Epoch =  4223 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960922 learning rate =  0.0001\n",
      "Epoch =  4224 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960922 learning rate =  0.0001\n",
      "Epoch =  4225 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960913 learning rate =  0.0001\n",
      "Epoch =  4226 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960905 learning rate =  0.0001\n",
      "Epoch =  4227 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960896 learning rate =  0.0001\n",
      "Epoch =  4228 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960887 learning rate =  0.0001\n",
      "Epoch =  4229 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960896 learning rate =  0.0001\n",
      "Epoch =  4230 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396088 learning rate =  0.0001\n",
      "Epoch =  4231 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960874 learning rate =  0.0001\n",
      "Epoch =  4232 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396087 learning rate =  0.0001\n",
      "Epoch =  4233 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960847 learning rate =  0.0001\n",
      "Epoch =  4234 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960842 learning rate =  0.0001\n",
      "Epoch =  4235 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396083 learning rate =  0.0001\n",
      "Epoch =  4236 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960825 learning rate =  0.0001\n",
      "Epoch =  4237 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396081 learning rate =  0.0001\n",
      "Epoch =  4238 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960785 learning rate =  0.0001\n",
      "Epoch =  4239 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396078 learning rate =  0.0001\n",
      "Epoch =  4240 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960776 learning rate =  0.0001\n",
      "Epoch =  4241 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960762 learning rate =  0.0001\n",
      "Epoch =  4242 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396076 learning rate =  0.0001\n",
      "Epoch =  4243 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960736 learning rate =  0.0001\n",
      "Epoch =  4244 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396073 learning rate =  0.0001\n",
      "Epoch =  4245 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396072 learning rate =  0.0001\n",
      "Epoch =  4246 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960705 learning rate =  0.0001\n",
      "Epoch =  4247 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396069 learning rate =  0.0001\n",
      "Epoch =  4248 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960687 learning rate =  0.0001\n",
      "Epoch =  4249 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960674 learning rate =  0.0001\n",
      "Epoch =  4250 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396067 learning rate =  0.0001\n",
      "Epoch =  4251 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396066 learning rate =  0.0001\n",
      "Epoch =  4252 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960656 learning rate =  0.0001\n",
      "Epoch =  4253 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396065 learning rate =  0.0001\n",
      "Epoch =  4254 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960643 learning rate =  0.0001\n",
      "Epoch =  4255 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396063 learning rate =  0.0001\n",
      "Epoch =  4256 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396061 learning rate =  0.0001\n",
      "Epoch =  4257 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960607 learning rate =  0.0001\n",
      "Epoch =  4258 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960603 learning rate =  0.0001\n",
      "Epoch =  4259 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960594 learning rate =  0.0001\n",
      "Epoch =  4260 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939606 learning rate =  0.0001\n",
      "Epoch =  4261 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960585 learning rate =  0.0001\n",
      "Epoch =  4262 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396058 learning rate =  0.0001\n",
      "Epoch =  4263 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960576 learning rate =  0.0001\n",
      "Epoch =  4264 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396056 learning rate =  0.0001\n",
      "Epoch =  4265 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960554 learning rate =  0.0001\n",
      "Epoch =  4266 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396055 learning rate =  0.0001\n",
      "Epoch =  4267 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960545 learning rate =  0.0001\n",
      "Epoch =  4268 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960545 learning rate =  0.0001\n",
      "Epoch =  4269 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396054 learning rate =  0.0001\n",
      "Epoch =  4270 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396052 learning rate =  0.0001\n",
      "Epoch =  4271 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960505 learning rate =  0.0001\n",
      "Epoch =  4272 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960505 learning rate =  0.0001\n",
      "Epoch =  4273 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939605 learning rate =  0.0001\n",
      "Epoch =  4274 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960496 learning rate =  0.0001\n",
      "Epoch =  4275 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396049 learning rate =  0.0001\n",
      "Epoch =  4276 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396048 learning rate =  0.0001\n",
      "Epoch =  4277 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396048 learning rate =  0.0001\n",
      "Epoch =  4278 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396045 learning rate =  0.0001\n",
      "Epoch =  4279 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960447 learning rate =  0.0001\n",
      "Epoch =  4280 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960443 learning rate =  0.0001\n",
      "Epoch =  4281 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960434 learning rate =  0.0001\n",
      "Epoch =  4282 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396042 learning rate =  0.0001\n",
      "Epoch =  4283 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960407 learning rate =  0.0001\n",
      "Epoch =  4284 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960403 learning rate =  0.0001\n",
      "Epoch =  4285 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396039 learning rate =  0.0001\n",
      "Epoch =  4286 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960385 learning rate =  0.0001\n",
      "Epoch =  4287 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396038 learning rate =  0.0001\n",
      "Epoch =  4288 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960367 learning rate =  0.0001\n",
      "Epoch =  4289 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960363 learning rate =  0.0001\n",
      "Epoch =  4290 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960363 learning rate =  0.0001\n",
      "Epoch =  4291 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396035 learning rate =  0.0001\n",
      "Epoch =  4292 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960345 learning rate =  0.0001\n",
      "Epoch =  4293 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960336 learning rate =  0.0001\n",
      "Epoch =  4294 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960323 learning rate =  0.0001\n",
      "Epoch =  4295 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960323 learning rate =  0.0001\n",
      "Epoch =  4296 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396032 learning rate =  0.0001\n",
      "Epoch =  4297 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396032 learning rate =  0.0001\n",
      "Epoch =  4298 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960314 learning rate =  0.0001\n",
      "Epoch =  4299 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939603 learning rate =  0.0001\n",
      "Epoch =  4300 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960287 learning rate =  0.0001\n",
      "Epoch =  4301 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960287 learning rate =  0.0001\n",
      "Epoch =  4302 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960283 learning rate =  0.0001\n",
      "Epoch =  4303 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960283 learning rate =  0.0001\n",
      "Epoch =  4304 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396028 learning rate =  0.0001\n",
      "Epoch =  4305 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960265 learning rate =  0.0001\n",
      "Epoch =  4306 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960256 learning rate =  0.0001\n",
      "Epoch =  4307 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396025 learning rate =  0.0001\n",
      "Epoch =  4308 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960243 learning rate =  0.0001\n",
      "Epoch =  4309 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396024 learning rate =  0.0001\n",
      "Epoch =  4310 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396024 learning rate =  0.0001\n",
      "Epoch =  4311 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960234 learning rate =  0.0001\n",
      "Epoch =  4312 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960234 learning rate =  0.0001\n",
      "Epoch =  4313 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960225 learning rate =  0.0001\n",
      "Epoch =  4314 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396021 learning rate =  0.0001\n",
      "Epoch =  4315 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396022 learning rate =  0.0001\n",
      "Epoch =  4316 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960207 learning rate =  0.0001\n",
      "Epoch =  4317 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960203 learning rate =  0.0001\n",
      "Epoch =  4318 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960203 learning rate =  0.0001\n",
      "Epoch =  4319 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939602 learning rate =  0.0001\n",
      "Epoch =  4320 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939602 learning rate =  0.0001\n",
      "Epoch =  4321 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960176 learning rate =  0.0001\n",
      "Epoch =  4322 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960167 learning rate =  0.0001\n",
      "Epoch =  4323 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396016 learning rate =  0.0001\n",
      "Epoch =  4324 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396016 learning rate =  0.0001\n",
      "Epoch =  4325 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960145 learning rate =  0.0001\n",
      "Epoch =  4326 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396014 learning rate =  0.0001\n",
      "Epoch =  4327 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396013 learning rate =  0.0001\n",
      "Epoch =  4328 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396012 learning rate =  0.0001\n",
      "Epoch =  4329 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960114 learning rate =  0.0001\n",
      "Epoch =  4330 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960114 learning rate =  0.0001\n",
      "Epoch =  4331 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960114 learning rate =  0.0001\n",
      "Epoch =  4332 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396011 learning rate =  0.0001\n",
      "Epoch =  4333 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960096 learning rate =  0.0001\n",
      "Epoch =  4334 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960087 learning rate =  0.0001\n",
      "Epoch =  4335 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396008 learning rate =  0.0001\n",
      "Epoch =  4336 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396008 learning rate =  0.0001\n",
      "Epoch =  4337 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960074 learning rate =  0.0001\n",
      "Epoch =  4338 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960074 learning rate =  0.0001\n",
      "Epoch =  4339 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960065 learning rate =  0.0001\n",
      "Epoch =  4340 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960048 learning rate =  0.0001\n",
      "Epoch =  4341 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960048 learning rate =  0.0001\n",
      "Epoch =  4342 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960043 learning rate =  0.0001\n",
      "Epoch =  4343 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396004 learning rate =  0.0001\n",
      "Epoch =  4344 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396003 learning rate =  0.0001\n",
      "Epoch =  4345 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960025 learning rate =  0.0001\n",
      "Epoch =  4346 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893960025 learning rate =  0.0001\n",
      "Epoch =  4347 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396003 learning rate =  0.0001\n",
      "Epoch =  4348 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396002 learning rate =  0.0001\n",
      "Epoch =  4349 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396001 learning rate =  0.0001\n",
      "Epoch =  4350 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396001 learning rate =  0.0001\n",
      "Epoch =  4351 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396 learning rate =  0.0001\n",
      "Epoch =  4352 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589396 learning rate =  0.0001\n",
      "Epoch =  4353 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395999 learning rate =  0.0001\n",
      "Epoch =  4354 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395999 learning rate =  0.0001\n",
      "Epoch =  4355 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395999 learning rate =  0.0001\n",
      "Epoch =  4356 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395999 learning rate =  0.0001\n",
      "Epoch =  4357 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959985 learning rate =  0.0001\n",
      "Epoch =  4358 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395998 learning rate =  0.0001\n",
      "Epoch =  4359 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395998 learning rate =  0.0001\n",
      "Epoch =  4360 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395998 learning rate =  0.0001\n",
      "Epoch =  4361 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395998 learning rate =  0.0001\n",
      "Epoch =  4362 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959976 learning rate =  0.0001\n",
      "Epoch =  4363 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959976 learning rate =  0.0001\n",
      "Epoch =  4364 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959968 learning rate =  0.0001\n",
      "Epoch =  4365 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959963 learning rate =  0.0001\n",
      "Epoch =  4366 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959963 learning rate =  0.0001\n",
      "Epoch =  4367 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959963 learning rate =  0.0001\n",
      "Epoch =  4368 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395996 learning rate =  0.0001\n",
      "Epoch =  4369 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395996 learning rate =  0.0001\n",
      "Epoch =  4370 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959945 learning rate =  0.0001\n",
      "Epoch =  4371 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959945 learning rate =  0.0001\n",
      "Epoch =  4372 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959945 learning rate =  0.0001\n",
      "Epoch =  4373 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959923 learning rate =  0.0001\n",
      "Epoch =  4374 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395992 learning rate =  0.0001\n",
      "Epoch =  4375 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395992 learning rate =  0.0001\n",
      "Epoch =  4376 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959914 learning rate =  0.0001\n",
      "Epoch =  4377 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959914 learning rate =  0.0001\n",
      "Epoch =  4378 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939599 learning rate =  0.0001\n",
      "Epoch =  4379 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939599 learning rate =  0.0001\n",
      "Epoch =  4380 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959888 learning rate =  0.0001\n",
      "Epoch =  4381 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959883 learning rate =  0.0001\n",
      "Epoch =  4382 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959874 learning rate =  0.0001\n",
      "Epoch =  4383 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959874 learning rate =  0.0001\n",
      "Epoch =  4384 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395987 learning rate =  0.0001\n",
      "Epoch =  4385 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959857 learning rate =  0.0001\n",
      "Epoch =  4386 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959857 learning rate =  0.0001\n",
      "Epoch =  4387 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395985 learning rate =  0.0001\n",
      "Epoch =  4388 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959848 learning rate =  0.0001\n",
      "Epoch =  4389 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395984 learning rate =  0.0001\n",
      "Epoch =  4390 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395982 learning rate =  0.0001\n",
      "Epoch =  4391 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959817 learning rate =  0.0001\n",
      "Epoch =  4392 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395981 learning rate =  0.0001\n",
      "Epoch =  4393 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395981 learning rate =  0.0001\n",
      "Epoch =  4394 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939598 learning rate =  0.0001\n",
      "Epoch =  4395 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939598 learning rate =  0.0001\n",
      "Epoch =  4396 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959785 learning rate =  0.0001\n",
      "Epoch =  4397 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959785 learning rate =  0.0001\n",
      "Epoch =  4398 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959785 learning rate =  0.0001\n",
      "Epoch =  4399 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959785 learning rate =  0.0001\n",
      "Epoch =  4400 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395978 learning rate =  0.0001\n",
      "Epoch =  4401 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959777 learning rate =  0.0001\n",
      "Epoch =  4402 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959777 learning rate =  0.0001\n",
      "Epoch =  4403 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959754 learning rate =  0.0001\n",
      "Epoch =  4404 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959754 learning rate =  0.0001\n",
      "Epoch =  4405 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395975 learning rate =  0.0001\n",
      "Epoch =  4406 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395975 learning rate =  0.0001\n",
      "Epoch =  4407 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395974 learning rate =  0.0001\n",
      "Epoch =  4408 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395974 learning rate =  0.0001\n",
      "Epoch =  4409 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959737 learning rate =  0.0001\n",
      "Epoch =  4410 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959737 learning rate =  0.0001\n",
      "Epoch =  4411 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395973 learning rate =  0.0001\n",
      "Epoch =  4412 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395973 learning rate =  0.0001\n",
      "Epoch =  4413 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395973 learning rate =  0.0001\n",
      "Epoch =  4414 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395973 learning rate =  0.0001\n",
      "Epoch =  4415 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959728 learning rate =  0.0001\n",
      "Epoch =  4416 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959728 learning rate =  0.0001\n",
      "Epoch =  4417 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959723 learning rate =  0.0001\n",
      "Epoch =  4418 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959714 learning rate =  0.0001\n",
      "Epoch =  4419 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959723 learning rate =  0.0001\n",
      "Epoch =  4420 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959723 learning rate =  0.0001\n",
      "Epoch =  4421 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959723 learning rate =  0.0001\n",
      "Epoch =  4422 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395972 learning rate =  0.0001\n",
      "Epoch =  4423 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395972 learning rate =  0.0001\n",
      "Epoch =  4424 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395972 learning rate =  0.0001\n",
      "Epoch =  4425 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395972 learning rate =  0.0001\n",
      "Epoch =  4426 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959714 learning rate =  0.0001\n",
      "Epoch =  4427 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959714 learning rate =  0.0001\n",
      "Epoch =  4428 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959714 learning rate =  0.0001\n",
      "Epoch =  4429 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959714 learning rate =  0.0001\n",
      "Epoch =  4430 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959714 learning rate =  0.0001\n",
      "Epoch =  4431 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959714 learning rate =  0.0001\n",
      "Epoch =  4432 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395971 learning rate =  0.0001\n",
      "Epoch =  4433 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395971 learning rate =  0.0001\n",
      "Epoch =  4434 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395971 learning rate =  0.0001\n",
      "Epoch =  4435 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959706 learning rate =  0.0001\n",
      "Epoch =  4436 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959706 learning rate =  0.0001\n",
      "Epoch =  4437 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959706 learning rate =  0.0001\n",
      "Epoch =  4438 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959706 learning rate =  0.0001\n",
      "Epoch =  4439 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959706 learning rate =  0.0001\n",
      "Epoch =  4440 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939597 learning rate =  0.0001\n",
      "Epoch =  4441 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939597 learning rate =  0.0001\n",
      "Epoch =  4442 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959692 learning rate =  0.0001\n",
      "Epoch =  4443 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395969 learning rate =  0.0001\n",
      "Epoch =  4444 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395968 learning rate =  0.0001\n",
      "Epoch =  4445 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395968 learning rate =  0.0001\n",
      "Epoch =  4446 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395968 learning rate =  0.0001\n",
      "Epoch =  4447 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959674 learning rate =  0.0001\n",
      "Epoch =  4448 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395967 learning rate =  0.0001\n",
      "Epoch =  4449 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959652 learning rate =  0.0001\n",
      "Epoch =  4450 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959652 learning rate =  0.0001\n",
      "Epoch =  4451 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959652 learning rate =  0.0001\n",
      "Epoch =  4452 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959652 learning rate =  0.0001\n",
      "Epoch =  4453 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959652 learning rate =  0.0001\n",
      "Epoch =  4454 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959652 learning rate =  0.0001\n",
      "Epoch =  4455 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395965 learning rate =  0.0001\n",
      "Epoch =  4456 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395965 learning rate =  0.0001\n",
      "Epoch =  4457 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395965 learning rate =  0.0001\n",
      "Epoch =  4458 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959643 learning rate =  0.0001\n",
      "Epoch =  4459 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959643 learning rate =  0.0001\n",
      "Epoch =  4460 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959635 learning rate =  0.0001\n",
      "Epoch =  4461 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959635 learning rate =  0.0001\n",
      "Epoch =  4462 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395963 learning rate =  0.0001\n",
      "Epoch =  4463 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395962 learning rate =  0.0001\n",
      "Epoch =  4464 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959617 learning rate =  0.0001\n",
      "Epoch =  4465 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959617 learning rate =  0.0001\n",
      "Epoch =  4466 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959617 learning rate =  0.0001\n",
      "Epoch =  4467 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959612 learning rate =  0.0001\n",
      "Epoch =  4468 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395961 learning rate =  0.0001\n",
      "Epoch =  4469 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395961 learning rate =  0.0001\n",
      "Epoch =  4470 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395961 learning rate =  0.0001\n",
      "Epoch =  4471 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939596 learning rate =  0.0001\n",
      "Epoch =  4472 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959595 learning rate =  0.0001\n",
      "Epoch =  4473 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959586 learning rate =  0.0001\n",
      "Epoch =  4474 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959586 learning rate =  0.0001\n",
      "Epoch =  4475 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395958 learning rate =  0.0001\n",
      "Epoch =  4476 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395956 learning rate =  0.0001\n",
      "Epoch =  4477 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395956 learning rate =  0.0001\n",
      "Epoch =  4478 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959555 learning rate =  0.0001\n",
      "Epoch =  4479 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959555 learning rate =  0.0001\n",
      "Epoch =  4480 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959555 learning rate =  0.0001\n",
      "Epoch =  4481 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959546 learning rate =  0.0001\n",
      "Epoch =  4482 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959555 learning rate =  0.0001\n",
      "Epoch =  4483 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959555 learning rate =  0.0001\n",
      "Epoch =  4484 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959546 learning rate =  0.0001\n",
      "Epoch =  4485 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395954 learning rate =  0.0001\n",
      "Epoch =  4486 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395954 learning rate =  0.0001\n",
      "Epoch =  4487 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959532 learning rate =  0.0001\n",
      "Epoch =  4488 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395953 learning rate =  0.0001\n",
      "Epoch =  4489 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395953 learning rate =  0.0001\n",
      "Epoch =  4490 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395953 learning rate =  0.0001\n",
      "Epoch =  4491 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4492 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4493 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4494 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4495 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395952 learning rate =  0.0001\n",
      "Epoch =  4496 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395952 learning rate =  0.0001\n",
      "Epoch =  4497 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395952 learning rate =  0.0001\n",
      "Epoch =  4498 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395952 learning rate =  0.0001\n",
      "Epoch =  4499 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395952 learning rate =  0.0001\n",
      "Epoch =  4500 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4501 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4502 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4503 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4504 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4505 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959506 learning rate =  0.0001\n",
      "Epoch =  4506 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4507 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4508 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959506 learning rate =  0.0001\n",
      "Epoch =  4509 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959506 learning rate =  0.0001\n",
      "Epoch =  4510 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959506 learning rate =  0.0001\n",
      "Epoch =  4511 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959506 learning rate =  0.0001\n",
      "Epoch =  4512 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4513 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4514 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4515 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4516 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4517 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4518 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4519 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4520 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4521 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4522 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4523 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4524 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4525 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4526 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959523 learning rate =  0.0001\n",
      "Epoch =  4527 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4528 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4529 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4530 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4531 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959515 learning rate =  0.0001\n",
      "Epoch =  4532 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395951 learning rate =  0.0001\n",
      "Epoch =  4533 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959506 learning rate =  0.0001\n",
      "Epoch =  4534 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959506 learning rate =  0.0001\n",
      "Epoch =  4535 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395951 learning rate =  0.0001\n",
      "Epoch =  4536 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959506 learning rate =  0.0001\n",
      "Epoch =  4537 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959497 learning rate =  0.0001\n",
      "Epoch =  4538 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395949 learning rate =  0.0001\n",
      "Epoch =  4539 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959497 learning rate =  0.0001\n",
      "Epoch =  4540 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939595 learning rate =  0.0001\n",
      "Epoch =  4541 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959492 learning rate =  0.0001\n",
      "Epoch =  4542 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395949 learning rate =  0.0001\n",
      "Epoch =  4543 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395949 learning rate =  0.0001\n",
      "Epoch =  4544 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395949 learning rate =  0.0001\n",
      "Epoch =  4545 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959484 learning rate =  0.0001\n",
      "Epoch =  4546 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959484 learning rate =  0.0001\n",
      "Epoch =  4547 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959484 learning rate =  0.0001\n",
      "Epoch =  4548 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395948 learning rate =  0.0001\n",
      "Epoch =  4549 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395948 learning rate =  0.0001\n",
      "Epoch =  4550 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395948 learning rate =  0.0001\n",
      "Epoch =  4551 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395948 learning rate =  0.0001\n",
      "Epoch =  4552 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395947 learning rate =  0.0001\n",
      "Epoch =  4553 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959466 learning rate =  0.0001\n",
      "Epoch =  4554 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959466 learning rate =  0.0001\n",
      "Epoch =  4555 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959466 learning rate =  0.0001\n",
      "Epoch =  4556 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959466 learning rate =  0.0001\n",
      "Epoch =  4557 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959466 learning rate =  0.0001\n",
      "Epoch =  4558 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959466 learning rate =  0.0001\n",
      "Epoch =  4559 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959457 learning rate =  0.0001\n",
      "Epoch =  4560 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959457 learning rate =  0.0001\n",
      "Epoch =  4561 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959457 learning rate =  0.0001\n",
      "Epoch =  4562 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959457 learning rate =  0.0001\n",
      "Epoch =  4563 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959457 learning rate =  0.0001\n",
      "Epoch =  4564 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959457 learning rate =  0.0001\n",
      "Epoch =  4565 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959457 learning rate =  0.0001\n",
      "Epoch =  4566 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959457 learning rate =  0.0001\n",
      "Epoch =  4567 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959452 learning rate =  0.0001\n",
      "Epoch =  4568 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959452 learning rate =  0.0001\n",
      "Epoch =  4569 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959452 learning rate =  0.0001\n",
      "Epoch =  4570 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959452 learning rate =  0.0001\n",
      "Epoch =  4571 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959452 learning rate =  0.0001\n",
      "Epoch =  4572 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959452 learning rate =  0.0001\n",
      "Epoch =  4573 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959444 learning rate =  0.0001\n",
      "Epoch =  4574 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959444 learning rate =  0.0001\n",
      "Epoch =  4575 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959444 learning rate =  0.0001\n",
      "Epoch =  4576 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395944 learning rate =  0.0001\n",
      "Epoch =  4577 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395944 learning rate =  0.0001\n",
      "Epoch =  4578 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395944 learning rate =  0.0001\n",
      "Epoch =  4579 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395944 learning rate =  0.0001\n",
      "Epoch =  4580 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395944 learning rate =  0.0001\n",
      "Epoch =  4581 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959435 learning rate =  0.0001\n",
      "Epoch =  4582 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959426 learning rate =  0.0001\n",
      "Epoch =  4583 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959426 learning rate =  0.0001\n",
      "Epoch =  4584 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959417 learning rate =  0.0001\n",
      "Epoch =  4585 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959404 learning rate =  0.0001\n",
      "Epoch =  4586 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959395 learning rate =  0.0001\n",
      "Epoch =  4587 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959404 learning rate =  0.0001\n",
      "Epoch =  4588 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395939 learning rate =  0.0001\n",
      "Epoch =  4589 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959395 learning rate =  0.0001\n",
      "Epoch =  4590 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395939 learning rate =  0.0001\n",
      "Epoch =  4591 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395939 learning rate =  0.0001\n",
      "Epoch =  4592 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959386 learning rate =  0.0001\n",
      "Epoch =  4593 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395938 learning rate =  0.0001\n",
      "Epoch =  4594 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959372 learning rate =  0.0001\n",
      "Epoch =  4595 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395937 learning rate =  0.0001\n",
      "Epoch =  4596 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395937 learning rate =  0.0001\n",
      "Epoch =  4597 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395937 learning rate =  0.0001\n",
      "Epoch =  4598 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395937 learning rate =  0.0001\n",
      "Epoch =  4599 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959372 learning rate =  0.0001\n",
      "Epoch =  4600 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395937 learning rate =  0.0001\n",
      "Epoch =  4601 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395936 learning rate =  0.0001\n",
      "Epoch =  4602 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395936 learning rate =  0.0001\n",
      "Epoch =  4603 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4604 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4605 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4606 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959346 learning rate =  0.0001\n",
      "Epoch =  4607 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959346 learning rate =  0.0001\n",
      "Epoch =  4608 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4609 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4610 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4611 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4612 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4613 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4614 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4615 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959355 learning rate =  0.0001\n",
      "Epoch =  4616 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4617 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4618 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4619 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4620 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4621 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4622 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4623 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395934 learning rate =  0.0001\n",
      "Epoch =  4624 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4625 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4626 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4627 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4628 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4629 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4630 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4631 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4632 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4633 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4634 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4635 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4636 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4637 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4638 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4639 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4640 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4641 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4642 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4643 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4644 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4645 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4646 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4647 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4648 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4649 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4650 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4651 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4652 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4653 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4654 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4655 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4656 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4657 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4658 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4659 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4660 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4661 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4662 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4663 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4664 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4665 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4666 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4667 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4668 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4669 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4670 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4671 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4672 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4673 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4674 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4675 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4676 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4677 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4678 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4679 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4680 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4681 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4682 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4683 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4684 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4685 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395933 learning rate =  0.0001\n",
      "Epoch =  4686 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4687 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4688 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959346 learning rate =  0.0001\n",
      "Epoch =  4689 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959346 learning rate =  0.0001\n",
      "Epoch =  4690 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959346 learning rate =  0.0001\n",
      "Epoch =  4691 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959346 learning rate =  0.0001\n",
      "Epoch =  4692 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4693 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4694 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4695 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4696 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4697 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4698 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959346 learning rate =  0.0001\n",
      "Epoch =  4699 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395935 learning rate =  0.0001\n",
      "Epoch =  4700 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959346 learning rate =  0.0001\n",
      "Epoch =  4701 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395934 learning rate =  0.0001\n",
      "Epoch =  4702 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395934 learning rate =  0.0001\n",
      "Epoch =  4703 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395934 learning rate =  0.0001\n",
      "Epoch =  4704 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4705 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4706 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4707 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4708 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4709 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4710 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4711 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959337 learning rate =  0.0001\n",
      "Epoch =  4712 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4713 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4714 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4715 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4716 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4717 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4718 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395934 learning rate =  0.0001\n",
      "Epoch =  4719 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4720 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4721 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4722 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4723 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4724 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4725 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4726 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4727 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4728 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4729 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959333 learning rate =  0.0001\n",
      "Epoch =  4730 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4731 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4732 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4733 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4734 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4735 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959324 learning rate =  0.0001\n",
      "Epoch =  4736 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959315 learning rate =  0.0001\n",
      "Epoch =  4737 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959315 learning rate =  0.0001\n",
      "Epoch =  4738 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395931 learning rate =  0.0001\n",
      "Epoch =  4739 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395931 learning rate =  0.0001\n",
      "Epoch =  4740 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959293 learning rate =  0.0001\n",
      "Epoch =  4741 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939593 learning rate =  0.0001\n",
      "Epoch =  4742 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395929 learning rate =  0.0001\n",
      "Epoch =  4743 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395929 learning rate =  0.0001\n",
      "Epoch =  4744 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4745 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4746 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4747 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959293 learning rate =  0.0001\n",
      "Epoch =  4748 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4749 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939593 learning rate =  0.0001\n",
      "Epoch =  4750 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939593 learning rate =  0.0001\n",
      "Epoch =  4751 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939593 learning rate =  0.0001\n",
      "Epoch =  4752 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959293 learning rate =  0.0001\n",
      "Epoch =  4753 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939593 learning rate =  0.0001\n",
      "Epoch =  4754 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.40655458939593 learning rate =  0.0001\n",
      "Epoch =  4755 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959293 learning rate =  0.0001\n",
      "Epoch =  4756 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4757 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4758 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959293 learning rate =  0.0001\n",
      "Epoch =  4759 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4760 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4761 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4762 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4763 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4764 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4765 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4766 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4767 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4768 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4769 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4770 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4771 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4772 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4773 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4774 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4775 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4776 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4777 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4778 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4779 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4780 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4781 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4782 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4783 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4784 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4785 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4786 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4787 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4788 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4789 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4790 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4791 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4792 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4793 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4794 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4795 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4796 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4797 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4798 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4799 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4800 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395928 learning rate =  0.0001\n",
      "Epoch =  4801 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4802 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4803 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4804 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4805 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959284 learning rate =  0.0001\n",
      "Epoch =  4806 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959275 learning rate =  0.0001\n",
      "Epoch =  4807 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4808 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4809 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395927 learning rate =  0.0001\n",
      "Epoch =  4810 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959266 learning rate =  0.0001\n",
      "Epoch =  4811 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959266 learning rate =  0.0001\n",
      "Epoch =  4812 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959266 learning rate =  0.0001\n",
      "Epoch =  4813 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959266 learning rate =  0.0001\n",
      "Epoch =  4814 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395926 learning rate =  0.0001\n",
      "Epoch =  4815 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395926 learning rate =  0.0001\n",
      "Epoch =  4816 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959253 learning rate =  0.0001\n",
      "Epoch =  4817 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959253 learning rate =  0.0001\n",
      "Epoch =  4818 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959253 learning rate =  0.0001\n",
      "Epoch =  4819 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395925 learning rate =  0.0001\n",
      "Epoch =  4820 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395925 learning rate =  0.0001\n",
      "Epoch =  4821 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395923 learning rate =  0.0001\n",
      "Epoch =  4822 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395923 learning rate =  0.0001\n",
      "Epoch =  4823 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395923 learning rate =  0.0001\n",
      "Epoch =  4824 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395923 learning rate =  0.0001\n",
      "Epoch =  4825 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959226 learning rate =  0.0001\n",
      "Epoch =  4826 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959226 learning rate =  0.0001\n",
      "Epoch =  4827 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959226 learning rate =  0.0001\n",
      "Epoch =  4828 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959226 learning rate =  0.0001\n",
      "Epoch =  4829 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959226 learning rate =  0.0001\n",
      "Epoch =  4830 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959226 learning rate =  0.0001\n",
      "Epoch =  4831 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959217 learning rate =  0.0001\n",
      "Epoch =  4832 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959213 learning rate =  0.0001\n",
      "Epoch =  4833 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395922 learning rate =  0.0001\n",
      "Epoch =  4834 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395922 learning rate =  0.0001\n",
      "Epoch =  4835 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959213 learning rate =  0.0001\n",
      "Epoch =  4836 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959213 learning rate =  0.0001\n",
      "Epoch =  4837 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959217 learning rate =  0.0001\n",
      "Epoch =  4838 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959217 learning rate =  0.0001\n",
      "Epoch =  4839 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395921 learning rate =  0.0001\n",
      "Epoch =  4840 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395921 learning rate =  0.0001\n",
      "Epoch =  4841 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959217 learning rate =  0.0001\n",
      "Epoch =  4842 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959213 learning rate =  0.0001\n",
      "Epoch =  4843 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959213 learning rate =  0.0001\n",
      "Epoch =  4844 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959213 learning rate =  0.0001\n",
      "Epoch =  4845 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959213 learning rate =  0.0001\n",
      "Epoch =  4846 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959213 learning rate =  0.0001\n",
      "Epoch =  4847 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959213 learning rate =  0.0001\n",
      "Epoch =  4848 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959195 learning rate =  0.0001\n",
      "Epoch =  4849 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959195 learning rate =  0.0001\n",
      "Epoch =  4850 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959195 learning rate =  0.0001\n",
      "Epoch =  4851 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959195 learning rate =  0.0001\n",
      "Epoch =  4852 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959195 learning rate =  0.0001\n",
      "Epoch =  4853 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959195 learning rate =  0.0001\n",
      "Epoch =  4854 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959195 learning rate =  0.0001\n",
      "Epoch =  4855 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4856 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4857 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4858 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4859 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395919 learning rate =  0.0001\n",
      "Epoch =  4860 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395919 learning rate =  0.0001\n",
      "Epoch =  4861 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395919 learning rate =  0.0001\n",
      "Epoch =  4862 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395919 learning rate =  0.0001\n",
      "Epoch =  4863 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4864 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395919 learning rate =  0.0001\n",
      "Epoch =  4865 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4866 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4867 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4868 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4869 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4870 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4871 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4872 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4873 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4874 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4875 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4876 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4877 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4878 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4879 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4880 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4881 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4882 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4883 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959164 learning rate =  0.0001\n",
      "Epoch =  4884 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4885 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4886 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4887 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4888 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4889 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4890 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4891 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4892 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4893 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4894 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4895 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4896 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4897 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4898 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4899 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4900 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4901 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4902 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4903 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4904 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4905 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4906 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4907 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4908 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4909 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4910 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4911 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4912 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4913 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4914 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4915 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4916 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4917 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4918 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4919 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4920 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4921 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4922 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4923 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4924 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4925 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4926 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4927 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4928 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4929 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4930 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4931 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4932 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4933 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4934 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4935 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4936 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4937 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4938 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4939 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4940 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4941 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4942 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4943 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4944 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4945 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395919 learning rate =  0.0001\n",
      "Epoch =  4946 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395919 learning rate =  0.0001\n",
      "Epoch =  4947 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4948 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4949 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4950 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4951 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4952 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4953 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4954 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4955 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959186 learning rate =  0.0001\n",
      "Epoch =  4956 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395919 learning rate =  0.0001\n",
      "Epoch =  4957 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395918 learning rate =  0.0001\n",
      "Epoch =  4958 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4959 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4960 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4961 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4962 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4963 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4964 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4965 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4966 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4967 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4968 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4969 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4970 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4971 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4972 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4973 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4974 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4975 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4976 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4977 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4978 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4979 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4980 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4981 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395917 learning rate =  0.0001\n",
      "Epoch =  4982 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395917 learning rate =  0.0001\n",
      "Epoch =  4983 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395917 learning rate =  0.0001\n",
      "Epoch =  4984 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395917 learning rate =  0.0001\n",
      "Epoch =  4985 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395917 learning rate =  0.0001\n",
      "Epoch =  4986 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395917 learning rate =  0.0001\n",
      "Epoch =  4987 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395917 learning rate =  0.0001\n",
      "Epoch =  4988 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395917 learning rate =  0.0001\n",
      "Epoch =  4989 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395917 learning rate =  0.0001\n",
      "Epoch =  4990 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.406554589395916 learning rate =  0.0001\n",
      "Epoch =  4991 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4992 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959173 learning rate =  0.0001\n",
      "Epoch =  4993 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4994 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4995 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4996 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4997 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4998 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n",
      "Epoch =  4999 Batch =  0 Loss =  [9.21780618] Gradient_max =  3.4065545893959177 learning rate =  0.0001\n"
     ]
    }
   ],
   "source": [
    "SGD = Solver(simplelogistic, x_train, y_train, lr = 1e-4, batch_size = 15, num_epochs = 5000, print_every = 1000)\n",
    "SGD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(0,SGD.num_epochs)\n",
    "plt.plot(epochs, SGD.loss_history, label = 'Loss', color = 'red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745113369820172"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def training_accuracy(model, x_train,y_train):\n",
    "    y_pred = model.predict(x_train)\n",
    "    y_new = y_train.reshape(y_pred.shape)\n",
    "    return np.sum(y_new == y_pred) / y_new.shape[0]\n",
    "\n",
    "\n",
    "training_accuracy(simplelogistic, x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74375"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_accuracy(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_new = y_test.reshape(y_pred.shape)\n",
    "    return np.sum(y_new == y_pred) / y_new.shape[0]\n",
    "\n",
    "x_test_new = x_test - np.mean(x_test, axis = 0)\n",
    "x_test_new = x_test_new / np.std(x_test_new, axis = 0)\n",
    "test_accuracy(simplelogistic, x_test_new, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8201e9342a4a492ddbd4e81efec90b2ccf0d205cda2cc39ac893f0c43374b5e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
