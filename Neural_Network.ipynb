{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Models.Layers import *\n",
    "from Solver import *\n",
    "from Models.Classifiers.Logistic_Classifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n",
      "320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>percentage_free_sulphur</th>\n",
       "      <th>n_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>30.909091</td>\n",
       "      <td>0.6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>0.8290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.7440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "      <td>35.294118</td>\n",
       "      <td>0.7195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>30.909091</td>\n",
       "      <td>0.6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.6610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1</td>\n",
       "      <td>13.076923</td>\n",
       "      <td>0.7110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.793103</td>\n",
       "      <td>0.7540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.6615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>1.2075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  percentage_free_sulphur  n_value  \n",
       "0         9.4        0                30.909091   0.6080  \n",
       "1         9.8        0                26.800000   0.8290  \n",
       "2         9.8        0                36.000000   0.7440  \n",
       "3         9.8        1                35.294118   0.7195  \n",
       "4         9.4        0                30.909091   0.6080  \n",
       "...       ...      ...                      ...      ...  \n",
       "1594     10.5        0                13.750000   0.6610  \n",
       "1595     11.2        1                13.076923   0.7110  \n",
       "1596     11.0        1                13.793103   0.7540  \n",
       "1597     10.2        0                13.750000   0.6615  \n",
       "1598     11.0        1                23.333333   1.2075  \n",
       "\n",
       "[1599 rows x 14 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first step would be to import the dataset\n",
    "X_full = pd.read_csv('./Datasets/red_wine_dataset.csv')\n",
    "percentage = 0.8\n",
    "X_full.pop('k_value')\n",
    "X_full.pop('l_value')\n",
    "X_full.pop('m_value')\n",
    "X_train = X_full.sample(frac=percentage, random_state=0)\n",
    "y_train = X_train.pop('quality')\n",
    "X_test = X_full.drop(X_train.index)\n",
    "y_test = X_test.pop('quality')\n",
    "print(len(X_train.index))\n",
    "print(len(X_test.index))\n",
    "X_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 13)\n",
      "(320, 13)\n",
      "float64\n",
      "(1279,)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train.to_numpy()\n",
    "x_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_train.dtype)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 13)\n",
      "(1279,)\n",
      "3.999927832269524e-16\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x_max = np.max(x_train, axis = 0)\n",
    "# x_min = np.min(x_train, axis = 0)\n",
    "# x_train = (x_train - x_min) / (x_max - x_min)\n",
    "mu = x_train.mean(axis = 0)\n",
    "sigma = x_train.std(axis = 0)\n",
    "x_train  = (x_train - x_train.mean(axis = 0))/x_train.std(axis = 0)\n",
    "# mu = x_min\n",
    "# sigma = (x_max - x_min)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=50, cmap='spring')\n",
    "print(x_train[:,3].mean())\n",
    "print(x_train[:,3].std())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet():\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, reg = 0.01):\n",
    "        self.reg = reg\n",
    "        self.params = {}\n",
    "        # Now we do intialization of the weights\n",
    "        np.random.seed(0)\n",
    "        self.params['W1'] = np.random.randn(input_dim, hidden_dim)*0.01\n",
    "        self.params['b1'] = np.zeros(hidden_dim)\n",
    "        self.params['W2'] = np.random.randn(hidden_dim, output_dim)*0.01\n",
    "        self.params['b2'] = np.zeros(output_dim)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def loss(self,X,y = None):\n",
    "        # define a mode here, i.e. a training mode or a test mode\n",
    "        mode = 'test' if y is None else 'train'\n",
    "        if(mode == 'train'):\n",
    "            cache = {}\n",
    "            loss = 0\n",
    "            grads = {}\n",
    "            z1, cache['affine_1'] = affine_forward(X,self.params['W1'],self.params['b1'])\n",
    "            a1,cache['relu_1'] = relu_forward(z1)\n",
    "            a2,cache['affine_2'] = affine_forward(a1,self.params['W2'], self.params['b2'])\n",
    "            loss, da2 = softmax_loss(a2,y)\n",
    "            da1, grads['W2'],grads['b2'] = affine_backward(da2,cache['affine_2'])\n",
    "            dz1 = relu_backward(da1, cache['relu_1'])\n",
    "            dx, grads['W1'],grads['b1'] = affine_backward(dz1, cache['affine_1'])\n",
    "            loss = loss + self.reg * (np.sum(self.params['W1']**2) + np.sum(self.params['W2']**2))\n",
    "            grads['W1'] = grads['W1'] + 2 * self.reg * self.params['W1']\n",
    "            grads['W2'] = grads['W2'] + 2 * self.reg * self.params['W2']\n",
    "            return loss, grads\n",
    "        else:\n",
    "            z1, cache['affine_1'] = affine_forward(X,self.params['W1'],self.params['b1'])\n",
    "            a1,cache['relu_1'] = relu_forward(z1)\n",
    "            a2,cache['affine_2'] = affine_forward(a1,self.params['W2'], self.params['b2'])\n",
    "            loss, da2 = softmax_loss(a2,y)\n",
    "            loss = loss + self.reg * (np.sum( (self.params['W1'] * self.params['W1'])))\n",
    "            loss = loss + self.reg * (np.sum( (self.params['W2'] * self.params['W2'])))\n",
    "            return loss\n",
    "    def predict(self, X):\n",
    "        cache= {}\n",
    "        z1, cache['affine_1'] = affine_forward(X,self.params['W1'],self.params['b1'])\n",
    "        a1,cache['relu_1'] = relu_forward(z1)\n",
    "        a2,cache['affine_2'] = affine_forward(a1,self.params['W2'], self.params['b2'])\n",
    "        print(a2.shape)\n",
    "        return np.argmax(a2,axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.6931489451046741\n",
      "shape of W_1 =  (13, 10)\n",
      "(1279, 2)\n"
     ]
    }
   ],
   "source": [
    "Model = TwoLayerNet(input_dim = x_train.shape[1], hidden_dim = 10, output_dim = 2)\n",
    "\n",
    "loss, grads = Model.loss(x_train, y_train)\n",
    "\n",
    "print(\"loss =\", loss)\n",
    "\n",
    "Model.params['W1'] = Model.params['W1'] - 0.1*grads['W1']\n",
    "Model.params['b1'] = Model.params['b1'] - 0.1*grads['b1']\n",
    "Model.params['W2'] = Model.params['W2'] - 0.1*grads['W2']\n",
    "Model.params['b2'] = Model.params['b2'] - 0.1*grads['b2']\n",
    "\n",
    "loss, grads = Model.loss(x_train, y_train)\n",
    "print(\"shape of W_1 = \",Model.params['W1'].shape)\n",
    "scores = Model.predict(x_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewSolver():\n",
    "    def __init__(self, model, X_train, y_train, lr = 0.05, batch_size = 20, num_epochs = 10, print_every = 1000):\n",
    "        self.lr = lr\n",
    "        self.data = {}\n",
    "        self.data['X_train'] = X_train\n",
    "        self.data['y_train'] = y_train\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.print_every = print_every\n",
    "        self.loss_history = np.array([])\n",
    "        self.grad_history = np.array([])\n",
    "        self.vel_history = np.array([])\n",
    "        self.loss_2_history = np.array([])\n",
    "        self.grad_2_history = np.array([])\n",
    "        pass\n",
    "    def train(self):\n",
    "        # mu = 0.95\n",
    "        # v_w = np.zeros(self.model.params['W'].shape)\n",
    "        # v_b = np.zeros_like(self.model.params['b']) \n",
    "        for i in range(self.num_epochs):\n",
    "            for j in range(self.data['X_train'].shape[0] // self.batch_size):\n",
    "                X_batch = self.data['X_train'][j * self.batch_size:(j + 1) * self.batch_size, :]\n",
    "                y_batch = self.data['y_train'][j * self.batch_size:(j + 1) * self.batch_size].reshape(-1,1)\n",
    "                loss, grads = self.model.loss(X_batch, y_batch)\n",
    "                # v_w = v_w*mu - self.lr * grads['W']\n",
    "                # v_b = v_b*mu - self.lr * grads['b']\n",
    "                # self.model.params['W'] += v_w\n",
    "                # self.model.params['b'] += v_b\n",
    "                self.model.params['W1'] += -1 * self.lr * grads['W1']\n",
    "                self.model.params['b1'] += -1 * self.lr * grads['b1']\n",
    "                self.model.params['W2'] += -1 * self.lr * grads['W2']\n",
    "                self.model.params['b2'] += -1 * self.lr * grads['b2']\n",
    "                if(j  == 0):\n",
    "                    print(\"Epoch = \", i, \"Batch = \", j, \"Loss = \", loss, \"Gradient_max = \", np.max(abs(grads['W1'])), \"learning rate ratio = \",np.max(self.lr*grads['W1']/self.model.params['W1']))\n",
    "                    self.loss_history = np.append(self.loss_history, loss)\n",
    "                    self.grad_history = np.append(self.grad_history, np.linalg.norm(grads['W1']))            \n",
    "                    # self.loss_history = np.append(self.loss_history, loss)\n",
    "                    # self.grad_history = np.append(self.grad_history, np.sum(grads['W1'] * grads['W1']))\n",
    "                    # self.vel_history = np.append(self.vel_history, np.sum(v_w * v_w))\n",
    "                if(j == 35):\n",
    "                    self.loss_2_history = np.append(self.loss_2_history, loss)\n",
    "                    self.grad_2_history = np.append(self.grad_2_history, np.linalg.norm(grads['W1']))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0 Batch =  0 Loss =  13.870685461350291 Gradient_max =  0.0032098391810733355 learning rate ratio =  0.00023407288408260372\n",
      "Epoch =  1 Batch =  0 Loss =  13.870680858450504 Gradient_max =  0.0032072313045777233 learning rate ratio =  0.0002354943871039326\n",
      "Epoch =  2 Batch =  0 Loss =  13.870676258930187 Gradient_max =  0.0032046246956387425 learning rate ratio =  0.0002369307501191725\n",
      "Epoch =  3 Batch =  0 Loss =  13.870671662787103 Gradient_max =  0.0032020193532253695 learning rate ratio =  0.00023838221074559134\n",
      "Epoch =  4 Batch =  0 Loss =  13.870667070019024 Gradient_max =  0.0031994152763070985 learning rate ratio =  0.00023984901168399824\n",
      "Epoch =  5 Batch =  0 Loss =  13.870662480623706 Gradient_max =  0.0031968124638539455 learning rate ratio =  0.00024133140085544986\n",
      "Epoch =  6 Batch =  0 Loss =  13.87065789459893 Gradient_max =  0.0031942109148364374 learning rate ratio =  0.00024282963154239316\n",
      "Epoch =  7 Batch =  0 Loss =  13.870653311906588 Gradient_max =  0.0031916106282816755 learning rate ratio =  0.0002443439625387023\n",
      "Epoch =  8 Batch =  0 Loss =  13.87064873259835 Gradient_max =  0.0031890113795048003 learning rate ratio =  0.0002458746582865855\n",
      "Epoch =  9 Batch =  0 Loss =  13.87064415665416 Gradient_max =  0.0031864133896906425 learning rate ratio =  0.0002474219890472323\n",
      "Epoch =  10 Batch =  0 Loss =  13.870639584071807 Gradient_max =  0.0031838166578131584 learning rate ratio =  0.0002489862310548375\n",
      "Epoch =  11 Batch =  0 Loss =  13.870635014849055 Gradient_max =  0.0031812211828468238 learning rate ratio =  0.00025056766668339863\n",
      "Epoch =  12 Batch =  0 Loss =  13.870630448996842 Gradient_max =  0.0031786269637699397 learning rate ratio =  0.00025216730031374157\n",
      "Epoch =  13 Batch =  0 Loss =  13.870625886499804 Gradient_max =  0.0031760339995546808 learning rate ratio =  0.00025378472898496604\n",
      "Epoch =  14 Batch =  0 Loss =  13.870621327355726 Gradient_max =  0.0031734422891770554 learning rate ratio =  0.00025542025499969423\n",
      "Epoch =  15 Batch =  0 Loss =  13.8706167715624 Gradient_max =  0.0031708518316135846 learning rate ratio =  0.0002570741875362834\n",
      "Epoch =  16 Batch =  0 Loss =  13.8706122191176 Gradient_max =  0.003168262625841302 learning rate ratio =  0.0002587468428454575\n",
      "Epoch =  17 Batch =  0 Loss =  13.870607669918735 Gradient_max =  0.0031656746173694345 learning rate ratio =  0.00026043854445566896\n",
      "Epoch =  18 Batch =  0 Loss =  13.870603124063996 Gradient_max =  0.0031630878556693077 learning rate ratio =  0.0002621496233777563\n",
      "Epoch =  19 Batch =  0 Loss =  13.870598581551173 Gradient_max =  0.0031605023397222897 learning rate ratio =  0.00026388041832859103\n",
      "Epoch =  20 Batch =  0 Loss =  13.87059404237806 Gradient_max =  0.003157918068510259 learning rate ratio =  0.0002656312759546199\n",
      "Epoch =  21 Batch =  0 Loss =  13.870589506588432 Gradient_max =  0.003155335040985181 learning rate ratio =  0.000267402551059376\n",
      "Epoch =  22 Batch =  0 Loss =  13.870584974133985 Gradient_max =  0.003152753256160394 learning rate ratio =  0.00026919460686294894\n",
      "Epoch =  23 Batch =  0 Loss =  13.87058044501252 Gradient_max =  0.0031501727130192943 learning rate ratio =  0.0002710078152347107\n",
      "Epoch =  24 Batch =  0 Loss =  13.870575919221835 Gradient_max =  0.0031475934105457833 learning rate ratio =  0.0002728425569585647\n",
      "Epoch =  25 Batch =  0 Loss =  13.870571396759741 Gradient_max =  0.0031450153477242727 learning rate ratio =  0.00027469922200171784\n",
      "Epoch =  26 Batch =  0 Loss =  13.870566877624036 Gradient_max =  0.0031424385235396726 learning rate ratio =  0.00027657820979323626\n",
      "Epoch =  27 Batch =  0 Loss =  13.870562361812537 Gradient_max =  0.0031398629369774013 learning rate ratio =  0.0002784799295127994\n",
      "Epoch =  28 Batch =  0 Loss =  13.870557849323042 Gradient_max =  0.003137288587023382 learning rate ratio =  0.0002804048003900964\n",
      "Epoch =  29 Batch =  0 Loss =  13.87055334015997 Gradient_max =  0.0031347155418892467 learning rate ratio =  0.0002823532520207104\n",
      "Epoch =  30 Batch =  0 Loss =  13.87054883431454 Gradient_max =  0.003132143728610099 learning rate ratio =  0.00028432572467208665\n",
      "Epoch =  31 Batch =  0 Loss =  13.870544331861286 Gradient_max =  0.0031295731464330828 learning rate ratio =  0.00028632266963786473\n",
      "Epoch =  32 Batch =  0 Loss =  13.870539832721105 Gradient_max =  0.0031270037940908206 learning rate ratio =  0.00028834454956712375\n",
      "Epoch =  33 Batch =  0 Loss =  13.870535336823488 Gradient_max =  0.003124435670623736 learning rate ratio =  0.0002903918388443332\n",
      "Epoch =  34 Batch =  0 Loss =  13.870530844234844 Gradient_max =  0.0031218687749737825 learning rate ratio =  0.0002924650239354934\n",
      "Epoch =  35 Batch =  0 Loss =  13.870526354953002 Gradient_max =  0.003119303106132918 learning rate ratio =  0.00029456460379983173\n",
      "Epoch =  36 Batch =  0 Loss =  13.870521869038674 Gradient_max =  0.0031167386630243667 learning rate ratio =  0.0002966910902791064\n",
      "Epoch =  37 Batch =  0 Loss =  13.870517386489249 Gradient_max =  0.0031141754446412854 learning rate ratio =  0.0002988450085283377\n",
      "Epoch =  38 Batch =  0 Loss =  13.870512907239577 Gradient_max =  0.0031116134500463704 learning rate ratio =  0.00030102689745147335\n",
      "Epoch =  39 Batch =  0 Loss =  13.870508431287485 Gradient_max =  0.003109052678233574 learning rate ratio =  0.00030323731014190113\n",
      "Epoch =  40 Batch =  0 Loss =  13.87050395864945 Gradient_max =  0.0031064931283144464 learning rate ratio =  0.00030548345307750043\n",
      "Epoch =  41 Batch =  0 Loss =  13.870499489304667 Gradient_max =  0.003103934799166965 learning rate ratio =  0.0003077594598179376\n",
      "Epoch =  42 Batch =  0 Loss =  13.870495023250974 Gradient_max =  0.003101377689786575 learning rate ratio =  0.0003100659347926692\n",
      "Epoch =  43 Batch =  0 Loss =  13.870490560486209 Gradient_max =  0.0030988217991692124 learning rate ratio =  0.000312403498839247\n",
      "Epoch =  44 Batch =  0 Loss =  13.870486100937574 Gradient_max =  0.003096267126381488 learning rate ratio =  0.00031477278976978245\n",
      "Epoch =  45 Batch =  0 Loss =  13.870481644673955 Gradient_max =  0.0030937136703504727 learning rate ratio =  0.00031717446293720174\n",
      "Epoch =  46 Batch =  0 Loss =  13.870477191693187 Gradient_max =  0.0030911614300735907 learning rate ratio =  0.0003196091918607736\n",
      "Epoch =  47 Batch =  0 Loss =  13.870472741993117 Gradient_max =  0.0030886104045487585 learning rate ratio =  0.00032207766885376744\n",
      "Epoch =  48 Batch =  0 Loss =  13.870468295571591 Gradient_max =  0.0030860605927743937 learning rate ratio =  0.00032458060568364174\n",
      "Epoch =  49 Batch =  0 Loss =  13.870463852433637 Gradient_max =  0.003083511993838856 learning rate ratio =  0.00032711873426018986\n",
      "Epoch =  50 Batch =  0 Loss =  13.87045941257017 Gradient_max =  0.003080964606652281 learning rate ratio =  0.0003296928073533827\n",
      "Epoch =  51 Batch =  0 Loss =  13.870454975979044 Gradient_max =  0.0030784184302145644 learning rate ratio =  0.00033230359934169714\n",
      "Epoch =  52 Batch =  0 Loss =  13.870450542658103 Gradient_max =  0.0030758734635260906 learning rate ratio =  0.00033495190699319143\n",
      "Epoch =  53 Batch =  0 Loss =  13.870446112767445 Gradient_max =  0.003073329543260372 learning rate ratio =  0.00033763855028963224\n",
      "Epoch =  54 Batch =  0 Loss =  13.870441686071942 Gradient_max =  0.0030707868228506204 learning rate ratio =  0.0003403643732536951\n",
      "Epoch =  55 Batch =  0 Loss =  13.870437262639975 Gradient_max =  0.003068245301362532 learning rate ratio =  0.0003431302448552538\n",
      "Epoch =  56 Batch =  0 Loss =  13.870432842425549 Gradient_max =  0.00306570497774625 learning rate ratio =  0.00034593705994131606\n",
      "Epoch =  57 Batch =  0 Loss =  13.870428425406164 Gradient_max =  0.003063165850929095 learning rate ratio =  0.00034878574019934583\n",
      "Epoch =  58 Batch =  0 Loss =  13.870424011699505 Gradient_max =  0.0030606279199884897 learning rate ratio =  0.00035167723515464646\n",
      "Epoch =  59 Batch =  0 Loss =  13.870419601248107 Gradient_max =  0.0030580911840120653 learning rate ratio =  0.0003546125232912215\n",
      "Epoch =  60 Batch =  0 Loss =  13.870415194049844 Gradient_max =  0.0030555556420116285 learning rate ratio =  0.00035759261309982704\n",
      "Epoch =  61 Batch =  0 Loss =  13.870410790102579 Gradient_max =  0.003053021292999469 learning rate ratio =  0.0003606185442590661\n",
      "Epoch =  62 Batch =  0 Loss =  13.870406389404183 Gradient_max =  0.0030504881359883574 learning rate ratio =  0.0003636913888504518\n",
      "Epoch =  63 Batch =  0 Loss =  13.87040199195253 Gradient_max =  0.003047956169991546 learning rate ratio =  0.0003668122526307156\n",
      "Epoch =  64 Batch =  0 Loss =  13.87039759787814 Gradient_max =  0.00304542539418588 learning rate ratio =  0.0003699822763383264\n",
      "Epoch =  65 Batch =  0 Loss =  13.870393206964657 Gradient_max =  0.0030428958074271463 learning rate ratio =  0.00037320263717480304\n",
      "Epoch =  66 Batch =  0 Loss =  13.870388819291119 Gradient_max =  0.0030403674087248838 learning rate ratio =  0.0003764745501682295\n",
      "Epoch =  67 Batch =  0 Loss =  13.870384434727969 Gradient_max =  0.0030378401970447043 learning rate ratio =  0.0003797992697722811\n",
      "Epoch =  68 Batch =  0 Loss =  13.870380053475783 Gradient_max =  0.003035314170929037 learning rate ratio =  0.0003831841476541333\n",
      "Epoch =  69 Batch =  0 Loss =  13.870375675355975 Gradient_max =  0.0030327893296777 learning rate ratio =  0.00038662467637433676\n",
      "Epoch =  70 Batch =  0 Loss =  13.87037130046818 Gradient_max =  0.003030265672546339 learning rate ratio =  0.0003901222465956931\n",
      "Epoch =  71 Batch =  0 Loss =  13.870366928855928 Gradient_max =  0.0030277431485415092 learning rate ratio =  0.00039367829568395985\n",
      "Epoch =  72 Batch =  0 Loss =  13.87036256047134 Gradient_max =  0.0030252218043889264 learning rate ratio =  0.00039729430968171806\n",
      "Epoch =  73 Batch =  0 Loss =  13.870358195312315 Gradient_max =  0.0030227016391088535 learning rate ratio =  0.00040097182539466475\n",
      "Epoch =  74 Batch =  0 Loss =  13.870353833376742 Gradient_max =  0.003020182651722028 learning rate ratio =  0.0004047124325752132\n",
      "Epoch =  75 Batch =  0 Loss =  13.870349474754043 Gradient_max =  0.003017664847813563 learning rate ratio =  0.00040851777622683965\n",
      "Epoch =  76 Batch =  0 Loss =  13.87034511922783 Gradient_max =  0.003015148215769344 learning rate ratio =  0.00041238955902220265\n",
      "Epoch =  77 Batch =  0 Loss =  13.870340766855445 Gradient_max =  0.0030126327542925307 learning rate ratio =  0.0004163295438224983\n",
      "Epoch =  78 Batch =  0 Loss =  13.870336417698532 Gradient_max =  0.0030101184626901588 learning rate ratio =  0.00042033955642705226\n",
      "Epoch =  79 Batch =  0 Loss =  13.870332071675485 Gradient_max =  0.003007605339750811 learning rate ratio =  0.0004244214883420666\n",
      "Epoch =  80 Batch =  0 Loss =  13.870327728863929 Gradient_max =  0.003005093384740574 learning rate ratio =  0.00042857729982267203\n",
      "Epoch =  81 Batch =  0 Loss =  13.870323389261769 Gradient_max =  0.0030025825966874353 learning rate ratio =  0.00043280902297801455\n",
      "Epoch =  82 Batch =  0 Loss =  13.870319052866906 Gradient_max =  0.0030000729746198504 learning rate ratio =  0.00043711876511010007\n",
      "Epoch =  83 Batch =  0 Loss =  13.870314719677248 Gradient_max =  0.0029975645175667415 learning rate ratio =  0.00044150871222422666\n",
      "Epoch =  84 Batch =  0 Loss =  13.87031038970572 Gradient_max =  0.002995057224559992 learning rate ratio =  0.00044597293279245167\n",
      "Epoch =  85 Batch =  0 Loss =  13.870306062935223 Gradient_max =  0.0029925510946269573 learning rate ratio =  0.00045052165614748455\n",
      "Epoch =  86 Batch =  0 Loss =  13.870301739324383 Gradient_max =  0.0029900461269370387 learning rate ratio =  0.00045515731324686506\n",
      "Epoch =  87 Batch =  0 Loss =  13.870297418910685 Gradient_max =  0.0029875423203821975 learning rate ratio =  0.00045988242914740137\n",
      "Epoch =  88 Batch =  0 Loss =  13.870293101692056 Gradient_max =  0.002985039673993695 learning rate ratio =  0.0004646996276378107\n",
      "Epoch =  89 Batch =  0 Loss =  13.870288787683291 Gradient_max =  0.002982538186876436 learning rate ratio =  0.00046961081118274193\n",
      "Epoch =  90 Batch =  0 Loss =  13.870284476865432 Gradient_max =  0.0029800378579895024 learning rate ratio =  0.00047461960689375977\n",
      "Epoch =  91 Batch =  0 Loss =  13.87028016923228 Gradient_max =  0.0029775386864014115 learning rate ratio =  0.000479714291207533\n",
      "Epoch =  92 Batch =  0 Loss =  13.870275864785807 Gradient_max =  0.0029750406711094037 learning rate ratio =  0.0004849119848577249\n",
      "Epoch =  93 Batch =  0 Loss =  13.870271563526067 Gradient_max =  0.002972543811157205 learning rate ratio =  0.0004902158536276488\n",
      "Epoch =  94 Batch =  0 Loss =  13.870267265387895 Gradient_max =  0.002970048105735473 learning rate ratio =  0.000495629194366525\n",
      "Epoch =  95 Batch =  0 Loss =  13.87026297043043 Gradient_max =  0.0029675535537122774 learning rate ratio =  0.0005011554417601494\n",
      "Epoch =  96 Batch =  0 Loss =  13.870258678651597 Gradient_max =  0.0029650601541225947 learning rate ratio =  0.0005067981756986752\n",
      "Epoch =  97 Batch =  0 Loss =  13.870254390049334 Gradient_max =  0.0029625679060018696 learning rate ratio =  0.0005125611290012675\n",
      "Epoch =  98 Batch =  0 Loss =  13.870250104621569 Gradient_max =  0.0029600768083860064 learning rate ratio =  0.000518448195670546\n",
      "Epoch =  99 Batch =  0 Loss =  13.870245822366238 Gradient_max =  0.0029575868603113706 learning rate ratio =  0.0005244634396875059\n",
      "Epoch =  100 Batch =  0 Loss =  13.87024154337987 Gradient_max =  0.0029550980609564415 learning rate ratio =  0.000530611104379037\n",
      "Epoch =  101 Batch =  0 Loss =  13.87023726756145 Gradient_max =  0.002952610409216402 learning rate ratio =  0.0005368956224511818\n",
      "Epoch =  102 Batch =  0 Loss =  13.87023299495189 Gradient_max =  0.0029501237963760526 learning rate ratio =  0.00054332162665041\n",
      "Epoch =  103 Batch =  0 Loss =  13.870228725571698 Gradient_max =  0.0029476383206198985 learning rate ratio =  0.0005498939611613408\n",
      "Epoch =  104 Batch =  0 Loss =  13.870224459353036 Gradient_max =  0.002945153985049898 learning rate ratio =  0.0005566176938427633\n",
      "Epoch =  105 Batch =  0 Loss =  13.87022019629384 Gradient_max =  0.0029426707887096285 learning rate ratio =  0.0005634981292686583\n",
      "Epoch =  106 Batch =  0 Loss =  13.87021593637321 Gradient_max =  0.0029401887305754246 learning rate ratio =  0.0005705408227147491\n",
      "Epoch =  107 Batch =  0 Loss =  13.870211679670287 Gradient_max =  0.002937707675596568 learning rate ratio =  0.000577751595152035\n",
      "Epoch =  108 Batch =  0 Loss =  13.870207426153302 Gradient_max =  0.0029352277544596526 learning rate ratio =  0.0005851365492977919\n",
      "Epoch =  109 Batch =  0 Loss =  13.870203175788111 Gradient_max =  0.00293274896596994 learning rate ratio =  0.0005927020869341471\n",
      "Epoch =  110 Batch =  0 Loss =  13.870198928572673 Gradient_max =  0.0029302713091760237 learning rate ratio =  0.0006004549273924451\n",
      "Epoch =  111 Batch =  0 Loss =  13.870194684504941 Gradient_max =  0.0029277947831269453 learning rate ratio =  0.0006084021275112104\n",
      "Epoch =  112 Batch =  0 Loss =  13.870190443582874 Gradient_max =  0.0029253193868722027 learning rate ratio =  0.0006165511030966777\n",
      "Epoch =  113 Batch =  0 Loss =  13.87018620580443 Gradient_max =  0.002922845119461742 learning rate ratio =  0.0006249096520419015\n",
      "Epoch =  114 Batch =  0 Loss =  13.870181971167572 Gradient_max =  0.0029203719799459618 learning rate ratio =  0.0006334859792559538\n",
      "Epoch =  115 Batch =  0 Loss =  13.87017773972 Gradient_max =  0.00291789983572181 learning rate ratio =  0.0006422887235683744\n",
      "Epoch =  116 Batch =  0 Loss =  13.870173511403376 Gradient_max =  0.0029154288156599973 learning rate ratio =  0.0006513269868037766\n",
      "Epoch =  117 Batch =  0 Loss =  13.870169286121252 Gradient_max =  0.002912958918580743 learning rate ratio =  0.0006606103651980041\n",
      "Epoch =  118 Batch =  0 Loss =  13.87016506397278 Gradient_max =  0.0029104901437898146 learning rate ratio =  0.0006701489834940181\n",
      "Epoch =  119 Batch =  0 Loss =  13.870160844771215 Gradient_max =  0.0029080224900494885 learning rate ratio =  0.0006799535317570461\n",
      "Epoch =  120 Batch =  0 Loss =  13.870156628676542 Gradient_max =  0.002905555956672 learning rate ratio =  0.000690035305446425\n",
      "Epoch =  121 Batch =  0 Loss =  13.870152415723954 Gradient_max =  0.002903090561431994 learning rate ratio =  0.0007004062488646052\n",
      "Epoch =  122 Batch =  0 Loss =  13.870148205897136 Gradient_max =  0.0029006262839290065 learning rate ratio =  0.000711079002395684\n",
      "Epoch =  123 Batch =  0 Loss =  13.870143999148137 Gradient_max =  0.002898162946425973 learning rate ratio =  0.0007220669540199033\n",
      "Epoch =  124 Batch =  0 Loss =  13.870139795532447 Gradient_max =  0.0028957007241354115 learning rate ratio =  0.0007334387513521506\n",
      "Epoch =  125 Batch =  0 Loss =  13.870135595036459 Gradient_max =  0.002893239616003823 learning rate ratio =  0.0007451584256336219\n",
      "Epoch =  126 Batch =  0 Loss =  13.870131397658161 Gradient_max =  0.0028907796210904417 learning rate ratio =  0.000757242208209056\n",
      "Epoch =  127 Batch =  0 Loss =  13.870127203395528 Gradient_max =  0.0028883207384549506 learning rate ratio =  0.0007697073560866952\n",
      "Epoch =  128 Batch =  0 Loss =  13.870123012269183 Gradient_max =  0.002885862864050745 learning rate ratio =  0.0007825722342528315\n",
      "Epoch =  129 Batch =  0 Loss =  13.8701188242544 Gradient_max =  0.0028834060984188207 learning rate ratio =  0.0007958564060472048\n",
      "Epoch =  130 Batch =  0 Loss =  13.870114639349168 Gradient_max =  0.0028809504406218157 learning rate ratio =  0.0008095807325245926\n",
      "Epoch =  131 Batch =  0 Loss =  13.870110457551483 Gradient_max =  0.0028784958897228076 learning rate ratio =  0.0008237674818740374\n",
      "Epoch =  132 Batch =  0 Loss =  13.870106278872477 Gradient_max =  0.0028760423148781796 learning rate ratio =  0.000838440450099989\n",
      "Epoch =  133 Batch =  0 Loss =  13.870102103188692 Gradient_max =  0.002873589843961419 learning rate ratio =  0.0008536250943512108\n",
      "Epoch =  134 Batch =  0 Loss =  13.87009793060667 Gradient_max =  0.002871138476059249 learning rate ratio =  0.0008693486803897847\n",
      "Epoch =  135 Batch =  0 Loss =  13.870093761137564 Gradient_max =  0.0028686882102029066 learning rate ratio =  0.0008856433967665336\n",
      "Epoch =  136 Batch =  0 Loss =  13.870089594687641 Gradient_max =  0.0028662390449547565 learning rate ratio =  0.0009025379089004922\n",
      "Epoch =  137 Batch =  0 Loss =  13.870085431333765 Gradient_max =  0.00286379097992036 learning rate ratio =  0.0009200659836719823\n",
      "Epoch =  138 Batch =  0 Loss =  13.870081271079025 Gradient_max =  0.0028613440142762976 learning rate ratio =  0.0009381219652578958\n",
      "Epoch =  139 Batch =  0 Loss =  13.870077113916324 Gradient_max =  0.0028588981469810145 learning rate ratio =  0.0009568757117674727\n",
      "Epoch =  140 Batch =  0 Loss =  13.870072959843672 Gradient_max =  0.0028564533771026288 learning rate ratio =  0.0009763684990992954\n",
      "Epoch =  141 Batch =  0 Loss =  13.870068808859056 Gradient_max =  0.0028540097037096997 learning rate ratio =  0.0009966449240820512\n",
      "Epoch =  142 Batch =  0 Loss =  13.870064660960498 Gradient_max =  0.0028515671258712177 learning rate ratio =  0.001017753245320239\n",
      "Epoch =  143 Batch =  0 Loss =  13.870060516146005 Gradient_max =  0.0028491256426566086 learning rate ratio =  0.0010397457668991506\n",
      "Epoch =  144 Batch =  0 Loss =  13.870056374413586 Gradient_max =  0.0028466852531357327 learning rate ratio =  0.001062679271371215\n",
      "Epoch =  145 Batch =  0 Loss =  13.870052235761255 Gradient_max =  0.0028442459563788893 learning rate ratio =  0.001086615509569628\n",
      "Epoch =  146 Batch =  0 Loss =  13.87004810018703 Gradient_max =  0.002841807751456802 learning rate ratio =  0.0011116217561452052\n",
      "Epoch =  147 Batch =  0 Loss =  13.870043967688916 Gradient_max =  0.0028393706374406353 learning rate ratio =  0.0011377714413503427\n",
      "Epoch =  148 Batch =  0 Loss =  13.870039838264942 Gradient_max =  0.0028369346134019873 learning rate ratio =  0.0011651448715649208\n",
      "Epoch =  149 Batch =  0 Loss =  13.870035711975264 Gradient_max =  0.0028344996783632483 learning rate ratio =  0.0011942476129391624\n",
      "Epoch =  150 Batch =  0 Loss =  13.870031588755799 Gradient_max =  0.002832065831446444 learning rate ratio =  0.001224801180282871\n",
      "Epoch =  151 Batch =  0 Loss =  13.870027468604574 Gradient_max =  0.002829633071724467 learning rate ratio =  0.0012569168875128135\n",
      "Epoch =  152 Batch =  0 Loss =  13.870023351519608 Gradient_max =  0.0028272013982706444 learning rate ratio =  0.0012907177346114145\n",
      "Epoch =  153 Batch =  0 Loss =  13.870019237498926 Gradient_max =  0.0028247708101587325 learning rate ratio =  0.001326339982508484\n",
      "Epoch =  154 Batch =  0 Loss =  13.870015126540562 Gradient_max =  0.0028223413064629188 learning rate ratio =  0.0013639349897059379\n",
      "Epoch =  155 Batch =  0 Loss =  13.87001101864254 Gradient_max =  0.00281991288625782 learning rate ratio =  0.0014036713628428682\n",
      "Epoch =  156 Batch =  0 Loss =  13.870006913802891 Gradient_max =  0.0028174855486184877 learning rate ratio =  0.0014457374856435548\n",
      "Epoch =  157 Batch =  0 Loss =  13.87000281201965 Gradient_max =  0.0028150592926204023 learning rate ratio =  0.0014903445062655486\n",
      "Epoch =  158 Batch =  0 Loss =  13.869998713290842 Gradient_max =  0.002812634117339469 learning rate ratio =  0.0015377298830061084\n",
      "Epoch =  159 Batch =  0 Loss =  13.869994617614509 Gradient_max =  0.002810210021852027 learning rate ratio =  0.0015881616140368008\n",
      "Epoch =  160 Batch =  0 Loss =  13.86999052498869 Gradient_max =  0.0028077870052348433 learning rate ratio =  0.0016419433102351291\n",
      "Epoch =  161 Batch =  0 Loss =  13.869986435411423 Gradient_max =  0.0028053650665651134 learning rate ratio =  0.001699420313910263\n",
      "Epoch =  162 Batch =  0 Loss =  13.869982348880741 Gradient_max =  0.0028029442049204633 learning rate ratio =  0.0017609871239527976\n",
      "Epoch =  163 Batch =  0 Loss =  13.86997826539469 Gradient_max =  0.002800524419378941 learning rate ratio =  0.0018270964648437573\n",
      "Epoch =  164 Batch =  0 Loss =  13.869974184938647 Gradient_max =  0.0027981057089663517 learning rate ratio =  0.0018982704403696028\n",
      "Epoch =  165 Batch =  0 Loss =  13.86997010752331 Gradient_max =  0.0027956880728141446 learning rate ratio =  0.0019751143534596513\n",
      "Epoch =  166 Batch =  0 Loss =  13.869966033146737 Gradient_max =  0.002793271510001652 learning rate ratio =  0.002058333966377472\n",
      "Epoch =  167 Batch =  0 Loss =  13.86996196172952 Gradient_max =  0.0027908560196525795 learning rate ratio =  0.0021487572439289455\n",
      "Epoch =  168 Batch =  0 Loss =  13.869957893347287 Gradient_max =  0.0027884416008032834 learning rate ratio =  0.0022473619990818353\n",
      "Epoch =  169 Batch =  0 Loss =  13.869953827979495 Gradient_max =  0.0027860282525658904 learning rate ratio =  0.0023553113999335226\n",
      "Epoch =  170 Batch =  0 Loss =  13.869949765642748 Gradient_max =  0.002783615973990028 learning rate ratio =  0.0024740000745742505\n",
      "Epoch =  171 Batch =  0 Loss =  13.869945706335097 Gradient_max =  0.002781204764157157 learning rate ratio =  0.0026051146982949457\n",
      "Epoch =  172 Batch =  0 Loss =  13.869941650066414 Gradient_max =  0.0027787946221900874 learning rate ratio =  0.002750714663314796\n",
      "Epoch =  173 Batch =  0 Loss =  13.869937596808999 Gradient_max =  0.0027763855470719087 learning rate ratio =  0.002913755603829477\n",
      "Epoch =  174 Batch =  0 Loss =  13.869933546574964 Gradient_max =  0.0027739775379436453 learning rate ratio =  0.0030971022382566262\n",
      "Epoch =  175 Batch =  0 Loss =  13.86992949936237 Gradient_max =  0.0027715705938884576 learning rate ratio =  0.0033048002108627106\n",
      "Epoch =  176 Batch =  0 Loss =  13.869925455025419 Gradient_max =  0.0027691647142651073 learning rate ratio =  0.003542046339878157\n",
      "Epoch =  177 Batch =  0 Loss =  13.869921413603311 Gradient_max =  0.0027667598976040034 learning rate ratio =  0.003815629426750793\n",
      "Epoch =  178 Batch =  0 Loss =  13.869917375197181 Gradient_max =  0.002764356143268306 learning rate ratio =  0.0041345904231066795\n",
      "Epoch =  179 Batch =  0 Loss =  13.869913339719586 Gradient_max =  0.002761953450168891 learning rate ratio =  0.0045112408147600855\n",
      "Epoch =  180 Batch =  0 Loss =  13.869909307254332 Gradient_max =  0.0027595518175651174 learning rate ratio =  0.004962788546322356\n",
      "Epoch =  181 Batch =  0 Loss =  13.869905277799491 Gradient_max =  0.002757151244542678 learning rate ratio =  0.005514041018346448\n",
      "Epoch =  182 Batch =  0 Loss =  13.869901251353129 Gradient_max =  0.002754751730187688 learning rate ratio =  0.00620212087549421\n",
      "Epoch =  183 Batch =  0 Loss =  13.869897227913325 Gradient_max =  0.0027523532735866806 learning rate ratio =  0.007085190988286317\n",
      "Epoch =  184 Batch =  0 Loss =  13.869893207478153 Gradient_max =  0.0027499558738266136 learning rate ratio =  0.008259821651187039\n",
      "Epoch =  185 Batch =  0 Loss =  13.869889190045683 Gradient_max =  0.002747559529994863 learning rate ratio =  0.009898961633789448\n",
      "Epoch =  186 Batch =  0 Loss =  13.869885175560277 Gradient_max =  0.0027451642411788746 learning rate ratio =  0.01234605919706019\n",
      "Epoch =  187 Batch =  0 Loss =  13.869881164074148 Gradient_max =  0.0027427700064674896 learning rate ratio =  0.016393887363690112\n",
      "Epoch =  188 Batch =  0 Loss =  13.869877155614413 Gradient_max =  0.00274037682498221 learning rate ratio =  0.024401082710303078\n",
      "Epoch =  189 Batch =  0 Loss =  13.869873150150129 Gradient_max =  0.0027379846957792334 learning rate ratio =  0.047643622391999894\n",
      "Epoch =  190 Batch =  0 Loss =  13.869869147723747 Gradient_max =  0.002735593612043014 learning rate ratio =  0.9799936759671586\n",
      "Epoch =  191 Batch =  0 Loss =  13.8698651482889 Gradient_max =  0.0027332035772134134 learning rate ratio =  0.00010409763990906213\n",
      "Epoch =  192 Batch =  0 Loss =  13.869861151967324 Gradient_max =  0.002730814510922972 learning rate ratio =  0.00010447768605634625\n",
      "Epoch =  193 Batch =  0 Loss =  13.869857158633137 Gradient_max =  0.0027284264856329444 learning rate ratio =  0.00010485995485352957\n",
      "Epoch =  194 Batch =  0 Loss =  13.86985316833837 Gradient_max =  0.002726039500481771 learning rate ratio =  0.00010524455718097415\n",
      "Epoch =  195 Batch =  0 Loss =  13.86984918105269 Gradient_max =  0.002723653554575761 learning rate ratio =  0.00010563151537492533\n",
      "Epoch =  196 Batch =  0 Loss =  13.869845196748587 Gradient_max =  0.002721268646968147 learning rate ratio =  0.00010602075979443527\n",
      "Epoch =  197 Batch =  0 Loss =  13.869841215424154 Gradient_max =  0.002718884776759053 learning rate ratio =  0.00010641231124705378\n",
      "Epoch =  198 Batch =  0 Loss =  13.869837237077494 Gradient_max =  0.002716501943049017 learning rate ratio =  0.00010680619079241102\n",
      "Epoch =  199 Batch =  0 Loss =  13.869833261706699 Gradient_max =  0.0027141201449389853 learning rate ratio =  0.00010720241974604999\n",
      "Epoch =  200 Batch =  0 Loss =  13.869829289265567 Gradient_max =  0.0027117396105122205 learning rate ratio =  0.00010760101968477909\n",
      "Epoch =  201 Batch =  0 Loss =  13.869825319796533 Gradient_max =  0.002709360108810999 learning rate ratio =  0.00010800201244628424\n",
      "Epoch =  202 Batch =  0 Loss =  13.869821353297699 Gradient_max =  0.002706981638938429 learning rate ratio =  0.00010840542013754447\n",
      "Epoch =  203 Batch =  0 Loss =  13.869817389767169 Gradient_max =  0.002704604199998026 learning rate ratio =  0.0001088112651374946\n",
      "Epoch =  204 Batch =  0 Loss =  13.869813429097348 Gradient_max =  0.002702227790825703 learning rate ratio =  0.00010921957009838105\n",
      "Epoch =  205 Batch =  0 Loss =  13.869809471302142 Gradient_max =  0.002699852379173535 learning rate ratio =  0.00010963035795195397\n",
      "Epoch =  206 Batch =  0 Loss =  13.86980551646544 Gradient_max =  0.002697477993719389 learning rate ratio =  0.00011004252417816856\n",
      "Epoch =  207 Batch =  0 Loss =  13.869801564589997 Gradient_max =  0.002695104633418237 learning rate ratio =  0.00011045720439289022\n",
      "Epoch =  208 Batch =  0 Loss =  13.869797615673923 Gradient_max =  0.002692732297377869 learning rate ratio =  0.00011087442217241787\n",
      "Epoch =  209 Batch =  0 Loss =  13.869793669715342 Gradient_max =  0.0026903609847064823 learning rate ratio =  0.00011129420138754155\n",
      "Epoch =  210 Batch =  0 Loss =  13.869789726758285 Gradient_max =  0.002687990694585164 learning rate ratio =  0.00011171656620825235\n",
      "Epoch =  211 Batch =  0 Loss =  13.869785786754685 Gradient_max =  0.002685621426050071 learning rate ratio =  0.000112141541108138\n",
      "Epoch =  212 Batch =  0 Loss =  13.869781849715217 Gradient_max =  0.002683253178438923 learning rate ratio =  0.0001125691508734185\n",
      "Epoch =  213 Batch =  0 Loss =  13.869777915625681 Gradient_max =  0.0026808859506334236 learning rate ratio =  0.00011299942059604231\n",
      "Epoch =  214 Batch =  0 Loss =  13.869773984348772 Gradient_max =  0.0026785197415445474 learning rate ratio =  0.0001134323756904739\n",
      "Epoch =  215 Batch =  0 Loss =  13.869770055923016 Gradient_max =  0.002676154550128796 learning rate ratio =  0.00011386804188860268\n",
      "Epoch =  216 Batch =  0 Loss =  13.869766130442311 Gradient_max =  0.002673790375850996 learning rate ratio =  0.00011430644526269992\n",
      "Epoch =  217 Batch =  0 Loss =  13.869762207904785 Gradient_max =  0.0026714272178225487 learning rate ratio =  0.00011474761221320135\n",
      "Epoch =  218 Batch =  0 Loss =  13.869758288308562 Gradient_max =  0.002670853224730456 learning rate ratio =  0.00011519156947987404\n",
      "Epoch =  219 Batch =  0 Loss =  13.86975437174 Gradient_max =  0.0026739036577013387 learning rate ratio =  0.00011563834414694447\n",
      "Epoch =  220 Batch =  0 Loss =  13.86975045810928 Gradient_max =  0.0026769549969586732 learning rate ratio =  0.00011608796364974071\n",
      "Epoch =  221 Batch =  0 Loss =  13.869746547414538 Gradient_max =  0.002680007243709791 learning rate ratio =  0.00011654045577928575\n",
      "Epoch =  222 Batch =  0 Loss =  13.869742639684679 Gradient_max =  0.002683060420692434 learning rate ratio =  0.00011699557321047773\n",
      "Epoch =  223 Batch =  0 Loss =  13.86973873488719 Gradient_max =  0.0026861145102192274 learning rate ratio =  0.00011745361604393032\n",
      "Epoch =  224 Batch =  0 Loss =  13.869734833020205 Gradient_max =  0.002689169513501377 learning rate ratio =  0.0001179146131145613\n",
      "Epoch =  225 Batch =  0 Loss =  13.869730934081868 Gradient_max =  0.0026922254317504166 learning rate ratio =  0.00011837859363618139\n",
      "Epoch =  226 Batch =  0 Loss =  13.86972703794994 Gradient_max =  0.0026952822661647853 learning rate ratio =  0.00011884558720714908\n",
      "Epoch =  227 Batch =  0 Loss =  13.869723144780888 Gradient_max =  0.0026983398936453883 learning rate ratio =  0.00011931735422866375\n",
      "Epoch =  228 Batch =  0 Loss =  13.869719254551505 Gradient_max =  0.0027013985501945295 learning rate ratio =  0.0001197907122931959\n",
      "Epoch =  229 Batch =  0 Loss =  13.869715367202298 Gradient_max =  0.002704458130765956 learning rate ratio =  0.00012026717838341361\n",
      "Epoch =  230 Batch =  0 Loss =  13.869711482772962 Gradient_max =  0.002707518636417432 learning rate ratio =  0.00012074678375479537\n",
      "Epoch =  231 Batch =  0 Loss =  13.869707601235689 Gradient_max =  0.002710580402795963 learning rate ratio =  0.00012122482264476352\n",
      "Epoch =  232 Batch =  0 Loss =  13.869703722614545 Gradient_max =  0.0027136430993925493 learning rate ratio =  0.00012170599429079048\n",
      "Epoch =  233 Batch =  0 Loss =  13.86969984690768 Gradient_max =  0.0027167067274285316 learning rate ratio =  0.00012219033014070576\n",
      "Epoch =  234 Batch =  0 Loss =  13.869695974113245 Gradient_max =  0.00271977128812559 learning rate ratio =  0.0001226778620633191\n",
      "Epoch =  235 Batch =  0 Loss =  13.869692104229406 Gradient_max =  0.002722836782705737 learning rate ratio =  0.000123168622355491\n",
      "Epoch =  236 Batch =  0 Loss =  13.869688237254303 Gradient_max =  0.002725903212391321 learning rate ratio =  0.00012366264374934704\n",
      "Epoch =  237 Batch =  0 Loss =  13.869684373186104 Gradient_max =  0.002728970578405033 learning rate ratio =  0.00012415995941963853\n",
      "Epoch =  238 Batch =  0 Loss =  13.869680512022969 Gradient_max =  0.0027320388819698973 learning rate ratio =  0.0001246606029912526\n",
      "Epoch =  239 Batch =  0 Loss =  13.869676653763054 Gradient_max =  0.0027351081243092794 learning rate ratio =  0.00012516460854687604\n",
      "Epoch =  240 Batch =  0 Loss =  13.869672798309297 Gradient_max =  0.0027381783066363105 learning rate ratio =  0.000125672010634402\n",
      "Epoch =  241 Batch =  0 Loss =  13.869668945661466 Gradient_max =  0.0027412494302414075 learning rate ratio =  0.00012618284427889467\n",
      "Epoch =  242 Batch =  0 Loss =  13.869665095861233 Gradient_max =  0.002744321496298796 learning rate ratio =  0.00012669714498173132\n",
      "Epoch =  243 Batch =  0 Loss =  13.86966124883863 Gradient_max =  0.0027473945060059375 learning rate ratio =  0.00012721494873538182\n",
      "Epoch =  244 Batch =  0 Loss =  13.869657404711402 Gradient_max =  0.002750468460608763 learning rate ratio =  0.00012773629203308777\n",
      "Epoch =  245 Batch =  0 Loss =  13.869653563477728 Gradient_max =  0.0027535433613326823 learning rate ratio =  0.00012826121187434477\n",
      "Epoch =  246 Batch =  0 Loss =  13.869649725135769 Gradient_max =  0.0027566192094034396 learning rate ratio =  0.0001287897457747118\n",
      "Epoch =  247 Batch =  0 Loss =  13.86964588972047 Gradient_max =  0.0027596960060399627 learning rate ratio =  0.0001293219317745054\n",
      "Epoch =  248 Batch =  0 Loss =  13.869642057193168 Gradient_max =  0.002762773752475869 learning rate ratio =  0.00012985780844903234\n",
      "Epoch =  249 Batch =  0 Loss =  13.869638227552048 Gradient_max =  0.0027658524499379372 learning rate ratio =  0.00013039741491700178\n",
      "Epoch =  250 Batch =  0 Loss =  13.869634400788215 Gradient_max =  0.002768932099649605 learning rate ratio =  0.0001309407908503018\n",
      "Epoch =  251 Batch =  0 Loss =  13.869630576906964 Gradient_max =  0.002772012702842034 learning rate ratio =  0.00013148797648434392\n",
      "Epoch =  252 Batch =  0 Loss =  13.869626755944061 Gradient_max =  0.002775094260761817 learning rate ratio =  0.00013203901262846755\n",
      "Epoch =  253 Batch =  0 Loss =  13.869622937859857 Gradient_max =  0.002778176774618273 learning rate ratio =  0.00013259394067368342\n",
      "Epoch =  254 Batch =  0 Loss =  13.86961912265253 Gradient_max =  0.0027812602456399006 learning rate ratio =  0.00013315280260581668\n",
      "Epoch =  255 Batch =  0 Loss =  13.869615310320262 Gradient_max =  0.0027843446750555473 learning rate ratio =  0.00013371564101531062\n",
      "Epoch =  256 Batch =  0 Loss =  13.869611500861241 Gradient_max =  0.002787430064094408 learning rate ratio =  0.00013428249910815246\n",
      "Epoch =  257 Batch =  0 Loss =  13.869607694262019 Gradient_max =  0.002790516413990195 learning rate ratio =  0.0001348534207172498\n",
      "Epoch =  258 Batch =  0 Loss =  13.869603890532431 Gradient_max =  0.0027936037259685986 learning rate ratio =  0.0001354284503132185\n",
      "Epoch =  259 Batch =  0 Loss =  13.869600089670673 Gradient_max =  0.002796692001259858 learning rate ratio =  0.00013600763301664445\n",
      "Epoch =  260 Batch =  0 Loss =  13.869596291674931 Gradient_max =  0.0027997812410945583 learning rate ratio =  0.00013659101460980185\n",
      "Epoch =  261 Batch =  0 Loss =  13.869592496543406 Gradient_max =  0.0028028714467036348 learning rate ratio =  0.00013717864154883086\n",
      "Epoch =  262 Batch =  0 Loss =  13.869588704274287 Gradient_max =  0.0028059626193183704 learning rate ratio =  0.00013777056097618494\n",
      "Epoch =  263 Batch =  0 Loss =  13.869584914865776 Gradient_max =  0.0028090547601704033 learning rate ratio =  0.00013836682073335585\n",
      "Epoch =  264 Batch =  0 Loss =  13.869581128185155 Gradient_max =  0.002812147870637213 learning rate ratio =  0.00013896746938108786\n",
      "Epoch =  265 Batch =  0 Loss =  13.869577344361302 Gradient_max =  0.002815241951805932 learning rate ratio =  0.00013957255619116555\n",
      "Epoch =  266 Batch =  0 Loss =  13.869573563226414 Gradient_max =  0.0028183370046368675 learning rate ratio =  0.00014018213116787678\n",
      "Epoch =  267 Batch =  0 Loss =  13.86956978498063 Gradient_max =  0.0028214330305447325 learning rate ratio =  0.00014079624509084898\n",
      "Epoch =  268 Batch =  0 Loss =  13.869566009586679 Gradient_max =  0.002824530030853376 learning rate ratio =  0.0001414149495023224\n",
      "Epoch =  269 Batch =  0 Loss =  13.869562237042777 Gradient_max =  0.002827628006796548 learning rate ratio =  0.00014203829672167366\n",
      "Epoch =  270 Batch =  0 Loss =  13.869558467347119 Gradient_max =  0.0028307269596083476 learning rate ratio =  0.00014266633986477573\n",
      "Epoch =  271 Batch =  0 Loss =  13.86955470049792 Gradient_max =  0.0028338268905232283 learning rate ratio =  0.00014329913285921717\n",
      "Epoch =  272 Batch =  0 Loss =  13.869550936493392 Gradient_max =  0.002836927800776001 learning rate ratio =  0.0001439367304598707\n",
      "Epoch =  273 Batch =  0 Loss =  13.869547175351757 Gradient_max =  0.002840029691581744 learning rate ratio =  0.00014457918826379264\n",
      "Epoch =  274 Batch =  0 Loss =  13.86954341705115 Gradient_max =  0.0028431325641961333 learning rate ratio =  0.00014522656272959787\n",
      "Epoch =  275 Batch =  0 Loss =  13.869539661589775 Gradient_max =  0.0028462364198550446 learning rate ratio =  0.00014587891119107098\n",
      "Epoch =  276 Batch =  0 Loss =  13.869535908965855 Gradient_max =  0.0028493412597947144 learning rate ratio =  0.00014653629187525203\n",
      "Epoch =  277 Batch =  0 Loss =  13.869532159070163 Gradient_max =  0.0028524470852491486 learning rate ratio =  0.00014719876391978504\n",
      "Epoch =  278 Batch =  0 Loss =  13.869528412008695 Gradient_max =  0.002855553897457679 learning rate ratio =  0.0001478663873913564\n",
      "Epoch =  279 Batch =  0 Loss =  13.869524667723931 Gradient_max =  0.0028586616977450893 learning rate ratio =  0.00014939446571315853\n",
      "Epoch =  280 Batch =  0 Loss =  13.86952092626987 Gradient_max =  0.0028617704872615584 learning rate ratio =  0.00015202929597460456\n",
      "Epoch =  281 Batch =  0 Loss =  13.869517187644739 Gradient_max =  0.0028648802672451104 learning rate ratio =  0.0001547557301597199\n",
      "Epoch =  282 Batch =  0 Loss =  13.869513451803686 Gradient_max =  0.0028679910389677373 learning rate ratio =  0.00015757863296039036\n",
      "Epoch =  283 Batch =  0 Loss =  13.869509718787958 Gradient_max =  0.002871102803634627 learning rate ratio =  0.00016050321973725315\n",
      "Epoch =  284 Batch =  0 Loss =  13.86950598859579 Gradient_max =  0.002874215562484883 learning rate ratio =  0.0001635350886775016\n",
      "Epoch =  285 Batch =  0 Loss =  13.869502261252677 Gradient_max =  0.002877329316731691 learning rate ratio =  0.0001666802565779785\n",
      "Epoch =  286 Batch =  0 Loss =  13.86949853672962 Gradient_max =  0.0028804440676410592 learning rate ratio =  0.00016994519873806066\n",
      "Epoch =  287 Batch =  0 Loss =  13.869494815024867 Gradient_max =  0.0028835598164531776 learning rate ratio =  0.00017333689346141237\n",
      "Epoch =  288 Batch =  0 Loss =  13.86949109613664 Gradient_max =  0.002886676564408596 learning rate ratio =  0.00017686287187264425\n",
      "Epoch =  289 Batch =  0 Loss =  13.869487379963433 Gradient_max =  0.0028897943127430916 learning rate ratio =  0.0001805312737648668\n",
      "Epoch =  290 Batch =  0 Loss =  13.869483666603664 Gradient_max =  0.002892913062702837 learning rate ratio =  0.00018435091036330238\n",
      "Epoch =  291 Batch =  0 Loss =  13.86947995605558 Gradient_max =  0.002896032815529474 learning rate ratio =  0.00018833133501003517\n",
      "Epoch =  292 Batch =  0 Loss =  13.869476248317419 Gradient_max =  0.002899153572465009 learning rate ratio =  0.00019248292297580947\n",
      "Epoch =  293 Batch =  0 Loss =  13.869472543387419 Gradient_max =  0.002902275334751815 learning rate ratio =  0.00019681696179714404\n",
      "Epoch =  294 Batch =  0 Loss =  13.869468841278225 Gradient_max =  0.0029053981035092416 learning rate ratio =  0.00020134575380708055\n",
      "Epoch =  295 Batch =  0 Loss =  13.869465141866918 Gradient_max =  0.002908521880089861 learning rate ratio =  0.00020608273275894256\n",
      "Epoch =  296 Batch =  0 Loss =  13.869461445259395 Gradient_max =  0.002911646665750449 learning rate ratio =  0.00021104259701000227\n",
      "Epoch =  297 Batch =  0 Loss =  13.869457751358906 Gradient_max =  0.0029147724617630336 learning rate ratio =  0.0002162414618945677\n",
      "Epoch =  298 Batch =  0 Loss =  13.869454060080397 Gradient_max =  0.0029178992691683927 learning rate ratio =  0.00022169703460457108\n",
      "Epoch =  299 Batch =  0 Loss =  13.869450371542154 Gradient_max =  0.0029210270892622596 learning rate ratio =  0.00022742881590467754\n",
      "Epoch =  300 Batch =  0 Loss =  13.869446685801847 Gradient_max =  0.002924155923412906 learning rate ratio =  0.00023345833304589606\n",
      "Epoch =  301 Batch =  0 Loss =  13.86944300285773 Gradient_max =  0.002927285772865643 learning rate ratio =  0.0002398094099443465\n",
      "Epoch =  302 Batch =  0 Loss =  13.869439322729553 Gradient_max =  0.002930416638883747 learning rate ratio =  0.00024650848189446573\n",
      "Epoch =  303 Batch =  0 Loss =  13.869435645393894 Gradient_max =  0.0029335485226955884 learning rate ratio =  0.00025358496348531996\n",
      "Epoch =  304 Batch =  0 Loss =  13.869431970849018 Gradient_max =  0.002936681425547593 learning rate ratio =  0.000261071680441394\n",
      "Epoch =  305 Batch =  0 Loss =  13.86942829909317 Gradient_max =  0.002939815348686547 learning rate ratio =  0.0002690053788769486\n",
      "Epoch =  306 Batch =  0 Loss =  13.869424630117418 Gradient_max =  0.0029429504224366748 learning rate ratio =  0.000277462126422137\n",
      "Epoch =  307 Batch =  0 Loss =  13.869420963927183 Gradient_max =  0.0029460865230596053 learning rate ratio =  0.00028645817912318033\n",
      "Epoch =  308 Batch =  0 Loss =  13.869417300520725 Gradient_max =  0.002949223651807368 learning rate ratio =  0.0002960468349573625\n",
      "Epoch =  309 Batch =  0 Loss =  13.86941363992704 Gradient_max =  0.002952361790269859 learning rate ratio =  0.0003063152834428688\n",
      "Epoch =  310 Batch =  0 Loss =  13.869409982113654 Gradient_max =  0.002955500963469698 learning rate ratio =  0.00031730985886050875\n",
      "Epoch =  311 Batch =  0 Loss =  13.869406327078838 Gradient_max =  0.002958641172664272 learning rate ratio =  0.00032911041772615144\n",
      "Epoch =  312 Batch =  0 Loss =  13.86940267482086 Gradient_max =  0.002961782419111353 learning rate ratio =  0.00034180897185032957\n",
      "Epoch =  313 Batch =  0 Loss =  13.869399025337993 Gradient_max =  0.002964924704069092 learning rate ratio =  0.0003555120925889639\n",
      "Epoch =  314 Batch =  0 Loss =  13.8693953786285 Gradient_max =  0.002968068028796019 learning rate ratio =  0.0003703439092556236\n",
      "Epoch =  315 Batch =  0 Loss =  13.869391734758326 Gradient_max =  0.002971212465097132 learning rate ratio =  0.0003864764224787872\n",
      "Epoch =  316 Batch =  0 Loss =  13.86938809368062 Gradient_max =  0.002974357978028669 learning rate ratio =  0.00040406924511419257\n",
      "Epoch =  317 Batch =  0 Loss =  13.869384455271522 Gradient_max =  0.0029775045467717873 learning rate ratio =  0.0004233194770411778\n",
      "Epoch =  318 Batch =  0 Loss =  13.869380819629022 Gradient_max =  0.0029806521727175025 learning rate ratio =  0.0004444729329087723\n",
      "Epoch =  319 Batch =  0 Loss =  13.86937718675139 Gradient_max =  0.0029838008571389006 learning rate ratio =  0.0004678265652137787\n",
      "Epoch =  320 Batch =  0 Loss =  13.86937355663692 Gradient_max =  0.00298695060130946 learning rate ratio =  0.0004937424919912173\n",
      "Epoch =  321 Batch =  0 Loss =  13.86936992928388 Gradient_max =  0.002990101406503048 learning rate ratio =  0.000522666909959163\n",
      "Epoch =  322 Batch =  0 Loss =  13.869366304690564 Gradient_max =  0.002993253273993935 learning rate ratio =  0.0005551560006273\n",
      "Epoch =  323 Batch =  0 Loss =  13.869362682683386 Gradient_max =  0.0029964062052542854 learning rate ratio =  0.0005919120441324379\n",
      "Epoch =  324 Batch =  0 Loss =  13.869359063432269 Gradient_max =  0.0029995602013618053 learning rate ratio =  0.0006338347622245889\n",
      "Epoch =  325 Batch =  0 Loss =  13.869355446935486 Gradient_max =  0.003002715263591945 learning rate ratio =  0.0006820959513278191\n",
      "Epoch =  326 Batch =  0 Loss =  13.869351833123412 Gradient_max =  0.0030058713932758067 learning rate ratio =  0.0007382507368503244\n",
      "Epoch =  327 Batch =  0 Loss =  13.869348222093905 Gradient_max =  0.00300902859150117 learning rate ratio =  0.0008044082820181068\n",
      "Epoch =  328 Batch =  0 Loss =  13.869344613651679 Gradient_max =  0.0030121868596802797 learning rate ratio =  0.0008835026423557401\n",
      "Epoch =  329 Batch =  0 Loss =  13.869341007957305 Gradient_max =  0.0030153461990877622 learning rate ratio =  0.0009797397371426111\n",
      "Epoch =  330 Batch =  0 Loss =  13.869337405009077 Gradient_max =  0.003018506611001061 learning rate ratio =  0.001099370276456839\n",
      "Epoch =  331 Batch =  0 Loss =  13.869333804794277 Gradient_max =  0.0030216680967086306 learning rate ratio =  0.001252104336420716\n",
      "Epoch =  332 Batch =  0 Loss =  13.869330207272693 Gradient_max =  0.0030248306575141296 learning rate ratio =  0.0014538888158969635\n",
      "Epoch =  333 Batch =  0 Loss =  13.869326612492191 Gradient_max =  0.0030279942946601266 learning rate ratio =  0.001732872744040675\n",
      "Epoch =  334 Batch =  0 Loss =  13.869323020451066 Gradient_max =  0.0030311590094256645 learning rate ratio =  0.0021438388628392817\n",
      "Epoch =  335 Batch =  0 Loss =  13.869319431147622 Gradient_max =  0.0030343248030901835 learning rate ratio =  0.0028094632145927124\n",
      "Epoch =  336 Batch =  0 Loss =  13.869315844580163 Gradient_max =  0.003037491676933529 learning rate ratio =  0.00407270774025417\n",
      "Epoch =  337 Batch =  0 Loss =  13.86931226075068 Gradient_max =  0.003040659632238849 learning rate ratio =  0.007394057678510528\n",
      "Epoch =  338 Batch =  0 Loss =  13.86930867965379 Gradient_max =  0.003043828670283902 learning rate ratio =  0.03990398284624376\n",
      "Epoch =  339 Batch =  0 Loss =  13.8693051012878 Gradient_max =  0.003046998792349742 learning rate ratio =  0.00020129289947186702\n",
      "Epoch =  340 Batch =  0 Loss =  13.869301525651016 Gradient_max =  0.0030501699997178278 learning rate ratio =  0.00020244376424719487\n",
      "Epoch =  341 Batch =  0 Loss =  13.86929795274175 Gradient_max =  0.0030533422936700155 learning rate ratio =  0.00020360648162221203\n",
      "Epoch =  342 Batch =  0 Loss =  13.869294382558312 Gradient_max =  0.0030565156754885753 learning rate ratio =  0.000204781236678353\n",
      "Epoch =  343 Batch =  0 Loss =  13.86929081511078 Gradient_max =  0.0030596902122362853 learning rate ratio =  0.00020596586181732422\n",
      "Epoch =  344 Batch =  0 Loss =  13.869287250385705 Gradient_max =  0.0030628658408249675 learning rate ratio =  0.0002071628540607292\n",
      "Epoch =  345 Batch =  0 Loss =  13.869283688381403 Gradient_max =  0.0030660425625394215 learning rate ratio =  0.00020837240914083882\n",
      "Epoch =  346 Batch =  0 Loss =  13.869280129096186 Gradient_max =  0.0030692203786648583 learning rate ratio =  0.00020959472693963456\n",
      "Epoch =  347 Batch =  0 Loss =  13.869276572528383 Gradient_max =  0.0030723992904868923 learning rate ratio =  0.0002108300115993731\n",
      "Epoch =  348 Batch =  0 Loss =  13.869273018676303 Gradient_max =  0.0030755792992915487 learning rate ratio =  0.00021207847163670434\n",
      "Epoch =  349 Batch =  0 Loss =  13.869269467548706 Gradient_max =  0.003078760406380165 learning rate ratio =  0.0002133403200615126\n",
      "Epoch =  350 Batch =  0 Loss =  13.869265919133447 Gradient_max =  0.0030819426130247445 learning rate ratio =  0.00021461577449546156\n",
      "Epoch =  351 Batch =  0 Loss =  13.86926237342885 Gradient_max =  0.003085125920512544 learning rate ratio =  0.00021590505730067325\n",
      "Epoch =  352 Batch =  0 Loss =  13.869258830433239 Gradient_max =  0.0030883103301312252 learning rate ratio =  0.0002172083957084429\n",
      "Epoch =  353 Batch =  0 Loss =  13.869255290168077 Gradient_max =  0.003091495843119074 learning rate ratio =  0.00021852602194972947\n",
      "Epoch =  354 Batch =  0 Loss =  13.869251752608612 Gradient_max =  0.0030946824608142766 learning rate ratio =  0.00021985817340415244\n",
      "Epoch =  355 Batch =  0 Loss =  13.869248217753187 Gradient_max =  0.003097870184505733 learning rate ratio =  0.0002212050927326105\n",
      "Epoch =  356 Batch =  0 Loss =  13.869244685600124 Gradient_max =  0.0031010590154827576 learning rate ratio =  0.00022256702802865485\n",
      "Epoch =  357 Batch =  0 Loss =  13.869241156097253 Gradient_max =  0.003104248955087346 learning rate ratio =  0.00022394423297497106\n",
      "Epoch =  358 Batch =  0 Loss =  13.869237629309286 Gradient_max =  0.003107440004521483 learning rate ratio =  0.00022533696698739817\n",
      "Epoch =  359 Batch =  0 Loss =  13.869234105226042 Gradient_max =  0.003110632164902464 learning rate ratio =  0.00022674549538304723\n",
      "Epoch =  360 Batch =  0 Loss =  13.869230583839032 Gradient_max =  0.0031138254377301067 learning rate ratio =  0.00022817008958276802\n",
      "Epoch =  361 Batch =  0 Loss =  13.869227065106342 Gradient_max =  0.0031170198243110577 learning rate ratio =  0.00022961102724487683\n",
      "Epoch =  362 Batch =  0 Loss =  13.869223549026266 Gradient_max =  0.0031202153259371258 learning rate ratio =  0.00023106859245877121\n",
      "Epoch =  363 Batch =  0 Loss =  13.869220035670418 Gradient_max =  0.003123411957875311 learning rate ratio =  0.00023254249640944005\n",
      "Epoch =  364 Batch =  0 Loss =  13.869216525021521 Gradient_max =  0.0031266099205839656 learning rate ratio =  0.00023402412982446663\n",
      "Epoch =  365 Batch =  0 Loss =  13.869213017019419 Gradient_max =  0.0031298090103795773 learning rate ratio =  0.0002355230349472233\n",
      "Epoch =  366 Batch =  0 Loss =  13.869209511671814 Gradient_max =  0.0031330092285881427 learning rate ratio =  0.00023703951680903503\n",
      "Epoch =  367 Batch =  0 Loss =  13.869206009009218 Gradient_max =  0.003136210576587398 learning rate ratio =  0.00023857388766724007\n",
      "Epoch =  368 Batch =  0 Loss =  13.869202509029973 Gradient_max =  0.003139413055679741 learning rate ratio =  0.00024012646721068793\n",
      "Epoch =  369 Batch =  0 Loss =  13.869199011732427 Gradient_max =  0.0031426166671679997 learning rate ratio =  0.00024169758278808821\n",
      "Epoch =  370 Batch =  0 Loss =  13.869195517114939 Gradient_max =  0.003145821412355426 learning rate ratio =  0.0002432875696388167\n",
      "Epoch =  371 Batch =  0 Loss =  13.869192025175858 Gradient_max =  0.0031490272925456973 learning rate ratio =  0.00024489677113211593\n",
      "Epoch =  372 Batch =  0 Loss =  13.869188535913533 Gradient_max =  0.003152234309042924 learning rate ratio =  0.0002465255390150529\n",
      "Epoch =  373 Batch =  0 Loss =  13.869185049246385 Gradient_max =  0.0031554424629813556 learning rate ratio =  0.00024817423365620174\n",
      "Epoch =  374 Batch =  0 Loss =  13.869181565252939 Gradient_max =  0.0031586517558360554 learning rate ratio =  0.0002498432243522334\n",
      "Epoch =  375 Batch =  0 Loss =  13.869178083931544 Gradient_max =  0.0031618621889124192 learning rate ratio =  0.00025153288956471985\n",
      "Epoch =  376 Batch =  0 Loss =  13.869174605280566 Gradient_max =  0.003165073763516269 learning rate ratio =  0.00025324361722023597\n",
      "Epoch =  377 Batch =  0 Loss =  13.869171129298362 Gradient_max =  0.003168286480953857 learning rate ratio =  0.0002549758050081174\n",
      "Epoch =  378 Batch =  0 Loss =  13.869167655983293 Gradient_max =  0.0031715003425318703 learning rate ratio =  0.0002567298606895333\n",
      "Epoch =  379 Batch =  0 Loss =  13.869164185221662 Gradient_max =  0.0031747153494834607 learning rate ratio =  0.0002585062024123249\n",
      "Epoch =  380 Batch =  0 Loss =  13.869160717123684 Gradient_max =  0.0031779315031904352 learning rate ratio =  0.0002603052590622856\n",
      "Epoch =  381 Batch =  0 Loss =  13.869157251643946 Gradient_max =  0.003181148804993632 learning rate ratio =  0.00026212747059408813\n",
      "Epoch =  382 Batch =  0 Loss =  13.869153788833303 Gradient_max =  0.003184367256165138 learning rate ratio =  0.0002639732883884461\n",
      "Epoch =  383 Batch =  0 Loss =  13.869150328697355 Gradient_max =  0.0031875868580214317 learning rate ratio =  0.0002658431756350781\n",
      "Epoch =  384 Batch =  0 Loss =  13.869146871218407 Gradient_max =  0.0031908076118676953 learning rate ratio =  0.0002677376077167139\n",
      "Epoch =  385 Batch =  0 Loss =  13.869143416438044 Gradient_max =  0.0031940295189671667 learning rate ratio =  0.0002696570726103743\n",
      "Epoch =  386 Batch =  0 Loss =  13.869139964311511 Gradient_max =  0.0031972525806763788 learning rate ratio =  0.0002716020713188769\n",
      "Epoch =  387 Batch =  0 Loss =  13.869136514837175 Gradient_max =  0.0032004767983059195 learning rate ratio =  0.00027357311829620746\n",
      "Epoch =  388 Batch =  0 Loss =  13.869133068013413 Gradient_max =  0.003203702173166808 learning rate ratio =  0.00027557074190613396\n",
      "Epoch =  389 Batch =  0 Loss =  13.869129623838605 Gradient_max =  0.0032069287065705013 learning rate ratio =  0.00027759548489553047\n",
      "Epoch =  390 Batch =  0 Loss =  13.869126182362367 Gradient_max =  0.0032101563998092456 learning rate ratio =  0.0002796479048854304\n",
      "Epoch =  391 Batch =  0 Loss =  13.869122743531818 Gradient_max =  0.003213385254215054 learning rate ratio =  0.0002817285748893477\n",
      "Epoch =  392 Batch =  0 Loss =  13.869119307345343 Gradient_max =  0.003216615271100698 learning rate ratio =  0.00028383808384275995\n",
      "Epoch =  393 Batch =  0 Loss =  13.869115873801325 Gradient_max =  0.0032198464517793875 learning rate ratio =  0.0002859770371618045\n",
      "Epoch =  394 Batch =  0 Loss =  13.86911244294474 Gradient_max =  0.003223078797553707 learning rate ratio =  0.00028814605732285277\n",
      "Epoch =  395 Batch =  0 Loss =  13.869109015314638 Gradient_max =  0.003252931478160437 learning rate ratio =  0.0002927420267208521\n",
      "Epoch =  396 Batch =  0 Loss =  13.869105591231 Gradient_max =  0.003256193100310256 learning rate ratio =  0.0002949922724642954\n",
      "Epoch =  397 Batch =  0 Loss =  13.869102169784757 Gradient_max =  0.003259455904423831 learning rate ratio =  0.000297274847624947\n",
      "Epoch =  398 Batch =  0 Loss =  13.869098750844323 Gradient_max =  0.0032627198919487495 learning rate ratio =  0.0002995904557360137\n",
      "Epoch =  399 Batch =  0 Loss =  13.869095334537919 Gradient_max =  0.0032659850640966324 learning rate ratio =  0.0003019398208663236\n",
      "Epoch =  400 Batch =  0 Loss =  13.869091920762605 Gradient_max =  0.0032692514221823927 learning rate ratio =  0.00030432368840763304\n",
      "Epoch =  401 Batch =  0 Loss =  13.869088509494501 Gradient_max =  0.003272518967697787 learning rate ratio =  0.00030674282587217976\n",
      "Epoch =  402 Batch =  0 Loss =  13.869085100856324 Gradient_max =  0.0032757877018276347 learning rate ratio =  0.00030919802367242996\n",
      "Epoch =  403 Batch =  0 Loss =  13.869081694846477 Gradient_max =  0.0032790576259034802 learning rate ratio =  0.00031169009602591546\n",
      "Epoch =  404 Batch =  0 Loss =  13.869078291463346 Gradient_max =  0.0032823287412573235 learning rate ratio =  0.00031421988184389356\n",
      "Epoch =  405 Batch =  0 Loss =  13.869074890705333 Gradient_max =  0.003285601049221615 learning rate ratio =  0.000316788245674653\n",
      "Epoch =  406 Batch =  0 Loss =  13.869071492524034 Gradient_max =  0.0032888745511688243 learning rate ratio =  0.0003193960786942458\n",
      "Epoch =  407 Batch =  0 Loss =  13.869068096964616 Gradient_max =  0.0032921492483927767 learning rate ratio =  0.0003220442997278397\n",
      "Epoch =  408 Batch =  0 Loss =  13.86906470402549 Gradient_max =  0.0032954251422272745 learning rate ratio =  0.00032473385634238487\n",
      "Epoch =  409 Batch =  0 Loss =  13.869061313705057 Gradient_max =  0.003298702234006579 learning rate ratio =  0.00032746572597524995\n",
      "Epoch =  410 Batch =  0 Loss =  13.869057925908153 Gradient_max =  0.0033019805250307766 learning rate ratio =  0.0003302409171172845\n",
      "Epoch =  411 Batch =  0 Loss =  13.869054540755226 Gradient_max =  0.0033052601260354047 learning rate ratio =  0.0003330509400714306\n",
      "Epoch =  412 Batch =  0 Loss =  13.869051158216697 Gradient_max =  0.003308540932175468 learning rate ratio =  0.0003359060808330579\n",
      "Epoch =  413 Batch =  0 Loss =  13.869047778306868 Gradient_max =  0.003311822944834906 learning rate ratio =  0.0003388074369453378\n",
      "Epoch =  414 Batch =  0 Loss =  13.869044401008395 Gradient_max =  0.0033151061653083295 learning rate ratio =  0.0003417561418263212\n",
      "Epoch =  415 Batch =  0 Loss =  13.869041026319685 Gradient_max =  0.003318390594935728 learning rate ratio =  0.00034475336626117514\n",
      "Epoch =  416 Batch =  0 Loss =  13.869037654239161 Gradient_max =  0.003321676235057544 learning rate ratio =  0.00034780031994998176\n",
      "Epoch =  417 Batch =  0 Loss =  13.869034284765233 Gradient_max =  0.0033249630870146873 learning rate ratio =  0.0003508982531381316\n",
      "Epoch =  418 Batch =  0 Loss =  13.869030917896314 Gradient_max =  0.0033282511521485316 learning rate ratio =  0.0003540484583293781\n",
      "Epoch =  419 Batch =  0 Loss =  13.869027553656274 Gradient_max =  0.0033315404357653075 learning rate ratio =  0.0003572521146861386\n",
      "Epoch =  420 Batch =  0 Loss =  13.869024192018081 Gradient_max =  0.0033348309383601548 learning rate ratio =  0.0003605107568478295\n",
      "Epoch =  421 Batch =  0 Loss =  13.869020833016355 Gradient_max =  0.0033381226612338114 learning rate ratio =  0.00036382581505859323\n",
      "Epoch =  422 Batch =  0 Loss =  13.86901747661322 Gradient_max =  0.0033414156057775474 learning rate ratio =  0.00036719876960341404\n",
      "Epoch =  423 Batch =  0 Loss =  13.869014122807103 Gradient_max =  0.0033447097733380208 learning rate ratio =  0.0003706311530005224\n",
      "Epoch =  424 Batch =  0 Loss =  13.869010771596429 Gradient_max =  0.0033480051652623635 learning rate ratio =  0.00037412455233105127\n",
      "Epoch =  425 Batch =  0 Loss =  13.86900742297962 Gradient_max =  0.003351301782898172 learning rate ratio =  0.00037768061168914023\n",
      "Epoch =  426 Batch =  0 Loss =  13.869004076955106 Gradient_max =  0.003354599627593509 learning rate ratio =  0.00038130103476526013\n",
      "Epoch =  427 Batch =  0 Loss =  13.869000733521315 Gradient_max =  0.003357898700696903 learning rate ratio =  0.0003849875875712871\n",
      "Epoch =  428 Batch =  0 Loss =  13.868997392592538 Gradient_max =  0.0033611990035587393 learning rate ratio =  0.0003887421013166125\n",
      "Epoch =  429 Batch =  0 Loss =  13.868994054251543 Gradient_max =  0.0033645005375269607 learning rate ratio =  0.0003925664754444186\n",
      "Epoch =  430 Batch =  0 Loss =  13.86899071854745 Gradient_max =  0.003367803303929638 learning rate ratio =  0.00039646268083769854\n",
      "Epoch =  431 Batch =  0 Loss =  13.868987385428301 Gradient_max =  0.0033711073041388915 learning rate ratio =  0.0004004327632174202\n",
      "Epoch =  432 Batch =  0 Loss =  13.868984054892534 Gradient_max =  0.0033744125395056036 learning rate ratio =  0.0004044788467207186\n",
      "Epoch =  433 Batch =  0 Loss =  13.868980726987008 Gradient_max =  0.003377719025879353 learning rate ratio =  0.00040860135968543056\n",
      "Epoch =  434 Batch =  0 Loss =  13.868977401661757 Gradient_max =  0.003381026752897943 learning rate ratio =  0.0004128043012595242\n",
      "Epoch =  435 Batch =  0 Loss =  13.86897407891522 Gradient_max =  0.003384335721916299 learning rate ratio =  0.0004170900512874905\n",
      "Epoch =  436 Batch =  0 Loss =  13.868970758661092 Gradient_max =  0.0033876459342123107 learning rate ratio =  0.00042146108442302693\n",
      "Epoch =  437 Batch =  0 Loss =  13.86896744098261 Gradient_max =  0.00339095739121948 learning rate ratio =  0.0004259199749276153\n",
      "Epoch =  438 Batch =  0 Loss =  13.868964125878225 Gradient_max =  0.0033942700942941593 learning rate ratio =  0.0004304694017025576\n",
      "Epoch =  439 Batch =  0 Loss =  13.868960813327043 Gradient_max =  0.0033975840449094537 learning rate ratio =  0.00043511215368561684\n",
      "Epoch =  440 Batch =  0 Loss =  13.868957503346886 Gradient_max =  0.003400899244306407 learning rate ratio =  0.0004398511355136884\n",
      "Epoch =  441 Batch =  0 Loss =  13.868954195936206 Gradient_max =  0.0034042156938428017 learning rate ratio =  0.00044468937363259953\n",
      "Epoch =  442 Batch =  0 Loss =  13.868950891093451 Gradient_max =  0.0034075333948769015 learning rate ratio =  0.0004496300227337652\n",
      "Epoch =  443 Batch =  0 Loss =  13.86894758859235 Gradient_max =  0.003410852348797225 learning rate ratio =  0.0004546763726240452\n",
      "Epoch =  444 Batch =  0 Loss =  13.868944288656929 Gradient_max =  0.003414172556932752 learning rate ratio =  0.0004598318555259708\n",
      "Epoch =  445 Batch =  0 Loss =  13.868940991285633 Gradient_max =  0.003417494020643184 learning rate ratio =  0.00046510005387987965\n",
      "Epoch =  446 Batch =  0 Loss =  13.86893769647692 Gradient_max =  0.003420816741288705 learning rate ratio =  0.00047048470864945706\n",
      "Epoch =  447 Batch =  0 Loss =  13.868934404229242 Gradient_max =  0.0034241407202299777 learning rate ratio =  0.0004759897281881853\n",
      "Epoch =  448 Batch =  0 Loss =  13.868931114541065 Gradient_max =  0.0034274659588281547 learning rate ratio =  0.0004816191977076221\n",
      "Epoch =  449 Batch =  0 Loss =  13.868927827410834 Gradient_max =  0.0034307924584448626 learning rate ratio =  0.00048737738939570664\n",
      "Epoch =  450 Batch =  0 Loss =  13.868924542837016 Gradient_max =  0.0034341202204422115 learning rate ratio =  0.0004932687732377699\n",
      "Epoch =  451 Batch =  0 Loss =  13.868921260818068 Gradient_max =  0.0034374492461828015 learning rate ratio =  0.0004992980285978485\n",
      "Epoch =  452 Batch =  0 Loss =  13.868917981366288 Gradient_max =  0.003440779378659521 learning rate ratio =  0.0005055010151433659\n",
      "Epoch =  453 Batch =  0 Loss =  13.86891470446631 Gradient_max =  0.003444110778675893 learning rate ratio =  0.0005118534421629642\n",
      "Epoch =  454 Batch =  0 Loss =  13.868911430116587 Gradient_max =  0.003447443447597078 learning rate ratio =  0.0005183607802070059\n",
      "Epoch =  455 Batch =  0 Loss =  13.868908158315591 Gradient_max =  0.0034507773867887225 learning rate ratio =  0.0005250287701259958\n",
      "Epoch =  456 Batch =  0 Loss =  13.868904889230748 Gradient_max =  0.0034541125974301176 learning rate ratio =  0.0005318634399449635\n",
      "Epoch =  457 Batch =  0 Loss =  13.868901622697487 Gradient_max =  0.0034574491960086403 learning rate ratio =  0.0005388461619279009\n",
      "Epoch =  458 Batch =  0 Loss =  13.86889835870855 Gradient_max =  0.0034607870731718604 learning rate ratio =  0.0005460072428120948\n",
      "Epoch =  459 Batch =  0 Loss =  13.868895097262419 Gradient_max =  0.0034641262302912324 learning rate ratio =  0.0005533536094761076\n",
      "Epoch =  460 Batch =  0 Loss =  13.868891838242526 Gradient_max =  0.0034674666686901868 learning rate ratio =  0.0005608925521724147\n",
      "Epoch =  461 Batch =  0 Loss =  13.868888581762405 Gradient_max =  0.003470808389789629 learning rate ratio =  0.0005686317486961647\n",
      "Epoch =  462 Batch =  0 Loss =  13.86888532782054 Gradient_max =  0.0034741513949624963 learning rate ratio =  0.0005765792904574781\n",
      "Epoch =  463 Batch =  0 Loss =  13.86888207641539 Gradient_max =  0.0034774956855822188 learning rate ratio =  0.0005847437107237278\n",
      "Epoch =  464 Batch =  0 Loss =  13.86887882754545 Gradient_max =  0.0034808412630227188 learning rate ratio =  0.0005931340151983096\n",
      "Epoch =  465 Batch =  0 Loss =  13.868875581209187 Gradient_max =  0.003484188128658421 learning rate ratio =  0.0006017597151745074\n",
      "Epoch =  466 Batch =  0 Loss =  13.868872337405083 Gradient_max =  0.003487536283864241 learning rate ratio =  0.0006106308635210384\n",
      "Epoch =  467 Batch =  0 Loss =  13.8688690961926 Gradient_max =  0.003490885658703668 learning rate ratio =  0.0006197793187664082\n",
      "Epoch =  468 Batch =  0 Loss =  13.868865857461763 Gradient_max =  0.0034942363298430365 learning rate ratio =  0.0006291963907784983\n",
      "Epoch =  469 Batch =  0 Loss =  13.868862621259327 Gradient_max =  0.0034975882986506084 learning rate ratio =  0.0006388940926362682\n",
      "Epoch =  470 Batch =  0 Loss =  13.86885938758378 Gradient_max =  0.0035009415665069933 learning rate ratio =  0.0006488851645767717\n",
      "Epoch =  471 Batch =  0 Loss =  13.868856156317053 Gradient_max =  0.003504296134763713 learning rate ratio =  0.0006591831298519367\n",
      "Epoch =  472 Batch =  0 Loss =  13.868852927574526 Gradient_max =  0.003507652004831775 learning rate ratio =  0.0006698023558394842\n",
      "Epoch =  473 Batch =  0 Loss =  13.868849701354687 Gradient_max =  0.0035110091780932995 learning rate ratio =  0.0006807581209315372\n",
      "Epoch =  474 Batch =  0 Loss =  13.868846477656025 Gradient_max =  0.0035143676559309073 learning rate ratio =  0.0006920666879122218\n",
      "Epoch =  475 Batch =  0 Loss =  13.868843256546596 Gradient_max =  0.0035177274396324164 learning rate ratio =  0.0007037453845173656\n",
      "Epoch =  476 Batch =  0 Loss =  13.868840037863768 Gradient_max =  0.0035210885306356785 learning rate ratio =  0.000715812692109116\n",
      "Epoch =  477 Batch =  0 Loss =  13.868836821614124 Gradient_max =  0.0035244509304528005 learning rate ratio =  0.0007282883432881375\n",
      "Epoch =  478 Batch =  0 Loss =  13.868833607879813 Gradient_max =  0.0035278146403814005 learning rate ratio =  0.000741193429544279\n",
      "Epoch =  479 Batch =  0 Loss =  13.86883039665932 Gradient_max =  0.003531179661806626 learning rate ratio =  0.0007545505203502419\n",
      "Epoch =  480 Batch =  0 Loss =  13.868827188010766 Gradient_max =  0.003534545996046072 learning rate ratio =  0.0007683837948963112\n",
      "Epoch =  481 Batch =  0 Loss =  13.868823981873117 Gradient_max =  0.0035379136445538575 learning rate ratio =  0.0007827191882543056\n",
      "Epoch =  482 Batch =  0 Loss =  13.868820778302775 Gradient_max =  0.0035412826086748473 learning rate ratio =  0.0007975845536560345\n",
      "Epoch =  483 Batch =  0 Loss =  13.868817577153594 Gradient_max =  0.0035446528900007334 learning rate ratio =  0.0008130098432521912\n",
      "Epoch =  484 Batch =  0 Loss =  13.868814378510743 Gradient_max =  0.0035480244897565945 learning rate ratio =  0.0008290273094678836\n",
      "Epoch =  485 Batch =  0 Loss =  13.868811182372738 Gradient_max =  0.0035513974093306306 learning rate ratio =  0.0008456717302818098\n",
      "Epoch =  486 Batch =  0 Loss =  13.868807988738075 Gradient_max =  0.003554771650111555 learning rate ratio =  0.0008629806613259772\n",
      "Epoch =  487 Batch =  0 Loss =  13.868804797608165 Gradient_max =  0.0035581473186359265 learning rate ratio =  0.0008809362104161876\n",
      "Epoch =  488 Batch =  0 Loss =  13.868801609034596 Gradient_max =  0.003561524316112969 learning rate ratio =  0.0008996359109837385\n",
      "Epoch =  489 Batch =  0 Loss =  13.868798422981213 Gradient_max =  0.0035649026440356933 learning rate ratio =  0.000919127014696243\n",
      "Epoch =  490 Batch =  0 Loss =  13.868795239424925 Gradient_max =  0.003568282303801219 learning rate ratio =  0.0009394608601416246\n",
      "Epoch =  491 Batch =  0 Loss =  13.868792058353913 Gradient_max =  0.0035716632968296586 learning rate ratio =  0.0009606933244893568\n",
      "Epoch =  492 Batch =  0 Loss =  13.86878887977698 Gradient_max =  0.0035750456244930846 learning rate ratio =  0.000982885336346159\n",
      "Epoch =  493 Batch =  0 Loss =  13.86878570369264 Gradient_max =  0.0035784292881883314 learning rate ratio =  0.0010061034598056926\n",
      "Epoch =  494 Batch =  0 Loss =  13.868782530099411 Gradient_max =  0.0035818142893127626 learning rate ratio =  0.0010304205614196475\n",
      "Epoch =  495 Batch =  0 Loss =  13.868779358995814 Gradient_max =  0.0035852006292642555 learning rate ratio =  0.001055916574198186\n",
      "Epoch =  496 Batch =  0 Loss =  13.868776190380363 Gradient_max =  0.0035885883094412072 learning rate ratio =  0.0010826793755087537\n",
      "Epoch =  497 Batch =  0 Loss =  13.868773024251583 Gradient_max =  0.003591977331242534 learning rate ratio =  0.0011108057991881947\n",
      "Epoch =  498 Batch =  0 Loss =  13.868769860607985 Gradient_max =  0.0035953676960676787 learning rate ratio =  0.0011404028064323773\n",
      "Epoch =  499 Batch =  0 Loss =  13.868766699448106 Gradient_max =  0.0035987594053166 learning rate ratio =  0.001171588845303566\n",
      "Epoch =  500 Batch =  0 Loss =  13.86876354077046 Gradient_max =  0.0036021524603897796 learning rate ratio =  0.0012044954352818749\n",
      "Epoch =  501 Batch =  0 Loss =  13.868760384573577 Gradient_max =  0.0036055468626882275 learning rate ratio =  0.0012392690215549324\n",
      "Epoch =  502 Batch =  0 Loss =  13.868757230886446 Gradient_max =  0.0036089426136005826 learning rate ratio =  0.00127607315417569\n",
      "Epoch =  503 Batch =  0 Loss =  13.868754079707516 Gradient_max =  0.00361233971452894 learning rate ratio =  0.001315091060507544\n",
      "Epoch =  504 Batch =  0 Loss =  13.868750931028135 Gradient_max =  0.0036157381668959557 learning rate ratio =  0.0013565286963138302\n",
      "Epoch =  505 Batch =  0 Loss =  13.868747784823846 Gradient_max =  0.0036191379720973975 learning rate ratio =  0.0014006183826958218\n",
      "Epoch =  506 Batch =  0 Loss =  13.868744641093176 Gradient_max =  0.003622539131536894 learning rate ratio =  0.001447623164529096\n",
      "Epoch =  507 Batch =  0 Loss =  13.868741499604724 Gradient_max =  0.0036259416464258735 learning rate ratio =  0.0014978420629539447\n",
      "Epoch =  508 Batch =  0 Loss =  13.8687383605873 Gradient_max =  0.003629345518361463 learning rate ratio =  0.0015516164439018786\n",
      "Epoch =  509 Batch =  0 Loss =  13.868735224039442 Gradient_max =  0.0036334064940480608 learning rate ratio =  0.0016093377885432341\n",
      "Epoch =  510 Batch =  0 Loss =  13.868732089959682 Gradient_max =  0.0036394097605987955 learning rate ratio =  0.0016714572404369333\n",
      "Epoch =  511 Batch =  0 Loss =  13.868728958346566 Gradient_max =  0.0036454141743538495 learning rate ratio =  0.0017384974215358855\n",
      "Epoch =  512 Batch =  0 Loss =  13.868725829198628 Gradient_max =  0.003651419737602174 learning rate ratio =  0.0018110671721706757\n",
      "Epoch =  513 Batch =  0 Loss =  13.868722702514404 Gradient_max =  0.00365742645263308 learning rate ratio =  0.0018898800954843089\n",
      "Epoch =  514 Batch =  0 Loss =  13.868719578292447 Gradient_max =  0.0036634343217362393 learning rate ratio =  0.00197577810341611\n",
      "Epoch =  515 Batch =  0 Loss =  13.868716456531292 Gradient_max =  0.0036694433472016894 learning rate ratio =  0.0020697616120807782\n",
      "Epoch =  516 Batch =  0 Loss =  13.868713337258258 Gradient_max =  0.0036754535313467535 learning rate ratio =  0.002173028685485614\n",
      "Epoch =  517 Batch =  0 Loss =  13.868710220509081 Gradient_max =  0.003681464876551805 learning rate ratio =  0.002287026382130254\n",
      "Epoch =  518 Batch =  0 Loss =  13.868707106230584 Gradient_max =  0.00368747738495896 learning rate ratio =  0.0024135189862827464\n",
      "Epoch =  519 Batch =  0 Loss =  13.868703994447163 Gradient_max =  0.0036934910589616916 learning rate ratio =  0.0025546799761473915\n",
      "Epoch =  520 Batch =  0 Loss =  13.868700885117375 Gradient_max =  0.003699505900782278 learning rate ratio =  0.002713217957640375\n",
      "Epoch =  521 Batch =  0 Loss =  13.868697778151967 Gradient_max =  0.003705521912871664 learning rate ratio =  0.0028925521570267393\n",
      "Epoch =  522 Batch =  0 Loss =  13.868694673637707 Gradient_max =  0.0037115390973641365 learning rate ratio =  0.003097061823988818\n",
      "Epoch =  523 Batch =  0 Loss =  13.868691571573153 Gradient_max =  0.0037175574565526622 learning rate ratio =  0.003332448602200336\n",
      "Epoch =  524 Batch =  0 Loss =  13.868688471956862 Gradient_max =  0.0037235769927305706 learning rate ratio =  0.0036062764393958967\n",
      "Epoch =  525 Batch =  0 Loss =  13.86868537478739 Gradient_max =  0.003729597708191571 learning rate ratio =  0.003928799540867214\n",
      "Epoch =  526 Batch =  0 Loss =  13.868682280063284 Gradient_max =  0.0037356196052297365 learning rate ratio =  0.004314275147567273\n",
      "Epoch =  527 Batch =  0 Loss =  13.868679187717918 Gradient_max =  0.003741642686266904 learning rate ratio =  0.004783128174913865\n",
      "Epoch =  528 Batch =  0 Loss =  13.868676097840988 Gradient_max =  0.003747666953549846 learning rate ratio =  0.00536809814735719\n",
      "Epoch =  529 Batch =  0 Loss =  13.868673010405278 Gradient_max =  0.003753692409294674 learning rate ratio =  0.006115280290654599\n",
      "Epoch =  530 Batch =  0 Loss =  13.868669925409344 Gradient_max =  0.0037597190557969616 learning rate ratio =  0.0071030082334328105\n",
      "Epoch =  531 Batch =  0 Loss =  13.8686668428464 Gradient_max =  0.003765746895191399 learning rate ratio =  0.008465763562962142\n",
      "Epoch =  532 Batch =  0 Loss =  13.868663762720326 Gradient_max =  0.003771775929935408 learning rate ratio =  0.01047321700807056\n",
      "Epoch =  533 Batch =  0 Loss =  13.868660685029687 Gradient_max =  0.0037778061623256988 learning rate ratio =  0.013724611202107637\n",
      "Epoch =  534 Batch =  0 Loss =  13.86865760985779 Gradient_max =  0.0037838375947687247 learning rate ratio =  0.019895238576513763\n",
      "Epoch =  535 Batch =  0 Loss =  13.868654537118564 Gradient_max =  0.003789870229452719 learning rate ratio =  0.036119305293586645\n",
      "Epoch =  536 Batch =  0 Loss =  13.86865146681058 Gradient_max =  0.0037959040686755315 learning rate ratio =  0.19493195284197837\n",
      "Epoch =  537 Batch =  0 Loss =  13.86864839893241 Gradient_max =  0.003801939114735389 learning rate ratio =  4.7823134672275474e-05\n",
      "Epoch =  538 Batch =  0 Loss =  13.868645333625258 Gradient_max =  0.0038079753702384483 learning rate ratio =  4.801281785071823e-05\n",
      "Epoch =  539 Batch =  0 Loss =  13.868642270745253 Gradient_max =  0.003814012837176589 learning rate ratio =  4.820360497979195e-05\n",
      "Epoch =  540 Batch =  0 Loss =  13.86863921029098 Gradient_max =  0.0038200515178491872 learning rate ratio =  4.839550624760681e-05\n",
      "Epoch =  541 Batch =  0 Loss =  13.868636152261 Gradient_max =  0.003826091414556005 learning rate ratio =  4.8588531966512056e-05\n",
      "Epoch =  542 Batch =  0 Loss =  13.868633096653896 Gradient_max =  0.003832132529597188 learning rate ratio =  4.878269257499964e-05\n",
      "Epoch =  543 Batch =  0 Loss =  13.86863004346825 Gradient_max =  0.003838174865273271 learning rate ratio =  4.8977998639644376e-05\n",
      "Epoch =  544 Batch =  0 Loss =  13.868626992728359 Gradient_max =  0.0038442184238406817 learning rate ratio =  4.9174460863684956e-05\n",
      "Epoch =  545 Batch =  0 Loss =  13.868623944407059 Gradient_max =  0.003850263207645149 learning rate ratio =  4.9372090069270985e-05\n",
      "Epoch =  546 Batch =  0 Loss =  13.868620898502941 Gradient_max =  0.0038563092189883835 learning rate ratio =  4.957089721922348e-05\n",
      "Epoch =  547 Batch =  0 Loss =  13.868617855014634 Gradient_max =  0.003862356460237118 learning rate ratio =  4.9770893409658685e-05\n",
      "Epoch =  548 Batch =  0 Loss =  13.868614813956963 Gradient_max =  0.0038684049336309744 learning rate ratio =  4.997208988289535e-05\n",
      "Epoch =  549 Batch =  0 Loss =  13.86861177531243 Gradient_max =  0.003874454641471218 learning rate ratio =  5.0174498014773596e-05\n",
      "Epoch =  550 Batch =  0 Loss =  13.868608739021637 Gradient_max =  0.003880505450552945 learning rate ratio =  5.0378129339088143e-05\n",
      "Epoch =  551 Batch =  0 Loss =  13.868605705141063 Gradient_max =  0.003886557500305742 learning rate ratio =  5.058299550823548e-05\n",
      "Epoch =  552 Batch =  0 Loss =  13.86860267366929 Gradient_max =  0.0038926107930362342 learning rate ratio =  5.078910833498798e-05\n",
      "Epoch =  553 Batch =  0 Loss =  13.868599644611646 Gradient_max =  0.0038986654877087613 learning rate ratio =  5.099647978375032e-05\n",
      "Epoch =  554 Batch =  0 Loss =  13.868596617960067 Gradient_max =  0.0039047214342090137 learning rate ratio =  5.120512196686193e-05\n",
      "Epoch =  555 Batch =  0 Loss =  13.868593593765846 Gradient_max =  0.0039107786349132125 learning rate ratio =  5.141504715138905e-05\n",
      "Epoch =  556 Batch =  0 Loss =  13.868590571974925 Gradient_max =  0.003916837092072736 learning rate ratio =  5.1626267764560365e-05\n",
      "Epoch =  557 Batch =  0 Loss =  13.868587552649226 Gradient_max =  0.00392289680810843 learning rate ratio =  5.1838796395922205e-05\n",
      "Epoch =  558 Batch =  0 Loss =  13.868584535724185 Gradient_max =  0.003928957785228736 learning rate ratio =  5.205264578808978e-05\n",
      "Epoch =  559 Batch =  0 Loss =  13.868581521198408 Gradient_max =  0.0039350200257488276 learning rate ratio =  5.226782885253792e-05\n",
      "Epoch =  560 Batch =  0 Loss =  13.868578509070492 Gradient_max =  0.003941083531984289 learning rate ratio =  5.248435866797828e-05\n",
      "Epoch =  561 Batch =  0 Loss =  13.868575499339034 Gradient_max =  0.003947148306251111 learning rate ratio =  5.2702248483081074e-05\n",
      "Epoch =  562 Batch =  0 Loss =  13.86857249200264 Gradient_max =  0.003953214350865697 learning rate ratio =  5.29215117192493e-05\n",
      "Epoch =  563 Batch =  0 Loss =  13.868569486979803 Gradient_max =  0.003959281668310409 learning rate ratio =  5.3142161972320885e-05\n",
      "Epoch =  564 Batch =  0 Loss =  13.868566484349577 Gradient_max =  0.003965350260737256 learning rate ratio =  5.336421301900225e-05\n",
      "Epoch =  565 Batch =  0 Loss =  13.868563483981907 Gradient_max =  0.003971420130633041 learning rate ratio =  5.3587678823242186e-05\n",
      "Epoch =  566 Batch =  0 Loss =  13.868560486004524 Gradient_max =  0.003977491280147094 learning rate ratio =  5.381257352026588e-05\n",
      "Epoch =  567 Batch =  0 Loss =  13.868557490416046 Gradient_max =  0.003983563711597865 learning rate ratio =  5.4038911438742425e-05\n",
      "Epoch =  568 Batch =  0 Loss =  13.868554497215078 Gradient_max =  0.003989637427304213 learning rate ratio =  5.4266707097911844e-05\n",
      "Epoch =  569 Batch =  0 Loss =  13.868551506400227 Gradient_max =  0.003995712429585411 learning rate ratio =  5.449597521076972e-05\n",
      "Epoch =  570 Batch =  0 Loss =  13.868548517865667 Gradient_max =  0.0040017887208619275 learning rate ratio =  5.4726730693078e-05\n",
      "Epoch =  571 Batch =  0 Loss =  13.868545531610376 Gradient_max =  0.004007866303454289 learning rate ratio =  5.4958988655975966e-05\n",
      "Epoch =  572 Batch =  0 Loss =  13.868542547737547 Gradient_max =  0.004013945179582353 learning rate ratio =  5.519276440802794e-05\n",
      "Epoch =  573 Batch =  0 Loss =  13.868539566279447 Gradient_max =  0.004020025351619976 learning rate ratio =  5.578796798407364e-05\n",
      "Epoch =  574 Batch =  0 Loss =  13.868536587201106 Gradient_max =  0.004026106821835652 learning rate ratio =  5.6461532022232e-05\n",
      "Epoch =  575 Batch =  0 Loss =  13.86853361050115 Gradient_max =  0.004032189592551147 learning rate ratio =  5.715133496150182e-05\n",
      "Epoch =  576 Batch =  0 Loss =  13.868530636099065 Gradient_max =  0.004038273666103598 learning rate ratio =  5.785449335052125e-05\n",
      "Epoch =  577 Batch =  0 Loss =  13.868527664072845 Gradient_max =  0.004044359044800906 learning rate ratio =  5.8574934250755376e-05\n",
      "Epoch =  578 Batch =  0 Loss =  13.868524694421097 Gradient_max =  0.004050445730966109 learning rate ratio =  5.931330250234262e-05\n",
      "Epoch =  579 Batch =  0 Loss =  13.868521727343921 Gradient_max =  0.004056533727282494 learning rate ratio =  6.007024860235234e-05\n",
      "Epoch =  580 Batch =  0 Loss =  13.86851876263889 Gradient_max =  0.0040626230357147085 learning rate ratio =  6.0846509906315196e-05\n",
      "Epoch =  581 Batch =  0 Loss =  13.868515800211853 Gradient_max =  0.004068713658731076 learning rate ratio =  6.165143432656099e-05\n",
      "Epoch =  582 Batch =  0 Loss =  13.868512840154606 Gradient_max =  0.004074805598512702 learning rate ratio =  6.247767045199572e-05\n",
      "Epoch =  583 Batch =  0 Loss =  13.868509882465776 Gradient_max =  0.004080898857384745 learning rate ratio =  6.332607573887526e-05\n",
      "Epoch =  584 Batch =  0 Loss =  13.868506927144 Gradient_max =  0.004086993437672785 learning rate ratio =  6.419755426746547e-05\n",
      "Epoch =  585 Batch =  0 Loss =  13.868503974248684 Gradient_max =  0.004093089341841497 learning rate ratio =  6.50930599559948e-05\n",
      "Epoch =  586 Batch =  0 Loss =  13.868501023717736 Gradient_max =  0.004099186572078843 learning rate ratio =  6.601360003874854e-05\n",
      "Epoch =  587 Batch =  0 Loss =  13.868498075549779 Gradient_max =  0.004105285130711684 learning rate ratio =  6.696023884796922e-05\n",
      "Epoch =  588 Batch =  0 Loss =  13.868495129743456 Gradient_max =  0.004111385020067325 learning rate ratio =  6.793410191652026e-05\n",
      "Epoch =  589 Batch =  0 Loss =  13.868492186297393 Gradient_max =  0.004117486242473495 learning rate ratio =  6.89363804409478e-05\n",
      "Epoch =  590 Batch =  0 Loss =  13.868489245210233 Gradient_max =  0.0041235888002583545 learning rate ratio =  6.996833614088637e-05\n",
      "Epoch =  591 Batch =  0 Loss =  13.86848630650245 Gradient_max =  0.004129692695780878 learning rate ratio =  7.103130655460918e-05\n",
      "Epoch =  592 Batch =  0 Loss =  13.868483370161778 Gradient_max =  0.004135797931151857 learning rate ratio =  7.212671082648153e-05\n",
      "Epoch =  593 Batch =  0 Loss =  13.86848043608352 Gradient_max =  0.004141904579405969 learning rate ratio =  7.32560560155568e-05\n",
      "Epoch =  594 Batch =  0 Loss =  13.868477504395196 Gradient_max =  0.004148012577762677 learning rate ratio =  7.442094403076218e-05\n",
      "Epoch =  595 Batch =  0 Loss =  13.868474575085646 Gradient_max =  0.004154121928544981 learning rate ratio =  7.562093839847707e-05\n",
      "Epoch =  596 Batch =  0 Loss =  13.86847164828192 Gradient_max =  0.004160232634333752 learning rate ratio =  7.685985383393794e-05\n",
      "Epoch =  597 Batch =  0 Loss =  13.868468723827819 Gradient_max =  0.004166344697150124 learning rate ratio =  7.81396146775852e-05\n",
      "Epoch =  598 Batch =  0 Loss =  13.868465801721996 Gradient_max =  0.0041724581193328435 learning rate ratio =  7.946227425678053e-05\n",
      "Epoch =  599 Batch =  0 Loss =  13.868462872547422 Gradient_max =  0.004178572886231953 learning rate ratio =  0.00014587191181792441\n",
      "Epoch =  600 Batch =  0 Loss =  13.86845994369072 Gradient_max =  0.004184689013601039 learning rate ratio =  0.00014845116442210778\n",
      "Epoch =  601 Batch =  0 Loss =  13.868457017174116 Gradient_max =  0.0041908065073476925 learning rate ratio =  0.00015112218204179004\n",
      "Epoch =  602 Batch =  0 Loss =  13.86845409299184 Gradient_max =  0.004196925369742008 learning rate ratio =  0.00015388994965175124\n",
      "Epoch =  603 Batch =  0 Loss =  13.868451171142484 Gradient_max =  0.004203045603124815 learning rate ratio =  0.00015675981995295506\n",
      "Epoch =  604 Batch =  0 Loss =  13.868448251629095 Gradient_max =  0.0042091672099078955 learning rate ratio =  0.00015973754791758163\n",
      "Epoch =  605 Batch =  0 Loss =  13.868445334486623 Gradient_max =  0.004215290192632362 learning rate ratio =  0.00016282932932353252\n",
      "Epoch =  606 Batch =  0 Loss =  13.868442419616859 Gradient_max =  0.004221414473198081 learning rate ratio =  0.00016604184371372073\n",
      "Epoch =  607 Batch =  0 Loss =  13.868439507078833 Gradient_max =  0.004227540136334263 learning rate ratio =  0.00016938230262450096\n",
      "Epoch =  608 Batch =  0 Loss =  13.868436596871204 Gradient_max =  0.004233667184387112 learning rate ratio =  0.00017285850359946797\n",
      "Epoch =  609 Batch =  0 Loss =  13.868433688992626 Gradient_max =  0.004239795619703291 learning rate ratio =  0.0001764788909472764\n",
      "Epoch =  610 Batch =  0 Loss =  13.86843078344175 Gradient_max =  0.004245925444629916 learning rate ratio =  0.0001802526242187779\n",
      "Epoch =  611 Batch =  0 Loss =  13.868427880186232 Gradient_max =  0.004252056609724759 learning rate ratio =  0.0001841896555681521\n",
      "Epoch =  612 Batch =  0 Loss =  13.8684249792557 Gradient_max =  0.004258189169740449 learning rate ratio =  0.00018830081739501126\n",
      "Epoch =  613 Batch =  0 Loss =  13.868422080466729 Gradient_max =  0.004264323126945413 learning rate ratio =  0.0001926371615476674\n",
      "Epoch =  614 Batch =  0 Loss =  13.86841918388226 Gradient_max =  0.004270458483802126 learning rate ratio =  0.00019721717997578213\n",
      "Epoch =  615 Batch =  0 Loss =  13.868416289674363 Gradient_max =  0.004276595242770685 learning rate ratio =  0.00020200980659447433\n",
      "Epoch =  616 Batch =  0 Loss =  13.86841339778674 Gradient_max =  0.004282733406060639 learning rate ratio =  0.0002070390943945262\n",
      "Epoch =  617 Batch =  0 Loss =  13.868410508061787 Gradient_max =  0.004288872976138098 learning rate ratio =  0.00021239284119076827\n",
      "Epoch =  618 Batch =  0 Loss =  13.868407620702625 Gradient_max =  0.004295013955282158 learning rate ratio =  0.0002180284947746957\n",
      "Epoch =  619 Batch =  0 Loss =  13.86840473566013 Gradient_max =  0.004301156345802894 learning rate ratio =  0.010240140924564448\n",
      "Epoch =  620 Batch =  0 Loss =  13.868401852932974 Gradient_max =  0.004307300150052863 learning rate ratio =  0.0008980550456635978\n",
      "Epoch =  621 Batch =  0 Loss =  13.868398972519836 Gradient_max =  0.004313445370385092 learning rate ratio =  0.00046990515437386694\n",
      "Epoch =  622 Batch =  0 Loss =  13.868396094483241 Gradient_max =  0.004319592009249138 learning rate ratio =  0.00031833228438171306\n",
      "Epoch =  623 Batch =  0 Loss =  13.868393218758055 Gradient_max =  0.004325740068902996 learning rate ratio =  0.00025133272640146133\n",
      "Epoch =  624 Batch =  0 Loss =  13.868390345299538 Gradient_max =  0.004331889551756097 learning rate ratio =  0.0002592291145734746\n",
      "Epoch =  625 Batch =  0 Loss =  13.868387474149925 Gradient_max =  0.004338040460108476 learning rate ratio =  0.00026763427238545647\n",
      "Epoch =  626 Batch =  0 Loss =  13.868384605307872 Gradient_max =  0.004344192796315499 learning rate ratio =  0.00027659900395932043\n",
      "Epoch =  627 Batch =  0 Loss =  13.868381738772072 Gradient_max =  0.004350346562733 learning rate ratio =  0.00028618111050539774\n",
      "Epoch =  628 Batch =  0 Loss =  13.868378874541198 Gradient_max =  0.0043565017617172835 learning rate ratio =  0.0002964466378863021\n",
      "Epoch =  629 Batch =  0 Loss =  13.868376012613918 Gradient_max =  0.004362658395625131 learning rate ratio =  0.0003074714009763095\n",
      "Epoch =  630 Batch =  0 Loss =  13.868373152988921 Gradient_max =  0.004368816466813788 learning rate ratio =  0.0003193428592150679\n",
      "Epoch =  631 Batch =  0 Loss =  13.868370295664885 Gradient_max =  0.004374975977640986 learning rate ratio =  0.00033216244152117064\n",
      "Epoch =  632 Batch =  0 Loss =  13.868367440640494 Gradient_max =  0.004381136930464924 learning rate ratio =  0.00034604845139445025\n",
      "Epoch =  633 Batch =  0 Loss =  13.868364587914423 Gradient_max =  0.004387299327644279 learning rate ratio =  0.00036113972845660435\n",
      "Epoch =  634 Batch =  0 Loss =  13.868361737485364 Gradient_max =  0.004393463171538204 learning rate ratio =  0.0003776003066454907\n",
      "Epoch =  635 Batch =  0 Loss =  13.868358889371745 Gradient_max =  0.004399628464516551 learning rate ratio =  0.00039562540059746534\n",
      "Epoch =  636 Batch =  0 Loss =  13.868356043597064 Gradient_max =  0.004405795209006602 learning rate ratio =  0.0004154491841109784\n",
      "Epoch =  637 Batch =  0 Loss =  13.86835320011546 Gradient_max =  0.00441196340729159 learning rate ratio =  0.0004373550193837623\n",
      "Epoch =  638 Batch =  0 Loss =  13.868350358925635 Gradient_max =  0.004418133061732588 learning rate ratio =  0.0004616890879005202\n",
      "Epoch =  639 Batch =  0 Loss =  13.868347520026271 Gradient_max =  0.0044243041746911535 learning rate ratio =  0.0004888788200675608\n",
      "Epoch =  640 Batch =  0 Loss =  13.868344683416062 Gradient_max =  0.004430476748529318 learning rate ratio =  0.0005194582174362616\n",
      "Epoch =  641 Batch =  0 Loss =  13.8683418490937 Gradient_max =  0.0044366507856096055 learning rate ratio =  0.0005541032748580672\n",
      "Epoch =  642 Batch =  0 Loss =  13.868339017079347 Gradient_max =  0.00444282628827961 learning rate ratio =  0.0005936825370682362\n",
      "Epoch =  643 Batch =  0 Loss =  13.868336187350305 Gradient_max =  0.004449003258918101 learning rate ratio =  0.0006393309121507963\n",
      "Epoch =  644 Batch =  0 Loss =  13.868333359905266 Gradient_max =  0.004455181699889054 learning rate ratio =  0.0006925602599179496\n",
      "Epoch =  645 Batch =  0 Loss =  13.868330534742936 Gradient_max =  0.004461361613556935 learning rate ratio =  0.0007554300663179197\n",
      "Epoch =  646 Batch =  0 Loss =  13.868327711862001 Gradient_max =  0.0044675430022866865 learning rate ratio =  0.0008308200839580914\n",
      "Epoch =  647 Batch =  0 Loss =  13.868324891261173 Gradient_max =  0.004473725868443756 learning rate ratio =  0.0009228838685558321\n",
      "Epoch =  648 Batch =  0 Loss =  13.868322072939142 Gradient_max =  0.004479910214394066 learning rate ratio =  0.0010378406488891804\n",
      "Epoch =  649 Batch =  0 Loss =  13.868319256927816 Gradient_max =  0.0044860960424700945 learning rate ratio =  0.0011854417978118467\n",
      "Epoch =  650 Batch =  0 Loss =  13.868316443192574 Gradient_max =  0.004492283355072349 learning rate ratio =  0.0013818933375102196\n",
      "Epoch =  651 Batch =  0 Loss =  13.868313631784122 Gradient_max =  0.004498472317930158 learning rate ratio =  0.001656255543277094\n",
      "Epoch =  652 Batch =  0 Loss =  13.868310822518037 Gradient_max =  0.004504662773082837 learning rate ratio =  0.0020696531283267445\n",
      "Epoch =  653 Batch =  0 Loss =  13.868308015381604 Gradient_max =  0.004510854722819606 learning rate ratio =  0.002758897485585359\n",
      "Epoch =  654 Batch =  0 Loss =  13.868305210516406 Gradient_max =  0.004517048169620509 learning rate ratio =  0.004135579983299648\n",
      "Epoch =  655 Batch =  0 Loss =  13.868302408078648 Gradient_max =  0.004523243116118156 learning rate ratio =  0.008251231569426462\n",
      "Epoch =  656 Batch =  0 Loss =  13.868299607909611 Gradient_max =  0.004529439564426773 learning rate ratio =  1.5783535396401456\n",
      "Epoch =  657 Batch =  0 Loss =  13.868296810007998 Gradient_max =  0.004535637516920418 learning rate ratio =  8.299086350221227e-05\n",
      "Epoch =  658 Batch =  0 Loss =  13.868294014465166 Gradient_max =  0.004541836976103662 learning rate ratio =  8.344986331240407e-05\n",
      "Epoch =  659 Batch =  0 Loss =  13.86829122118727 Gradient_max =  0.0045480379442216865 learning rate ratio =  8.391321013511555e-05\n",
      "Epoch =  660 Batch =  0 Loss =  13.86828843017303 Gradient_max =  0.004554240423650051 learning rate ratio =  8.438096697947372e-05\n",
      "Epoch =  661 Batch =  0 Loss =  13.868285641471799 Gradient_max =  0.004560444416855989 learning rate ratio =  8.485319805662756e-05\n",
      "Epoch =  662 Batch =  0 Loss =  13.868282855031492 Gradient_max =  0.004566649926124692 learning rate ratio =  8.53299688661945e-05\n",
      "Epoch =  663 Batch =  0 Loss =  13.868280070861546 Gradient_max =  0.004572856953928546 learning rate ratio =  8.581134617075948e-05\n",
      "Epoch =  664 Batch =  0 Loss =  13.868277288950308 Gradient_max =  0.004579065502550131 learning rate ratio =  8.629739804529927e-05\n",
      "Epoch =  665 Batch =  0 Loss =  13.868274509296498 Gradient_max =  0.004585275574367533 learning rate ratio =  8.678819391018824e-05\n",
      "Epoch =  666 Batch =  0 Loss =  13.868271732000817 Gradient_max =  0.004591487172057626 learning rate ratio =  8.728380454678371e-05\n",
      "Epoch =  667 Batch =  0 Loss =  13.868268956960442 Gradient_max =  0.00459770029770166 learning rate ratio =  8.7784302184827e-05\n",
      "Epoch =  668 Batch =  0 Loss =  13.868266184174095 Gradient_max =  0.004603914953679251 learning rate ratio =  8.828976048501815e-05\n",
      "Epoch =  669 Batch =  0 Loss =  13.868263413328272 Gradient_max =  0.004610131142297982 learning rate ratio =  8.880025462091737e-05\n",
      "Epoch =  670 Batch =  0 Loss =  13.868260644734423 Gradient_max =  0.004616348866011502 learning rate ratio =  8.93158612352345e-05\n",
      "Epoch =  671 Batch =  0 Loss =  13.868257878391274 Gradient_max =  0.004622568127200963 learning rate ratio =  8.983665855789632e-05\n",
      "Epoch =  672 Batch =  0 Loss =  13.868255114297549 Gradient_max =  0.004628788928248025 learning rate ratio =  9.03627264202374e-05\n",
      "Epoch =  673 Batch =  0 Loss =  13.868252352680049 Gradient_max =  0.0046350112718428855 learning rate ratio =  9.089245546249748e-05\n",
      "Epoch =  674 Batch =  0 Loss =  13.86824959336098 Gradient_max =  0.004641235160138652 learning rate ratio =  9.142758185793343e-05\n",
      "Epoch =  675 Batch =  0 Loss =  13.868246836287796 Gradient_max =  0.0046474605954404885 learning rate ratio =  9.19681895931256e-05\n",
      "Epoch =  676 Batch =  0 Loss =  13.868244081459231 Gradient_max =  0.004653687580132121 learning rate ratio =  9.25143644005038e-05\n",
      "Epoch =  677 Batch =  0 Loss =  13.868241328943494 Gradient_max =  0.004659916116652492 learning rate ratio =  9.306619380709981e-05\n",
      "Epoch =  678 Batch =  0 Loss =  13.868238578696047 Gradient_max =  0.004666146207281563 learning rate ratio =  9.362376718731602e-05\n",
      "Epoch =  679 Batch =  0 Loss =  13.868235830689382 Gradient_max =  0.004672377854454789 learning rate ratio =  9.418717579332698e-05\n",
      "Epoch =  680 Batch =  0 Loss =  13.868233084870417 Gradient_max =  0.00467861099075047 learning rate ratio =  9.475651283220591e-05\n",
      "Epoch =  681 Batch =  0 Loss =  13.868230341217215 Gradient_max =  0.0046848456900121 learning rate ratio =  9.533187348745965e-05\n",
      "Epoch =  682 Batch =  0 Loss =  13.868227599728957 Gradient_max =  0.004691081954629026 learning rate ratio =  9.591335499678012e-05\n",
      "Epoch =  683 Batch =  0 Loss =  13.868224860495763 Gradient_max =  0.00469731978697238 learning rate ratio =  9.650105669541087e-05\n",
      "Epoch =  684 Batch =  0 Loss =  13.868222123342493 Gradient_max =  0.004703559189327154 learning rate ratio =  9.709508010054415e-05\n",
      "Epoch =  685 Batch =  0 Loss =  13.868219388423174 Gradient_max =  0.004709800164159911 learning rate ratio =  9.769552890905691e-05\n",
      "Epoch =  686 Batch =  0 Loss =  13.868216655663982 Gradient_max =  0.004716042714022507 learning rate ratio =  9.830250910713638e-05\n",
      "Epoch =  687 Batch =  0 Loss =  13.868213925136605 Gradient_max =  0.0047222868411459715 learning rate ratio =  9.891612903325107e-05\n",
      "Epoch =  688 Batch =  0 Loss =  13.86821119683979 Gradient_max =  0.004728532547922395 learning rate ratio =  9.953649942587639e-05\n",
      "Epoch =  689 Batch =  0 Loss =  13.868208470647113 Gradient_max =  0.004734779836709652 learning rate ratio =  0.00010016373350087299\n",
      "Epoch =  690 Batch =  0 Loss =  13.868205746682708 Gradient_max =  0.004741028709935851 learning rate ratio =  0.0001007979470081398\n",
      "Epoch =  691 Batch =  0 Loss =  13.868203024982357 Gradient_max =  0.004747279169985996 learning rate ratio =  0.00010143925832012483\n",
      "Epoch =  692 Batch =  0 Loss =  13.868200305376044 Gradient_max =  0.004753531219390381 learning rate ratio =  0.00010208778850323786\n",
      "Epoch =  693 Batch =  0 Loss =  13.868197587994965 Gradient_max =  0.004759784860416917 learning rate ratio =  0.0001027436613685456\n",
      "Epoch =  694 Batch =  0 Loss =  13.868194872869585 Gradient_max =  0.004766040095544754 learning rate ratio =  0.0001034070035747765\n",
      "Epoch =  695 Batch =  0 Loss =  13.868192159966949 Gradient_max =  0.0047722969270859 learning rate ratio =  0.00010407794472408561\n",
      "Epoch =  696 Batch =  0 Loss =  13.868189449285808 Gradient_max =  0.004778555357436718 learning rate ratio =  0.00010475661742237168\n",
      "Epoch =  697 Batch =  0 Loss =  13.86818674081774 Gradient_max =  0.0047848153889868345 learning rate ratio =  0.00010544315737524791\n",
      "Epoch =  698 Batch =  0 Loss =  13.868184034505179 Gradient_max =  0.0047910770242726285 learning rate ratio =  0.00010613770345525015\n",
      "Epoch =  699 Batch =  0 Loss =  13.86818133041061 Gradient_max =  0.004797340265560673 learning rate ratio =  0.00010684039786864322\n",
      "Epoch =  700 Batch =  0 Loss =  13.868178628535858 Gradient_max =  0.004803605115241078 learning rate ratio =  0.00010755138618467422\n",
      "Epoch =  701 Batch =  0 Loss =  13.868175928876601 Gradient_max =  0.0048098715757212826 learning rate ratio =  0.00010827081744996902\n",
      "Epoch =  702 Batch =  0 Loss =  13.8681732314316 Gradient_max =  0.00481613964940089 learning rate ratio =  0.00010899884430394477\n",
      "Epoch =  703 Batch =  0 Loss =  13.868170536199619 Gradient_max =  0.00482240933868005 learning rate ratio =  0.00010973562308406925\n",
      "Epoch =  704 Batch =  0 Loss =  13.868167833655622 Gradient_max =  0.004828680646358952 learning rate ratio =  0.00011048131389871122\n",
      "Epoch =  705 Batch =  0 Loss =  13.868165128606003 Gradient_max =  0.004834953574653759 learning rate ratio =  0.00011123608084278078\n",
      "Epoch =  706 Batch =  0 Loss =  13.868162425760877 Gradient_max =  0.004841228125752769 learning rate ratio =  0.00011200009206076465\n",
      "Epoch =  707 Batch =  0 Loss =  13.868159725119007 Gradient_max =  0.004847504302058313 learning rate ratio =  0.00011277351985153153\n",
      "Epoch =  708 Batch =  0 Loss =  13.868157026679162 Gradient_max =  0.004853782105973271 learning rate ratio =  0.0001135565408187185\n",
      "Epoch =  709 Batch =  0 Loss =  13.8681543304401 Gradient_max =  0.004860061539901067 learning rate ratio =  0.00011434933600602566\n",
      "Epoch =  710 Batch =  0 Loss =  13.868151636274918 Gradient_max =  0.0048663426061966416 learning rate ratio =  0.00011515209106884002\n",
      "Epoch =  711 Batch =  0 Loss =  13.868148944308466 Gradient_max =  0.004872625307313915 learning rate ratio =  0.00011596499632963723\n",
      "Epoch =  712 Batch =  0 Loss =  13.86814625453952 Gradient_max =  0.0048789096456579644 learning rate ratio =  0.00011678824701670433\n",
      "Epoch =  713 Batch =  0 Loss =  13.868143566966845 Gradient_max =  0.00488519562363442 learning rate ratio =  0.00011762204339327489\n",
      "Epoch =  714 Batch =  0 Loss =  13.868140881589218 Gradient_max =  0.004891483243649466 learning rate ratio =  0.00011846659092090788\n",
      "Epoch =  715 Batch =  0 Loss =  13.868138198299617 Gradient_max =  0.004897772507975017 learning rate ratio =  0.00011932210046127442\n",
      "Epoch =  716 Batch =  0 Loss =  13.868135517202841 Gradient_max =  0.004904063419153308 learning rate ratio =  0.0001201887883574715\n",
      "Epoch =  717 Batch =  0 Loss =  13.868132838297667 Gradient_max =  0.00491035597959219 learning rate ratio =  0.0001210668767118581\n",
      "Epoch =  718 Batch =  0 Loss =  13.868130161582862 Gradient_max =  0.004916650191700073 learning rate ratio =  0.00012195659354569523\n",
      "Epoch =  719 Batch =  0 Loss =  13.868127487057206 Gradient_max =  0.004922946057885918 learning rate ratio =  0.0001228581729976492\n",
      "Epoch =  720 Batch =  0 Loss =  13.86812481475158 Gradient_max =  0.004929243580486717 learning rate ratio =  0.00012377185556248526\n",
      "Epoch =  721 Batch =  0 Loss =  13.868122144629032 Gradient_max =  0.00493554276545652 learning rate ratio =  0.00012469788821116076\n",
      "Epoch =  722 Batch =  0 Loss =  13.868119476691978 Gradient_max =  0.004941843615301686 learning rate ratio =  0.00012563652470740411\n",
      "Epoch =  723 Batch =  0 Loss =  13.868116810939203 Gradient_max =  0.004948146132437623 learning rate ratio =  0.00012658802581129536\n",
      "Epoch =  724 Batch =  0 Loss =  13.868114147369488 Gradient_max =  0.0049544503192803045 learning rate ratio =  0.0001275526595214562\n",
      "Epoch =  725 Batch =  0 Loss =  13.868111485806317 Gradient_max =  0.004960756178241413 learning rate ratio =  0.00012853070136122338\n",
      "Epoch =  726 Batch =  0 Loss =  13.868108826229061 Gradient_max =  0.0049670637116686 learning rate ratio =  0.00012952243459878778\n",
      "Epoch =  727 Batch =  0 Loss =  13.868106168831899 Gradient_max =  0.004973372922054423 learning rate ratio =  0.00013052815046231395\n",
      "Epoch =  728 Batch =  0 Loss =  13.868103513617687 Gradient_max =  0.004979683811797015 learning rate ratio =  0.00013154814854584562\n",
      "Epoch =  729 Batch =  0 Loss =  13.868100860581125 Gradient_max =  0.004985996383335264 learning rate ratio =  0.0001325827370550145\n",
      "Epoch =  730 Batch =  0 Loss =  13.868098209721005 Gradient_max =  0.00499231063908855 learning rate ratio =  0.00013363223312487307\n",
      "Epoch =  731 Batch =  0 Loss =  13.868095561036109 Gradient_max =  0.004998626581476825 learning rate ratio =  0.00013469696314356093\n",
      "Epoch =  732 Batch =  0 Loss =  13.868092914525237 Gradient_max =  0.005004944212920614 learning rate ratio =  0.00013577726309227126\n",
      "Epoch =  733 Batch =  0 Loss =  13.868090270232015 Gradient_max =  0.005011263535844391 learning rate ratio =  0.00013687347890669666\n",
      "Epoch =  734 Batch =  0 Loss =  13.868087628110487 Gradient_max =  0.005017584552666392 learning rate ratio =  0.00013798596682901428\n",
      "Epoch =  735 Batch =  0 Loss =  13.868084988159445 Gradient_max =  0.00502390726580887 learning rate ratio =  0.00013911509381436807\n",
      "Epoch =  736 Batch =  0 Loss =  13.868082350485382 Gradient_max =  0.005030231677906427 learning rate ratio =  0.00014026123787790463\n",
      "Epoch =  737 Batch =  0 Loss =  13.868079714754359 Gradient_max =  0.005036557559095364 learning rate ratio =  0.00014142478869940385\n",
      "Epoch =  738 Batch =  0 Loss =  13.868077081190188 Gradient_max =  0.005042885146314341 learning rate ratio =  0.00014260614783473538\n",
      "Epoch =  739 Batch =  0 Loss =  13.86807444981364 Gradient_max =  0.005049214442143223 learning rate ratio =  0.0001438057293105161\n",
      "Epoch =  740 Batch =  0 Loss =  13.868071820969144 Gradient_max =  0.005055545448661877 learning rate ratio =  0.00014502396012147973\n",
      "Epoch =  741 Batch =  0 Loss =  13.868069194289328 Gradient_max =  0.005061878168496275 learning rate ratio =  0.00014626128062276086\n",
      "Epoch =  742 Batch =  0 Loss =  13.868066569772994 Gradient_max =  0.0050682126040758144 learning rate ratio =  0.00014751814517721093\n",
      "Epoch =  743 Batch =  0 Loss =  13.86806394741894 Gradient_max =  0.0050745487578304745 learning rate ratio =  0.00014879502267662893\n",
      "Epoch =  744 Batch =  0 Loss =  13.868061327225972 Gradient_max =  0.0050808866321908255 learning rate ratio =  0.00015009239712718904\n",
      "Epoch =  745 Batch =  0 Loss =  13.868058709192894 Gradient_max =  0.00508722622958802 learning rate ratio =  0.00015141076826340476\n",
      "Epoch =  746 Batch =  0 Loss =  13.868056093444597 Gradient_max =  0.005093567552443258 learning rate ratio =  0.0001527506522458063\n",
      "Epoch =  747 Batch =  0 Loss =  13.86805347985391 Gradient_max =  0.005099910603199312 learning rate ratio =  0.00015411258217832403\n",
      "Epoch =  748 Batch =  0 Loss =  13.868050868419648 Gradient_max =  0.005106255384289099 learning rate ratio =  0.00015549710897492014\n",
      "Epoch =  749 Batch =  0 Loss =  13.868048259140608 Gradient_max =  0.005112601898146131 learning rate ratio =  0.00015690480205352058\n",
      "Epoch =  750 Batch =  0 Loss =  13.868045652015615 Gradient_max =  0.005118950147204505 learning rate ratio =  0.00015833625011949326\n",
      "Epoch =  751 Batch =  0 Loss =  13.868043046966147 Gradient_max =  0.005125300133829681 learning rate ratio =  0.0001597920620096862\n",
      "Epoch =  752 Batch =  0 Loss =  13.868040444068463 Gradient_max =  0.005131651860526281 learning rate ratio =  0.00016127286749841965\n",
      "Epoch =  753 Batch =  0 Loss =  13.868037843321371 Gradient_max =  0.0051380053297301834 learning rate ratio =  0.0001627793182680617\n",
      "Epoch =  754 Batch =  0 Loss =  13.868035244623854 Gradient_max =  0.0051443603296359944 learning rate ratio =  0.0001643120888704758\n",
      "Epoch =  755 Batch =  0 Loss =  13.868032648074495 Gradient_max =  0.005150717082616038 learning rate ratio =  0.00016587187767258185\n",
      "Epoch =  756 Batch =  0 Loss =  13.868030053672097 Gradient_max =  0.005157075591114673 learning rate ratio =  0.0001674594079850703\n",
      "Epoch =  757 Batch =  0 Loss =  13.868027461415487 Gradient_max =  0.005163435857576858 learning rate ratio =  0.0001690754291627006\n",
      "Epoch =  758 Batch =  0 Loss =  13.868024871303481 Gradient_max =  0.005169797884448154 learning rate ratio =  0.00017072071778717073\n",
      "Epoch =  759 Batch =  0 Loss =  13.868022283563544 Gradient_max =  0.00517616167459455 learning rate ratio =  0.00017239747223770044\n",
      "Epoch =  760 Batch =  0 Loss =  13.868019698007418 Gradient_max =  0.005182527230080018 learning rate ratio =  0.00017410518800479693\n",
      "Epoch =  761 Batch =  0 Loss =  13.868017114592684 Gradient_max =  0.0051888945533154224 learning rate ratio =  0.00017584473342057534\n",
      "Epoch =  762 Batch =  0 Loss =  13.868014533318155 Gradient_max =  0.0051952636467487425 learning rate ratio =  0.00017761700959026432\n",
      "Epoch =  763 Batch =  0 Loss =  13.868011953945404 Gradient_max =  0.0052016345128285705 learning rate ratio =  0.00017942295194948608\n",
      "Epoch =  764 Batch =  0 Loss =  13.86800937278412 Gradient_max =  0.005208007154132039 learning rate ratio =  0.00018126353190088518\n",
      "Epoch =  765 Batch =  0 Loss =  13.86800679375776 Gradient_max =  0.005214381572981113 learning rate ratio =  0.00018313975860178805\n",
      "Epoch =  766 Batch =  0 Loss =  13.868004216865145 Gradient_max =  0.005220757771826208 learning rate ratio =  0.00018505268077405925\n",
      "Epoch =  767 Batch =  0 Loss =  13.868001642105108 Gradient_max =  0.005227135753118347 learning rate ratio =  0.00018700338867643583\n",
      "Epoch =  768 Batch =  0 Loss =  13.867999069476472 Gradient_max =  0.005233515519309174 learning rate ratio =  0.00018899301618156158\n",
      "Epoch =  769 Batch =  0 Loss =  13.867996498896801 Gradient_max =  0.005239896974514475 learning rate ratio =  0.00019102274300259145\n",
      "Epoch =  770 Batch =  0 Loss =  13.867993930373913 Gradient_max =  0.005246280224976403 learning rate ratio =  0.00019309379691907826\n",
      "Epoch =  771 Batch =  0 Loss =  13.867991363807777 Gradient_max =  0.005252665273029094 learning rate ratio =  0.000195207456472344\n",
      "Epoch =  772 Batch =  0 Loss =  13.867988799368888 Gradient_max =  0.005259052121071424 learning rate ratio =  0.00019736505340533082\n",
      "Epoch =  773 Batch =  0 Loss =  13.8679862369705 Gradient_max =  0.005265440771158202 learning rate ratio =  0.00019956840604915416\n",
      "Epoch =  774 Batch =  0 Loss =  13.867983676696898 Gradient_max =  0.0052718312261562095 learning rate ratio =  0.0002018185498684567\n",
      "Epoch =  775 Batch =  0 Loss =  13.867981118546915 Gradient_max =  0.005278223488527411 learning rate ratio =  0.00020411699493454722\n",
      "Epoch =  776 Batch =  0 Loss =  13.86797856251938 Gradient_max =  0.005284617560734397 learning rate ratio =  0.00020646531699421082\n",
      "Epoch =  777 Batch =  0 Loss =  13.867976008613141 Gradient_max =  0.005291013445240382 learning rate ratio =  0.0002088651610794003\n",
      "Epoch =  778 Batch =  0 Loss =  13.867973456780085 Gradient_max =  0.005297411144647982 learning rate ratio =  0.0002113182453667568\n",
      "Epoch =  779 Batch =  0 Loss =  13.867970907066168 Gradient_max =  0.005303810661283148 learning rate ratio =  0.00021382636526305268\n",
      "Epoch =  780 Batch =  0 Loss =  13.86796835937495 Gradient_max =  0.005310211735587149 learning rate ratio =  0.000216391397865691\n",
      "Epoch =  781 Batch =  0 Loss =  13.867965813726807 Gradient_max =  0.005316614623592607 learning rate ratio =  0.00021901530650059094\n",
      "Epoch =  782 Batch =  0 Loss =  13.867963270194258 Gradient_max =  0.005323019346073805 learning rate ratio =  0.00022170014590440113\n",
      "Epoch =  783 Batch =  0 Loss =  13.867960728776138 Gradient_max =  0.005329425905508925 learning rate ratio =  0.00022444806753187463\n",
      "Epoch =  784 Batch =  0 Loss =  13.867958189471297 Gradient_max =  0.005335834304376798 learning rate ratio =  0.00022726132532281543\n",
      "Epoch =  785 Batch =  0 Loss =  13.867955652278585 Gradient_max =  0.005342244545156893 learning rate ratio =  0.0002301422818780321\n",
      "Epoch =  786 Batch =  0 Loss =  13.867953117033123 Gradient_max =  0.005348656630282892 learning rate ratio =  0.00023309341513478283\n",
      "Epoch =  787 Batch =  0 Loss =  13.867950583897816 Gradient_max =  0.005355070562282261 learning rate ratio =  0.00023611732535275297\n",
      "Epoch =  788 Batch =  0 Loss =  13.867948052755233 Gradient_max =  0.005361486343741008 learning rate ratio =  0.00023921674285947212\n",
      "Epoch =  789 Batch =  0 Loss =  13.867945523720653 Gradient_max =  0.005367903977036742 learning rate ratio =  0.00024239453634185417\n",
      "Epoch =  790 Batch =  0 Loss =  13.867942996931285 Gradient_max =  0.005374323464839531 learning rate ratio =  0.0002456507450388773\n",
      "Epoch =  791 Batch =  0 Loss =  13.867940472247748 Gradient_max =  0.005380744809445475 learning rate ratio =  0.0002489913585003187\n",
      "Epoch =  792 Batch =  0 Loss =  13.867937949954127 Gradient_max =  0.005387168013759025 learning rate ratio =  0.0002524197066064387\n",
      "Epoch =  793 Batch =  0 Loss =  13.867935429764326 Gradient_max =  0.005393593079844788 learning rate ratio =  0.00025593929696206584\n",
      "Epoch =  794 Batch =  0 Loss =  13.867932911679814 Gradient_max =  0.005400020010186639 learning rate ratio =  0.00025955382652999043\n",
      "Epoch =  795 Batch =  0 Loss =  13.867930395696812 Gradient_max =  0.005406448807271859 learning rate ratio =  0.00026326719472392283\n",
      "Epoch =  796 Batch =  0 Loss =  13.867927881814184 Gradient_max =  0.005412879473587002 learning rate ratio =  0.00026708351745014777\n",
      "Epoch =  797 Batch =  0 Loss =  13.867925370030784 Gradient_max =  0.005419312011619272 learning rate ratio =  0.0002710071423464123\n",
      "Epoch =  798 Batch =  0 Loss =  13.867922860560913 Gradient_max =  0.005425746424356015 learning rate ratio =  0.0002750562429348864\n",
      "Epoch =  799 Batch =  0 Loss =  13.867920353247232 Gradient_max =  0.00543218271388542 learning rate ratio =  0.00027922291571909374\n",
      "Epoch =  800 Batch =  0 Loss =  13.86791784802948 Gradient_max =  0.005438620882597969 learning rate ratio =  0.00028351236326305205\n",
      "Epoch =  801 Batch =  0 Loss =  13.867915344906534 Gradient_max =  0.0054450609329834785 learning rate ratio =  0.00028793009964085323\n",
      "Epoch =  802 Batch =  0 Loss =  13.86791284387725 Gradient_max =  0.005451502867532417 learning rate ratio =  0.0002924819741258303\n",
      "Epoch =  803 Batch =  0 Loss =  13.867910344804665 Gradient_max =  0.005457946688718282 learning rate ratio =  0.00029717419708978057\n",
      "Epoch =  804 Batch =  0 Loss =  13.86790784784425 Gradient_max =  0.005464392398928981 learning rate ratio =  0.0003020133682820839\n",
      "Epoch =  805 Batch =  0 Loss =  13.867905352939793 Gradient_max =  0.005470839769534911 learning rate ratio =  0.00030700650770350046\n",
      "Epoch =  806 Batch =  0 Loss =  13.867902860124415 Gradient_max =  0.0054772890365782055 learning rate ratio =  0.00031216108975250174\n",
      "Epoch =  807 Batch =  0 Loss =  13.867900369396995 Gradient_max =  0.005483740202555365 learning rate ratio =  0.00031748508058786077\n",
      "Epoch =  808 Batch =  0 Loss =  13.867897880756399 Gradient_max =  0.005490193269963549 learning rate ratio =  0.00032298697921723387\n",
      "Epoch =  809 Batch =  0 Loss =  13.867895394022986 Gradient_max =  0.0054966482412622105 learning rate ratio =  0.00032867586287614636\n",
      "Epoch =  810 Batch =  0 Loss =  13.867892909374547 Gradient_max =  0.00550310511898857 learning rate ratio =  0.00033456143685815245\n",
      "Epoch =  811 Batch =  0 Loss =  13.867890426809952 Gradient_max =  0.005509563905641786 learning rate ratio =  0.0003406540899729735\n",
      "Epoch =  812 Batch =  0 Loss =  13.867887946176085 Gradient_max =  0.0055160246036664844 learning rate ratio =  0.0003469649557759081\n",
      "Epoch =  813 Batch =  0 Loss =  13.86788546744936 Gradient_max =  0.005522487215627 learning rate ratio =  0.0003535059805322138\n",
      "Epoch =  814 Batch =  0 Loss =  13.867882990803876 Gradient_max =  0.0055289517440165565 learning rate ratio =  0.00036028999854559595\n",
      "Epoch =  815 Batch =  0 Loss =  13.867880516156779 Gradient_max =  0.0055354181914136664 learning rate ratio =  0.0003673308164578416\n",
      "Epoch =  816 Batch =  0 Loss =  13.8678780435888 Gradient_max =  0.0055418865602443 learning rate ratio =  0.0003746433070925423\n",
      "Epoch =  817 Batch =  0 Loss =  13.867875572989032 Gradient_max =  0.005548356641386321 learning rate ratio =  0.0003822435143928398\n",
      "Epoch =  818 Batch =  0 Loss =  13.867873104466058 Gradient_max =  0.005554828650769782 learning rate ratio =  0.0003901487708311977\n",
      "Epoch =  819 Batch =  0 Loss =  13.867870637843387 Gradient_max =  0.0055613025910156945 learning rate ratio =  0.00039837782967051154\n",
      "Epoch =  820 Batch =  0 Loss =  13.867868173332207 Gradient_max =  0.005567778464598388 learning rate ratio =  0.0004069510133829596\n",
      "Epoch =  821 Batch =  0 Loss =  13.867865710914833 Gradient_max =  0.0055742562738762 learning rate ratio =  0.0004158903815466823\n",
      "Epoch =  822 Batch =  0 Loss =  13.867863250617653 Gradient_max =  0.005580736021497106 learning rate ratio =  0.0004252199201057829\n",
      "Epoch =  823 Batch =  0 Loss =  13.867860792235318 Gradient_max =  0.005587217709880261 learning rate ratio =  0.00043496575681818006\n",
      "Epoch =  824 Batch =  0 Loss =  13.867858335753954 Gradient_max =  0.005593701341648262 learning rate ratio =  0.0004451564053393236\n",
      "Epoch =  825 Batch =  0 Loss =  13.867855881342763 Gradient_max =  0.005600186919219715 learning rate ratio =  0.0004558230441671469\n",
      "Epoch =  826 Batch =  0 Loss =  13.867853428994389 Gradient_max =  0.005606674445132079 learning rate ratio =  0.0004669998353555962\n",
      "Epoch =  827 Batch =  0 Loss =  13.867850978764299 Gradient_max =  0.0056131639218901285 learning rate ratio =  0.0004787242901975132\n",
      "Epoch =  828 Batch =  0 Loss =  13.867848530601098 Gradient_max =  0.0056196553519880215 learning rate ratio =  0.0004910376898541059\n",
      "Epoch =  829 Batch =  0 Loss =  13.867846084503675 Gradient_max =  0.005626148737939223 learning rate ratio =  0.0005039855712598839\n",
      "Epoch =  830 Batch =  0 Loss =  13.867843640470936 Gradient_max =  0.005632644082257881 learning rate ratio =  0.0005176182900194866\n",
      "Epoch =  831 Batch =  0 Loss =  13.867841198501765 Gradient_max =  0.005639141387458837 learning rate ratio =  0.0005319916751110247\n",
      "Epoch =  832 Batch =  0 Loss =  13.867838758595061 Gradient_max =  0.00564564065605761 learning rate ratio =  0.0005471677933149841\n",
      "Epoch =  833 Batch =  0 Loss =  13.867836320574176 Gradient_max =  0.0056521418905942965 learning rate ratio =  0.0005632158455858848\n",
      "Epoch =  834 Batch =  0 Loss =  13.867833884613974 Gradient_max =  0.005658645093562304 learning rate ratio =  0.0005802132220236041\n",
      "Epoch =  835 Batch =  0 Loss =  13.867831450519052 Gradient_max =  0.005665150267575487 learning rate ratio =  0.0005982467504879814\n",
      "Epoch =  836 Batch =  0 Loss =  13.867829018288818 Gradient_max =  0.005671657415152697 learning rate ratio =  0.0006174141804749352\n",
      "Epoch =  837 Batch =  0 Loss =  13.867826587960426 Gradient_max =  0.005678166538682038 learning rate ratio =  0.0006378259561896022\n",
      "Epoch =  838 Batch =  0 Loss =  13.867824159689286 Gradient_max =  0.005684677640718228 learning rate ratio =  0.0006596073461227436\n",
      "Epoch =  839 Batch =  0 Loss =  13.867821733474303 Gradient_max =  0.005691190723781624 learning rate ratio =  0.0006829010170071535\n",
      "Epoch =  840 Batch =  0 Loss =  13.867819309314383 Gradient_max =  0.005697705790393283 learning rate ratio =  0.0007078701628635217\n",
      "Epoch =  841 Batch =  0 Loss =  13.867816887208425 Gradient_max =  0.005704222843074958 learning rate ratio =  0.0007347023346606002\n",
      "Epoch =  842 Batch =  0 Loss =  13.867814467155341 Gradient_max =  0.005710741884349096 learning rate ratio =  0.0007636141605800729\n",
      "Epoch =  843 Batch =  0 Loss =  13.867812049214855 Gradient_max =  0.005717262916882836 learning rate ratio =  0.0007948572080535733\n",
      "Epoch =  844 Batch =  0 Loss =  13.867809633431126 Gradient_max =  0.005723785943153257 learning rate ratio =  0.0008287253244263237\n",
      "Epoch =  845 Batch =  0 Loss =  13.867807219696916 Gradient_max =  0.005730310965587329 learning rate ratio =  0.0008655639081308304\n",
      "Epoch =  846 Batch =  0 Loss =  13.867804807849044 Gradient_max =  0.00573683798677319 learning rate ratio =  0.0009057817322457023\n",
      "Epoch =  847 Batch =  0 Loss =  13.86780239789762 Gradient_max =  0.005743367009307791 learning rate ratio =  0.0009498661784166595\n",
      "Epoch =  848 Batch =  0 Loss =  13.867799989707477 Gradient_max =  0.005749898035626463 learning rate ratio =  0.0009984030879123043\n",
      "Epoch =  849 Batch =  0 Loss =  13.867797583563657 Gradient_max =  0.005756431068215216 learning rate ratio =  0.0010521029457333548\n",
      "Epoch =  850 Batch =  0 Loss =  13.867795179465078 Gradient_max =  0.0057629661096020995 learning rate ratio =  0.00111183589294481\n",
      "Epoch =  851 Batch =  0 Loss =  13.867792777410646 Gradient_max =  0.005769503162315878 learning rate ratio =  0.0011786792420861356\n",
      "Epoch =  852 Batch =  0 Loss =  13.86779037739928 Gradient_max =  0.00577604222888603 learning rate ratio =  0.001253983036093013\n",
      "Epoch =  853 Batch =  0 Loss =  13.867787979429908 Gradient_max =  0.005782583311842733 learning rate ratio =  0.0013394621844369306\n",
      "Epoch =  854 Batch =  0 Loss =  13.867785583501437 Gradient_max =  0.005789126413716875 learning rate ratio =  0.0014373286572866093\n",
      "Epoch =  855 Batch =  0 Loss =  13.867783189612789 Gradient_max =  0.005795671537040055 learning rate ratio =  0.0015504856437325166\n",
      "Epoch =  856 Batch =  0 Loss =  13.867780797431962 Gradient_max =  0.005802218676149209 learning rate ratio =  0.0016828204322380208\n",
      "Epoch =  857 Batch =  0 Loss =  13.867778407288696 Gradient_max =  0.005808767856253083 learning rate ratio =  0.001839659991844393\n",
      "Epoch =  858 Batch =  0 Loss =  13.867776019181914 Gradient_max =  0.005815319079900521 learning rate ratio =  0.0020285054764691995\n",
      "Epoch =  859 Batch =  0 Loss =  13.867773633110543 Gradient_max =  0.0058218723496410996 learning rate ratio =  0.00226026749939996\n",
      "Epoch =  860 Batch =  0 Loss =  13.867771249073504 Gradient_max =  0.005828427668025115 learning rate ratio =  0.0025514515713263575\n",
      "Epoch =  861 Batch =  0 Loss =  13.867768867069724 Gradient_max =  0.005834985037603589 learning rate ratio =  0.002928271751842012\n",
      "Epoch =  862 Batch =  0 Loss =  13.867766487098127 Gradient_max =  0.005841544460928266 learning rate ratio =  0.0034350195070152684\n",
      "Epoch =  863 Batch =  0 Loss =  13.867764109174887 Gradient_max =  0.005848105940506562 learning rate ratio =  0.004152892761552644\n",
      "Epoch =  864 Batch =  0 Loss =  13.867761733114579 Gradient_max =  0.00585466947905034 learning rate ratio =  0.005248551880346011\n",
      "Epoch =  865 Batch =  0 Loss =  13.867759358925246 Gradient_max =  0.005861235078937352 learning rate ratio =  0.007126709226404299\n",
      "Epoch =  866 Batch =  0 Loss =  13.86775698676415 Gradient_max =  0.005867802742785179 learning rate ratio =  0.011091236780811847\n",
      "Epoch =  867 Batch =  0 Loss =  13.867754616630211 Gradient_max =  0.005874372473149222 learning rate ratio =  0.024962086003737022\n",
      "Epoch =  868 Batch =  0 Loss =  13.867752248495764 Gradient_max =  0.0058809442724218 learning rate ratio =  2.3041411490532632e-05\n",
      "Epoch =  869 Batch =  0 Loss =  13.867749882386311 Gradient_max =  0.005887518143323463 learning rate ratio =  2.306951541066671e-05\n",
      "Epoch =  870 Batch =  0 Loss =  13.867747518300789 Gradient_max =  0.005894094088411806 learning rate ratio =  2.3097628911463627e-05\n",
      "Epoch =  871 Batch =  0 Loss =  13.867745155991392 Gradient_max =  0.005900672210362141 learning rate ratio =  2.3125752001552405e-05\n",
      "Epoch =  872 Batch =  0 Loss =  13.867742795704002 Gradient_max =  0.0059072524188693385 learning rate ratio =  2.31538846968623e-05\n",
      "Epoch =  873 Batch =  0 Loss =  13.867740437510564 Gradient_max =  0.005913834716628457 learning rate ratio =  2.3182027008512224e-05\n",
      "Epoch =  874 Batch =  0 Loss =  13.86773808116528 Gradient_max =  0.005920419106001062 learning rate ratio =  2.32101789503561e-05\n",
      "Epoch =  875 Batch =  0 Loss =  13.867735726830983 Gradient_max =  0.005927005589641334 learning rate ratio =  2.3238340535331226e-05\n",
      "Epoch =  876 Batch =  0 Loss =  13.867733374389221 Gradient_max =  0.005933594170122218 learning rate ratio =  2.3266511773265287e-05\n",
      "Epoch =  877 Batch =  0 Loss =  13.867731023964694 Gradient_max =  0.0059401848500038835 learning rate ratio =  2.3294692677950108e-05\n",
      "Epoch =  878 Batch =  0 Loss =  13.867728675556345 Gradient_max =  0.005946777631857235 learning rate ratio =  2.3322883261710606e-05\n",
      "Epoch =  879 Batch =  0 Loss =  13.867726329242199 Gradient_max =  0.005953372518329452 learning rate ratio =  2.3351083536289492e-05\n",
      "Epoch =  880 Batch =  0 Loss =  13.867723985068768 Gradient_max =  0.005959969511915789 learning rate ratio =  2.3379293513178752e-05\n",
      "Epoch =  881 Batch =  0 Loss =  13.867721642572132 Gradient_max =  0.005966568614924091 learning rate ratio =  2.3407513207927715e-05\n",
      "Epoch =  882 Batch =  0 Loss =  13.867719302087654 Gradient_max =  0.005973169830194984 learning rate ratio =  2.3435742631093503e-05\n",
      "Epoch =  883 Batch =  0 Loss =  13.86771696346918 Gradient_max =  0.005979773160293617 learning rate ratio =  2.3463981795257854e-05\n",
      "Epoch =  884 Batch =  0 Loss =  13.867714626860923 Gradient_max =  0.005986378607805108 learning rate ratio =  2.349223071254459e-05\n",
      "Epoch =  885 Batch =  0 Loss =  13.867712292068454 Gradient_max =  0.005992986175399982 learning rate ratio =  2.3520489393807958e-05\n",
      "Epoch =  886 Batch =  0 Loss =  13.86770995915112 Gradient_max =  0.005999595865497854 learning rate ratio =  2.3548757855239517e-05\n",
      "Epoch =  887 Batch =  0 Loss =  13.867707628241105 Gradient_max =  0.006006207680739724 learning rate ratio =  2.3577036106896446e-05\n",
      "Epoch =  888 Batch =  0 Loss =  13.86770529933736 Gradient_max =  0.006012821623704031 learning rate ratio =  2.3605324161158364e-05\n",
      "Epoch =  889 Batch =  0 Loss =  13.867702972438842 Gradient_max =  0.006019437696969987 learning rate ratio =  2.3633622030410414e-05\n",
      "Epoch =  890 Batch =  0 Loss =  13.867700647603726 Gradient_max =  0.00602605590325413 learning rate ratio =  2.3661962901594074e-05\n",
      "Epoch =  891 Batch =  0 Loss =  13.867698324771744 Gradient_max =  0.006032676245000709 learning rate ratio =  2.369031375211901e-05\n",
      "Epoch =  892 Batch =  0 Loss =  13.86769600406986 Gradient_max =  0.006039298724837631 learning rate ratio =  2.3718681277582667e-05\n",
      "Epoch =  893 Batch =  0 Loss =  13.86769368544096 Gradient_max =  0.006045923345137245 learning rate ratio =  2.3747058846790882e-05\n",
      "Epoch =  894 Batch =  0 Loss =  13.86769136869913 Gradient_max =  0.006052549808358993 learning rate ratio =  2.377544647542587e-05\n",
      "Epoch =  895 Batch =  0 Loss =  13.867689053815685 Gradient_max =  0.006059178420212663 learning rate ratio =  2.38038441769788e-05\n",
      "Epoch =  896 Batch =  0 Loss =  13.867686740930722 Gradient_max =  0.006065809183293088 learning rate ratio =  2.3832251960954438e-05\n",
      "Epoch =  897 Batch =  0 Loss =  13.867684423165285 Gradient_max =  0.006072442087629644 learning rate ratio =  0.00016068136913341812\n",
      "Epoch =  898 Batch =  0 Loss =  13.867682095522559 Gradient_max =  0.00607907712651079 learning rate ratio =  0.00016129015227526555\n",
      "Epoch =  899 Batch =  0 Loss =  13.867679769868019 Gradient_max =  0.00608571432437271 learning rate ratio =  0.00016190327093043862\n",
      "Epoch =  900 Batch =  0 Loss =  13.867677446163189 Gradient_max =  0.006092353683929291 learning rate ratio =  0.0001625174747338569\n",
      "Epoch =  901 Batch =  0 Loss =  13.867675124444684 Gradient_max =  0.006098995207649147 learning rate ratio =  0.00016313605849039493\n",
      "Epoch =  902 Batch =  0 Loss =  13.867672804711459 Gradient_max =  0.0061056388981245695 learning rate ratio =  0.00016375906896420956\n",
      "Epoch =  903 Batch =  0 Loss =  13.867670486962473 Gradient_max =  0.006112284757948618 learning rate ratio =  0.0001643865535876798\n",
      "Epoch =  904 Batch =  0 Loss =  13.867668171196689 Gradient_max =  0.006118932789715141 learning rate ratio =  0.00016501856047338457\n",
      "Epoch =  905 Batch =  0 Loss =  13.867665857413073 Gradient_max =  0.006125582996018747 learning rate ratio =  0.0001656551384263401\n",
      "Epoch =  906 Batch =  0 Loss =  13.867663545729974 Gradient_max =  0.00613223537931117 learning rate ratio =  0.00016629633695132678\n",
      "Epoch =  907 Batch =  0 Loss =  13.867661236101553 Gradient_max =  0.0061388899424335164 learning rate ratio =  0.00016694220628190334\n",
      "Epoch =  908 Batch =  0 Loss =  13.86765892838697 Gradient_max =  0.006145546687746304 learning rate ratio =  0.0001675927973759652\n",
      "Epoch =  909 Batch =  0 Loss =  13.86765662265046 Gradient_max =  0.006152205617981783 learning rate ratio =  0.00016824816193557085\n",
      "Epoch =  910 Batch =  0 Loss =  13.867654318930107 Gradient_max =  0.006158866907434129 learning rate ratio =  0.00016890835244404013\n",
      "Epoch =  911 Batch =  0 Loss =  13.867652017185838 Gradient_max =  0.006165530389622703 learning rate ratio =  0.00016957342211052658\n",
      "Epoch =  912 Batch =  0 Loss =  13.867649717375317 Gradient_max =  0.006172196067073385 learning rate ratio =  0.00017024342495323798\n",
      "Epoch =  913 Batch =  0 Loss =  13.867647419538761 Gradient_max =  0.006178863942466179 learning rate ratio =  0.00017091841579013102\n",
      "Epoch =  914 Batch =  0 Loss =  13.867645123670219 Gradient_max =  0.00618553401835606 learning rate ratio =  0.00017159845023139182\n",
      "Epoch =  915 Batch =  0 Loss =  13.867642829773427 Gradient_max =  0.006192206297397065 learning rate ratio =  0.00017228358476434048\n",
      "Epoch =  916 Batch =  0 Loss =  13.867640537847361 Gradient_max =  0.006198880782194971 learning rate ratio =  0.00017297387669931158\n",
      "Epoch =  917 Batch =  0 Loss =  13.867638248136293 Gradient_max =  0.006205557475734133 learning rate ratio =  0.00017366938421537403\n",
      "Epoch =  918 Batch =  0 Loss =  13.867635960394017 Gradient_max =  0.006212236380244372 learning rate ratio =  0.00017437016635653112\n",
      "Epoch =  919 Batch =  0 Loss =  13.867633674619517 Gradient_max =  0.006218917498333841 learning rate ratio =  0.00017507628306838083\n",
      "Epoch =  920 Batch =  0 Loss =  13.867631390867848 Gradient_max =  0.006225600832772701 learning rate ratio =  0.00017578779522669385\n",
      "Epoch =  921 Batch =  0 Loss =  13.86762910908193 Gradient_max =  0.006232286386009637 learning rate ratio =  0.00017650476460041907\n",
      "Epoch =  922 Batch =  0 Loss =  13.867626829131217 Gradient_max =  0.006238973924031093 learning rate ratio =  0.00017722725392307803\n",
      "Epoch =  923 Batch =  0 Loss =  13.867624551144184 Gradient_max =  0.006245663691597518 learning rate ratio =  0.00017795532689473766\n",
      "Epoch =  924 Batch =  0 Loss =  13.86762227511982 Gradient_max =  0.006252355691326672 learning rate ratio =  0.0001786890481989142\n",
      "Epoch =  925 Batch =  0 Loss =  13.867620001110858 Gradient_max =  0.006259049925977588 learning rate ratio =  0.00017942848352122873\n",
      "Epoch =  926 Batch =  0 Loss =  13.86761772906255 Gradient_max =  0.006265746398029255 learning rate ratio =  0.0001801736995719205\n",
      "Epoch =  927 Batch =  0 Loss =  13.86761545898317 Gradient_max =  0.006272445110149381 learning rate ratio =  0.00018092476409534502\n",
      "Epoch =  928 Batch =  0 Loss =  13.867613190862405 Gradient_max =  0.006279146064911422 learning rate ratio =  0.00018168174591384534\n",
      "Epoch =  929 Batch =  0 Loss =  13.867610924699237 Gradient_max =  0.006285849264937182 learning rate ratio =  0.00018244471492632406\n",
      "Epoch =  930 Batch =  0 Loss =  13.867608660492655 Gradient_max =  0.0062925547128492645 learning rate ratio =  0.00018321374213709892\n",
      "Epoch =  931 Batch =  0 Loss =  13.867606398326112 Gradient_max =  0.006299262855650062 learning rate ratio =  0.00018398889966825371\n",
      "Epoch =  932 Batch =  0 Loss =  13.867604138114283 Gradient_max =  0.006305973255909653 learning rate ratio =  0.00018477026081069643\n",
      "Epoch =  933 Batch =  0 Loss =  13.867601879856164 Gradient_max =  0.006312685916257053 learning rate ratio =  0.00018555790001873495\n",
      "Epoch =  934 Batch =  0 Loss =  13.867599623550737 Gradient_max =  0.006319400839322096 learning rate ratio =  0.00018635189294305372\n",
      "Epoch =  935 Batch =  0 Loss =  13.867597369197005 Gradient_max =  0.00632611802773544 learning rate ratio =  0.00018715231645484128\n",
      "Epoch =  936 Batch =  0 Loss =  13.867595116793968 Gradient_max =  0.006332837484128545 learning rate ratio =  0.0001879592486705038\n",
      "Epoch =  937 Batch =  0 Loss =  13.867592866223124 Gradient_max =  0.006339559211262077 learning rate ratio =  0.00018876800144306446\n",
      "Epoch =  938 Batch =  0 Loss =  13.867590617601508 Gradient_max =  0.006346283211641264 learning rate ratio =  0.00018958334287733203\n",
      "Epoch =  939 Batch =  0 Loss =  13.867588370928093 Gradient_max =  0.006353009487900027 learning rate ratio =  0.00019040535348473824\n",
      "Epoch =  940 Batch =  0 Loss =  13.867586126079742 Gradient_max =  0.006359738042558223 learning rate ratio =  0.00019124549912721652\n",
      "Epoch =  941 Batch =  0 Loss =  13.867583883178224 Gradient_max =  0.006366468878366862 learning rate ratio =  0.00019209267427839269\n",
      "Epoch =  942 Batch =  0 Loss =  13.867581642222552 Gradient_max =  0.006373201997962346 learning rate ratio =  0.0001929469671859064\n",
      "Epoch =  943 Batch =  0 Loss =  13.867579403289483 Gradient_max =  0.006379937404038007 learning rate ratio =  0.000193808467574085\n",
      "Epoch =  944 Batch =  0 Loss =  13.867577166300414 Gradient_max =  0.006386675099175898 learning rate ratio =  0.00019467726669554724\n",
      "Epoch =  945 Batch =  0 Loss =  13.867574931254346 Gradient_max =  0.006393415086014882 learning rate ratio =  0.0001955534573431527\n",
      "Epoch =  946 Batch =  0 Loss =  13.86757269792313 Gradient_max =  0.006400157366762007 learning rate ratio =  0.00019644928231541336\n",
      "Epoch =  947 Batch =  0 Loss =  13.867570466532914 Gradient_max =  0.006406901944490334 learning rate ratio =  0.00019735290558474855\n",
      "Epoch =  948 Batch =  0 Loss =  13.867568237555922 Gradient_max =  0.006413648822394624 learning rate ratio =  0.00019826442902998255\n",
      "Epoch =  949 Batch =  0 Loss =  13.867566010768329 Gradient_max =  0.00642039800283847 learning rate ratio =  0.0001991839563530699\n",
      "Epoch =  950 Batch =  0 Loss =  13.867563785919256 Gradient_max =  0.00642714948819073 learning rate ratio =  0.0002001115930725278\n",
      "Epoch =  951 Batch =  0 Loss =  13.867561562429122 Gradient_max =  0.006433903280882563 learning rate ratio =  0.0002010445458631246\n",
      "Epoch =  952 Batch =  0 Loss =  13.867559340876738 Gradient_max =  0.006440659383772389 learning rate ratio =  0.00020198577234144854\n",
      "Epoch =  953 Batch =  0 Loss =  13.86755712126112 Gradient_max =  0.00644741779950573 learning rate ratio =  0.00020293538266688644\n",
      "Epoch =  954 Batch =  0 Loss =  13.867554903581276 Gradient_max =  0.006454178530728947 learning rate ratio =  0.00020389348896334062\n",
      "Epoch =  955 Batch =  0 Loss =  13.867552687836223 Gradient_max =  0.006460941580089239 learning rate ratio =  0.00020486020536321654\n",
      "Epoch =  956 Batch =  0 Loss =  13.867550473832075 Gradient_max =  0.006467706950016574 learning rate ratio =  0.00020583940819079714\n",
      "Epoch =  957 Batch =  0 Loss =  13.867548261760778 Gradient_max =  0.006474474643377912 learning rate ratio =  0.0002068275259774288\n",
      "Epoch =  958 Batch =  0 Loss =  13.867546051427658 Gradient_max =  0.006481244662946692 learning rate ratio =  0.00020781809956335517\n",
      "Epoch =  959 Batch =  0 Loss =  13.867543843026013 Gradient_max =  0.006488017011250448 learning rate ratio =  0.00020881771047477082\n",
      "Epoch =  960 Batch =  0 Loss =  13.867541636554854 Gradient_max =  0.006494791690940581 learning rate ratio =  0.000209826482536375\n",
      "Epoch =  961 Batch =  0 Loss =  13.867539432013213 Gradient_max =  0.006501568704669328 learning rate ratio =  0.0002108445418455539\n",
      "Epoch =  962 Batch =  0 Loss =  13.867537229400103 Gradient_max =  0.006508348055089793 learning rate ratio =  0.00021187201682476147\n",
      "Epoch =  963 Batch =  0 Loss =  13.86753502842819 Gradient_max =  0.006515129745396066 learning rate ratio =  0.00021289257589981135\n",
      "Epoch =  964 Batch =  0 Loss =  13.867532829383993 Gradient_max =  0.006521913777704079 learning rate ratio =  0.0002139225017763576\n",
      "Epoch =  965 Batch =  0 Loss =  13.86753063213069 Gradient_max =  0.006528700133536646 learning rate ratio =  0.00021496192355315244\n",
      "Epoch =  966 Batch =  0 Loss =  13.867528436803072 Gradient_max =  0.006535488839844001 learning rate ratio =  0.00021601097274202133\n",
      "Epoch =  967 Batch =  0 Loss =  13.867526242972676 Gradient_max =  0.006542279899175144 learning rate ratio =  0.00021707605283992164\n",
      "Epoch =  968 Batch =  0 Loss =  13.867524051066756 Gradient_max =  0.006549073314303411 learning rate ratio =  0.00021815115213727585\n",
      "Epoch =  969 Batch =  0 Loss =  13.8675218608106 Gradient_max =  0.0065558690874322895 learning rate ratio =  0.0002192364122683471\n",
      "Epoch =  970 Batch =  0 Loss =  13.867519672476654 Gradient_max =  0.006562667221682902 learning rate ratio =  0.0002203319774511843\n",
      "Epoch =  971 Batch =  0 Loss =  13.867517486532124 Gradient_max =  0.006569467719706681 learning rate ratio =  0.00022143799456643976\n",
      "Epoch =  972 Batch =  0 Loss =  13.867515302508032 Gradient_max =  0.006576270584180465 learning rate ratio =  0.00022255461356151732\n",
      "Epoch =  973 Batch =  0 Loss =  13.867513120527644 Gradient_max =  0.006583075817832446 learning rate ratio =  0.00022368198712924747\n",
      "Epoch =  974 Batch =  0 Loss =  13.86751094046584 Gradient_max =  0.006589883423266394 learning rate ratio =  0.00022482027091823482\n",
      "Epoch =  975 Batch =  0 Loss =  13.867508762321654 Gradient_max =  0.006596693403149581 learning rate ratio =  0.0002259696235908078\n",
      "Epoch =  976 Batch =  0 Loss =  13.867506586094118 Gradient_max =  0.00660350576015013 learning rate ratio =  0.00022713020690098463\n",
      "Epoch =  977 Batch =  0 Loss =  13.867504411782267 Gradient_max =  0.006610320496937028 learning rate ratio =  0.00022830218577015048\n",
      "Epoch =  978 Batch =  0 Loss =  13.867502239385141 Gradient_max =  0.006617137616180127 learning rate ratio =  0.00022948572836496283\n",
      "Epoch =  979 Batch =  0 Loss =  13.86750006888609 Gradient_max =  0.006623957121089166 learning rate ratio =  0.0002306802020190786\n",
      "Epoch =  980 Batch =  0 Loss =  13.867497900542645 Gradient_max =  0.006630779014542167 learning rate ratio =  0.0002318865695762939\n",
      "Epoch =  981 Batch =  0 Loss =  13.867495734111412 Gradient_max =  0.006637603298467344 learning rate ratio =  0.00023310500899893175\n",
      "Epoch =  982 Batch =  0 Loss =  13.867493569591437 Gradient_max =  0.006644429975538008 learning rate ratio =  0.00023433570188053686\n",
      "Epoch =  983 Batch =  0 Loss =  13.867491407114262 Gradient_max =  0.006651259048500542 learning rate ratio =  0.00023557883346921556\n",
      "Epoch =  984 Batch =  0 Loss =  13.867489246546569 Gradient_max =  0.006658090519957857 learning rate ratio =  0.0002368345927789263\n",
      "Epoch =  985 Batch =  0 Loss =  13.867487087887403 Gradient_max =  0.006664924392585875 learning rate ratio =  0.00023810317266664615\n",
      "Epoch =  986 Batch =  0 Loss =  13.867484931111822 Gradient_max =  0.0066717606689947065 learning rate ratio =  0.00023938476993503837\n",
      "Epoch =  987 Batch =  0 Loss =  13.867482776016224 Gradient_max =  0.006678599351523979 learning rate ratio =  0.00024067958544515732\n",
      "Epoch =  988 Batch =  0 Loss =  13.867480622825864 Gradient_max =  0.0066854404432563535 learning rate ratio =  0.00024198782419894684\n",
      "Epoch =  989 Batch =  0 Loss =  13.867478471539783 Gradient_max =  0.006692283946871252 learning rate ratio =  0.00024330969546549938\n",
      "Epoch =  990 Batch =  0 Loss =  13.86747632190692 Gradient_max =  0.006699129864809502 learning rate ratio =  0.00024465019593781525\n",
      "Epoch =  991 Batch =  0 Loss =  13.867474174176527 Gradient_max =  0.006705978199991818 learning rate ratio =  0.00024600486439357703\n",
      "Epoch =  992 Batch =  0 Loss =  13.86747202834765 Gradient_max =  0.006712828955100266 learning rate ratio =  0.0002473739260666291\n",
      "Epoch =  993 Batch =  0 Loss =  13.86746988432466 Gradient_max =  0.006719682132426886 learning rate ratio =  0.00024876866139294677\n",
      "Epoch =  994 Batch =  0 Loss =  13.867467742278533 Gradient_max =  0.006726537735046369 learning rate ratio =  0.00025017849991167465\n",
      "Epoch =  995 Batch =  0 Loss =  13.867465602131821 Gradient_max =  0.00673339576564363 learning rate ratio =  0.000251603687689061\n",
      "Epoch =  996 Batch =  0 Loss =  13.867463463563606 Gradient_max =  0.0067402562266419205 learning rate ratio =  0.00025305438364186785\n",
      "Epoch =  997 Batch =  0 Loss =  13.867461326893512 Gradient_max =  0.006747119120990494 learning rate ratio =  0.00025452116213695757\n",
      "Epoch =  998 Batch =  0 Loss =  13.867459192120592 Gradient_max =  0.0067539844513767044 learning rate ratio =  0.00025600429147143954\n",
      "Epoch =  999 Batch =  0 Loss =  13.867457059243915 Gradient_max =  0.0067608522204887795 learning rate ratio =  0.0002575040459439282\n"
     ]
    }
   ],
   "source": [
    "SGD = NewSolver(Model, x_train, y_train, lr = 1.9e-5, batch_size = 20, num_epochs = 1000, print_every = 1000)\n",
    "SGD.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(0,SGD.num_epochs)\n",
    "test_epochs  = range(0,SGD.loss_history.shape[0])\n",
    "plt.plot(test_epochs, SGD.loss_history, label = 'Loss Batch 0 ', color = 'red')\n",
    "plt.plot(test_epochs, SGD.loss_2_history, label = 'Loss Batch 35', color = 'blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epochs, SGD.vel_history, label = 'Velocity', color = 'blue')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_epochs, SGD.grad_history, label = 'Absolute Gradient', color = 'green')\n",
    "plt.plot(test_epochs, SGD.grad_2_history, label = 'Absolute Gradient_2', color = 'yellow')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 2)\n",
      "1258\n",
      "(1279, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5371383893666928"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def training_accuracy(model, x_train,y_train):\n",
    "    y_pred = model.predict(x_train)\n",
    "    y_new = y_train.reshape(y_pred.shape)\n",
    "    return np.sum(y_new == y_pred) / y_new.shape[0]\n",
    "\n",
    "print(np.sum(Model.predict(x_train)))\n",
    "training_accuracy(Model, x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.540625"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_accuracy(model, x_test, y_test, mu, sigma):\n",
    "    y_pred = model.predict((x_test - mu) / sigma)\n",
    "    y_new = y_test.reshape(y_pred.shape)\n",
    "    return np.sum(y_new == y_pred) / y_new.shape[0]\n",
    "\n",
    "x_test_new = x_test\n",
    "x_test_new = x_test_new \n",
    "test_accuracy(Model, x_test_new, y_test, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8201e9342a4a492ddbd4e81efec90b2ccf0d205cda2cc39ac893f0c43374b5e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
