{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Models.Layers import *\n",
    "from Solver import *\n",
    "from Models.Classifiers.Neural_Net import *\n",
    "from General_Solver import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n",
      "320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>k_value</th>\n",
       "      <th>l_value</th>\n",
       "      <th>m_value</th>\n",
       "      <th>percentage_free_sulphur</th>\n",
       "      <th>n_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>8.100</td>\n",
       "      <td>4.0500</td>\n",
       "      <td>0.636</td>\n",
       "      <td>30.909091</td>\n",
       "      <td>0.6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>8.680</td>\n",
       "      <td>4.3400</td>\n",
       "      <td>0.778</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>0.8290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>8.560</td>\n",
       "      <td>4.2800</td>\n",
       "      <td>0.742</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.7440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "      <td>11.480</td>\n",
       "      <td>5.7400</td>\n",
       "      <td>0.655</td>\n",
       "      <td>35.294118</td>\n",
       "      <td>0.7195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>8.100</td>\n",
       "      <td>4.0500</td>\n",
       "      <td>0.636</td>\n",
       "      <td>30.909091</td>\n",
       "      <td>0.6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0</td>\n",
       "      <td>6.800</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>0.670</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.6610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.450</td>\n",
       "      <td>3.2250</td>\n",
       "      <td>0.822</td>\n",
       "      <td>13.076923</td>\n",
       "      <td>0.7110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.810</td>\n",
       "      <td>3.4050</td>\n",
       "      <td>0.826</td>\n",
       "      <td>13.793103</td>\n",
       "      <td>0.7540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.545</td>\n",
       "      <td>3.2725</td>\n",
       "      <td>0.785</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.6615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.310</td>\n",
       "      <td>3.1550</td>\n",
       "      <td>0.727</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>1.2075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  k_value  l_value  m_value  percentage_free_sulphur  \\\n",
       "0         9.4        0    8.100   4.0500    0.636                30.909091   \n",
       "1         9.8        0    8.680   4.3400    0.778                26.800000   \n",
       "2         9.8        0    8.560   4.2800    0.742                36.000000   \n",
       "3         9.8        1   11.480   5.7400    0.655                35.294118   \n",
       "4         9.4        0    8.100   4.0500    0.636                30.909091   \n",
       "...       ...      ...      ...      ...      ...                      ...   \n",
       "1594     10.5        0    6.800   3.4000    0.670                13.750000   \n",
       "1595     11.2        1    6.450   3.2250    0.822                13.076923   \n",
       "1596     11.0        1    6.810   3.4050    0.826                13.793103   \n",
       "1597     10.2        0    6.545   3.2725    0.785                13.750000   \n",
       "1598     11.0        1    6.310   3.1550    0.727                23.333333   \n",
       "\n",
       "      n_value  \n",
       "0      0.6080  \n",
       "1      0.8290  \n",
       "2      0.7440  \n",
       "3      0.7195  \n",
       "4      0.6080  \n",
       "...       ...  \n",
       "1594   0.6610  \n",
       "1595   0.7110  \n",
       "1596   0.7540  \n",
       "1597   0.6615  \n",
       "1598   1.2075  \n",
       "\n",
       "[1599 rows x 17 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first step would be to import the dataset\n",
    "X_full = pd.read_csv('./Datasets/red_wine_dataset.csv')\n",
    "percentage = 0.8\n",
    "# X_full.pop('k_value')\n",
    "# X_full.pop('l_value')\n",
    "# X_full.pop('m_value')\n",
    "X_train = X_full.sample(frac=percentage, random_state=0)\n",
    "y_train = X_train.pop('quality')\n",
    "X_test = X_full.drop(X_train.index)\n",
    "y_test = X_test.pop('quality')\n",
    "print(len(X_train.index))\n",
    "print(len(X_test.index))\n",
    "X_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 16)\n",
      "(320, 16)\n",
      "float64\n",
      "(1279,)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train.to_numpy()\n",
    "x_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_train.dtype)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 16)\n",
      "(1279,)\n",
      "0.12433391471702653\n",
      "0.10003662221022867\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_max = np.max(x_train, axis = 0)\n",
    "x_min = np.min(x_train, axis = 0)\n",
    "x_train = (x_train - x_min) / (x_max - x_min)\n",
    "# mu = x_train.mean(axis = 0)\n",
    "# sigma = x_train.std(axis = 0)\n",
    "# x_train  = (x_train - x_train.mean(axis = 0))/x_train.std(axis = 0)\n",
    "mu = x_min\n",
    "sigma = (x_max - x_min)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=50, cmap='spring')\n",
    "print(x_train[:,3].mean())\n",
    "print(x_train[:,3].std())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TwoLayerNet():\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, reg = 0.01):\n",
    "#         self.reg = reg\n",
    "#         self.params = {}\n",
    "#         # Now we do intialization of the weights\n",
    "#         np.random.seed(0)\n",
    "#         self.params['W1'] = np.random.randn(input_dim, hidden_dim)*0.01\n",
    "#         self.params['b1'] = np.zeros(hidden_dim)\n",
    "#         self.params['W2'] = np.random.randn(hidden_dim, output_dim)*0.01\n",
    "#         self.params['b2'] = np.zeros(output_dim)\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#     def loss(self,X,y = None):\n",
    "#         # define a mode here, i.e. a training mode or a test mode\n",
    "#         mode = 'test' if y is None else 'train'\n",
    "#         if(mode == 'train'):\n",
    "#             cache = {}\n",
    "#             loss = 0\n",
    "#             grads = {}\n",
    "#             z1, cache['affine_1'] = affine_forward(X,self.params['W1'],self.params['b1'])\n",
    "#             a1,cache['relu_1'] = relu_forward(z1)\n",
    "#             a2,cache['affine_2'] = affine_forward(a1,self.params['W2'], self.params['b2'])\n",
    "#             loss, da2 = softmax_loss(a2,y)\n",
    "#             da1, grads['W2'],grads['b2'] = affine_backward(da2,cache['affine_2'])\n",
    "#             dz1 = relu_backward(da1, cache['relu_1'])\n",
    "#             dx, grads['W1'],grads['b1'] = affine_backward(dz1, cache['affine_1'])\n",
    "#             loss = loss + self.reg * (np.sum(self.params['W1']**2) + np.sum(self.params['W2']**2))\n",
    "#             grads['W1'] = grads['W1'] + 2 * self.reg * self.params['W1']\n",
    "#             grads['W2'] = grads['W2'] + 2 * self.reg * self.params['W2']\n",
    "#             return loss, grads\n",
    "#         else:\n",
    "#             z1, cache['affine_1'] = affine_forward(X,self.params['W1'],self.params['b1'])\n",
    "#             a1,cache['relu_1'] = relu_forward(z1)\n",
    "#             a2,cache['affine_2'] = affine_forward(a1,self.params['W2'], self.params['b2'])\n",
    "#             loss, da2 = softmax_loss(a2,y)\n",
    "#             loss = loss + self.reg * (np.sum( (self.params['W1'] * self.params['W1'])))\n",
    "#             loss = loss + self.reg * (np.sum( (self.params['W2'] * self.params['W2'])))\n",
    "#             return loss\n",
    "#     def predict(self, X):\n",
    "#         cache= {}\n",
    "#         z1, cache['affine_1'] = affine_forward(X,self.params['W1'],self.params['b1'])\n",
    "#         a1,cache['relu_1'] = relu_forward(z1)\n",
    "#         a2,cache['affine_2'] = affine_forward(a1,self.params['W2'], self.params['b2'])\n",
    "#         print(a2.shape)\n",
    "#         return np.argmax(a2,axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.69333351460128\n",
      "shape of W_1 =  (16, 10)\n",
      "(1279, 2)\n",
      "param =  W1 Value =  [[ 1.76614555e-02  3.97908839e-03  9.77762801e-03  2.23494041e-02\n",
      "   1.86233884e-02 -9.76268176e-03  9.45608424e-03 -1.48356355e-03\n",
      "  -1.02661852e-03  4.10583154e-03]\n",
      " [ 1.43170095e-03  1.45223071e-02  7.55116208e-03  1.21272497e-03\n",
      "   4.43847713e-03  3.33483476e-03  1.49253894e-02 -2.06787113e-03\n",
      "   3.12549594e-03 -8.52861167e-03]\n",
      " [-2.53923551e-02  6.49819119e-03  8.65822921e-03 -7.42745761e-03\n",
      "   2.26266432e-02 -1.45322166e-02  4.12396478e-04 -1.82335388e-03\n",
      "   1.53016469e-02  1.46780516e-02]\n",
      " [ 1.56245384e-03  3.77108798e-03 -8.86393221e-03 -1.97726166e-02\n",
      "  -3.47516903e-03  1.55845456e-03  1.22734653e-02  1.20037279e-02\n",
      "  -3.86425338e-03 -3.01534927e-03]\n",
      " [-1.04542352e-02 -1.41717893e-02 -1.70357577e-02  1.94660599e-02\n",
      "  -5.08638126e-03 -4.37314002e-03 -1.25041064e-02  7.75784510e-03\n",
      "  -1.61058383e-02 -2.12311949e-03]\n",
      " [-8.91838513e-03  3.85933027e-03 -5.11578841e-03 -1.17887589e-02\n",
      "  -2.83571947e-04  4.27148557e-03  6.59692316e-04  3.01870905e-03\n",
      "  -6.32858282e-03 -3.61890068e-03]\n",
      " [-6.71010592e-03 -3.58119290e-03 -8.14106736e-03 -1.72296562e-02\n",
      "   1.77768913e-03 -4.00676959e-03 -1.62570348e-02  4.60264870e-03\n",
      "  -9.05423284e-03  5.14626867e-04]\n",
      " [ 7.31727144e-03  1.28407708e-03  1.13488328e-02 -1.23376081e-02\n",
      "   4.01216328e-03 -6.83756256e-03 -8.69762342e-03 -5.77417559e-03\n",
      "  -3.10562477e-03  5.62271178e-04]\n",
      " [-1.15960721e-02  8.97970587e-03  4.62343692e-03 -1.53435324e-02\n",
      "   1.48420897e-02  1.89124008e-02  1.17451193e-02 -1.78469783e-03\n",
      "  -1.06823663e-02  1.05292206e-02]\n",
      " [-3.98005784e-03  1.21861419e-02  2.09045832e-03  9.73691182e-03\n",
      "   3.54248307e-03  7.04232237e-03  7.94381809e-05  1.78474938e-02\n",
      "   1.26922919e-03  4.01948512e-03]\n",
      " [ 1.88875732e-02 -1.34899427e-02 -1.26532838e-02  9.65226543e-03\n",
      "  -1.17479497e-02  1.93709124e-02 -4.19633239e-03 -7.39313866e-03\n",
      "   1.91970923e-02  1.47973549e-02]\n",
      " [ 1.86904889e-02  9.02975661e-03 -8.58933736e-03  1.90485421e-02\n",
      "  -2.68756380e-03  8.00033475e-03  9.43111237e-03 -1.52368281e-03\n",
      "   6.13186166e-03  9.21062210e-03]\n",
      " [ 3.80897723e-03 -1.09845890e-02  2.98211196e-03  1.32234251e-02\n",
      "  -6.94467739e-03 -1.50153279e-03 -4.36529472e-03  1.84789699e-02\n",
      "   6.71285122e-03  4.07346865e-03]\n",
      " [-7.64073937e-03  5.36898685e-03 -6.71971973e-03  3.07982057e-04\n",
      "  -6.35864039e-03  6.74197521e-03  5.73069694e-03 -2.05659321e-03\n",
      "   3.95475982e-03 -1.09017509e-02]\n",
      " [-1.48827765e-02  4.38671927e-03  1.65763570e-03  6.33686468e-03\n",
      "   2.37853624e-02  9.42663523e-03 -9.10683007e-03  1.11443539e-02\n",
      "  -1.31326408e-02 -4.60747096e-03]\n",
      " [-6.63664145e-04  1.70958871e-02 -7.43472545e-03 -8.25227203e-03\n",
      "  -9.85864448e-04 -6.62367021e-03  1.12383973e-02 -1.07730322e-02\n",
      "  -1.14504754e-02 -4.36764762e-03]]\n",
      "param =  b1 Value =  [ 1.08214170e-04 -2.66413569e-05 -4.20429143e-05 -3.32130155e-05\n",
      " -2.76073234e-05 -2.12021047e-05 -4.89670670e-05  3.69332694e-05\n",
      "  9.48251447e-06  1.49911355e-05]\n",
      "param =  W2 Value =  [[-0.00500593  0.0192923 ]\n",
      " [ 0.00947854  0.00087045]\n",
      " [-0.01221636  0.00841326]\n",
      " [-0.0100613  -0.01533767]\n",
      " [ 0.01182243  0.0031972 ]\n",
      " [ 0.00911193  0.00325914]\n",
      " [ 0.00851655 -0.00646262]\n",
      " [-0.01032434  0.00680491]\n",
      " [-0.00814043 -0.00675931]\n",
      " [-0.00469291  0.00032114]]\n",
      "param =  b2 Value =  [-0.0031693  0.0031693]\n"
     ]
    }
   ],
   "source": [
    "Model = TwoLayerNet(input_dim = x_train.shape[1], hidden_dim = 10, output_dim = 2)\n",
    "\n",
    "loss, grads = Model.loss(x_train, y_train)\n",
    "\n",
    "print(\"loss =\", loss)\n",
    "\n",
    "Model.params['W1'] = Model.params['W1'] - 0.1*grads['W1']\n",
    "Model.params['b1'] = Model.params['b1'] - 0.1*grads['b1']\n",
    "Model.params['W2'] = Model.params['W2'] - 0.1*grads['W2']\n",
    "Model.params['b2'] = Model.params['b2'] - 0.1*grads['b2']\n",
    "\n",
    "loss, grads = Model.loss(x_train, y_train)\n",
    "print(\"shape of W_1 = \",Model.params['W1'].shape)\n",
    "scores = Model.predict(x_train)\n",
    "\n",
    "\n",
    "\n",
    "for param,value in Model.params.items():\n",
    "    print(\"param = \", param , \"Value = \", Model.params[param])\n",
    "\n",
    "# print((Model.params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewSolver():\n",
    "    def __init__(self, model, X_train, y_train, lr = 0.05, batch_size = 20, num_epochs = 10, print_every = 1000):\n",
    "        self.lr = lr\n",
    "        self.data = {}\n",
    "        self.data['X_train'] = X_train\n",
    "        self.data['y_train'] = y_train\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.print_every = print_every\n",
    "        self.loss_history = np.array([])\n",
    "        self.grad_history = np.array([])\n",
    "        self.vel_history = np.array([])\n",
    "        self.loss_2_history = np.array([])\n",
    "        self.grad_2_history = np.array([])\n",
    "        pass\n",
    "    def train(self):\n",
    "        # mu = 0.95\n",
    "        # v_w = np.zeros(self.model.params['W'].shape)\n",
    "        # v_b = np.zeros_like(self.model.params['b']) \n",
    "        for i in range(self.num_epochs):\n",
    "            for j in range(self.data['X_train'].shape[0] // self.batch_size):\n",
    "                X_batch = self.data['X_train'][j * self.batch_size:(j + 1) * self.batch_size, :]\n",
    "                y_batch = self.data['y_train'][j * self.batch_size:(j + 1) * self.batch_size].reshape(-1,1)\n",
    "                loss, grads = self.model.loss(X_batch, y_batch)\n",
    "                # v_w = v_w*mu - self.lr * grads['W']\n",
    "                # v_b = v_b*mu - self.lr * grads['b']\n",
    "                # self.model.params['W'] += v_w\n",
    "                # self.model.params['b'] += v_b\n",
    "                self.model.params['W1'] += -1 * self.lr * grads['W1']\n",
    "                self.model.params['b1'] += -1 * self.lr * grads['b1']\n",
    "                self.model.params['W2'] += -1 * self.lr * grads['W2']\n",
    "                self.model.params['b2'] += -1 * self.lr * grads['b2']\n",
    "                if(j  == 0):\n",
    "                    print(\"Epoch = \", i, \"Batch = \", j, \"Loss = \", loss, \"Gradient_max = \", np.max(abs(grads['W1'])), \"learning rate ratio = \",np.max(self.lr*grads['W1']/self.model.params['W1']))\n",
    "                    self.loss_history = np.append(self.loss_history, loss)\n",
    "                    self.grad_history = np.append(self.grad_history, np.linalg.norm(grads['W1']))            \n",
    "                    # self.loss_history = np.append(self.loss_history, loss)\n",
    "                    # self.grad_history = np.append(self.grad_history, np.sum(grads['W1'] * grads['W1']))\n",
    "                    # self.vel_history = np.append(self.vel_history, np.sum(v_w * v_w))\n",
    "                if(j == 35):\n",
    "                    self.loss_2_history = np.append(self.loss_2_history, loss)\n",
    "                    self.grad_2_history = np.append(self.grad_2_history, np.linalg.norm(grads['W1']))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine till here !\n",
      "Epoch =  0 Batch =  0 Loss =  13.863044874778785\n",
      "Epoch =  1 Batch =  0 Loss =  13.86304467546339\n",
      "Epoch =  2 Batch =  0 Loss =  13.863044476029426\n",
      "Epoch =  3 Batch =  0 Loss =  13.863044277009175\n",
      "Epoch =  4 Batch =  0 Loss =  13.863044077917314\n",
      "Epoch =  5 Batch =  0 Loss =  13.863043878645879\n",
      "Epoch =  6 Batch =  0 Loss =  13.863043679304583\n",
      "Epoch =  7 Batch =  0 Loss =  13.863043479811905\n",
      "Epoch =  8 Batch =  0 Loss =  13.863043280231492\n",
      "Epoch =  9 Batch =  0 Loss =  13.863043080647316\n",
      "Epoch =  10 Batch =  0 Loss =  13.86304288093068\n",
      "Epoch =  11 Batch =  0 Loss =  13.863042681407443\n",
      "Epoch =  12 Batch =  0 Loss =  13.863042481955617\n",
      "Epoch =  13 Batch =  0 Loss =  13.863042282771982\n",
      "Epoch =  14 Batch =  0 Loss =  13.863042083043808\n",
      "Epoch =  15 Batch =  0 Loss =  13.863041882987298\n",
      "Epoch =  16 Batch =  0 Loss =  13.863041682785022\n",
      "Epoch =  17 Batch =  0 Loss =  13.863041482388553\n",
      "Epoch =  18 Batch =  0 Loss =  13.863041282058324\n",
      "Epoch =  19 Batch =  0 Loss =  13.863041082076617\n",
      "Epoch =  20 Batch =  0 Loss =  13.86304088195362\n",
      "Epoch =  21 Batch =  0 Loss =  13.863040682025893\n",
      "Epoch =  22 Batch =  0 Loss =  13.86304048201735\n",
      "Epoch =  23 Batch =  0 Loss =  13.86304028190733\n",
      "Epoch =  24 Batch =  0 Loss =  13.863040081638122\n",
      "Epoch =  25 Batch =  0 Loss =  13.86303988345205\n",
      "Epoch =  26 Batch =  0 Loss =  13.863039687369108\n",
      "Epoch =  27 Batch =  0 Loss =  13.863039491481818\n",
      "Epoch =  28 Batch =  0 Loss =  13.863039295696035\n",
      "Epoch =  29 Batch =  0 Loss =  13.863039099957216\n",
      "Epoch =  30 Batch =  0 Loss =  13.86303890451913\n",
      "Epoch =  31 Batch =  0 Loss =  13.86303870894354\n",
      "Epoch =  32 Batch =  0 Loss =  13.863038513233768\n",
      "Epoch =  33 Batch =  0 Loss =  13.863038317666\n",
      "Epoch =  34 Batch =  0 Loss =  13.863038122155109\n",
      "Epoch =  35 Batch =  0 Loss =  13.863037926559985\n",
      "Epoch =  36 Batch =  0 Loss =  13.863037731136199\n",
      "Epoch =  37 Batch =  0 Loss =  13.86303753561724\n",
      "Epoch =  38 Batch =  0 Loss =  13.863037339890376\n",
      "Epoch =  39 Batch =  0 Loss =  13.86303714394508\n",
      "Epoch =  40 Batch =  0 Loss =  13.863036947990162\n",
      "Epoch =  41 Batch =  0 Loss =  13.863036751982145\n",
      "Epoch =  42 Batch =  0 Loss =  13.863036556322909\n",
      "Epoch =  43 Batch =  0 Loss =  13.863036360774627\n",
      "Epoch =  44 Batch =  0 Loss =  13.863036165386575\n",
      "Epoch =  45 Batch =  0 Loss =  13.863035970026754\n",
      "Epoch =  46 Batch =  0 Loss =  13.863035774518142\n",
      "Epoch =  47 Batch =  0 Loss =  13.863035578612493\n",
      "Epoch =  48 Batch =  0 Loss =  13.863035382656197\n",
      "Epoch =  49 Batch =  0 Loss =  13.863035186913377\n",
      "Epoch =  50 Batch =  0 Loss =  13.8630349912346\n",
      "Epoch =  51 Batch =  0 Loss =  13.863034795453315\n",
      "Epoch =  52 Batch =  0 Loss =  13.863034599488133\n",
      "Epoch =  53 Batch =  0 Loss =  13.863034403526166\n",
      "Epoch =  54 Batch =  0 Loss =  13.863034208720551\n",
      "Epoch =  55 Batch =  0 Loss =  13.863034010200215\n",
      "Epoch =  56 Batch =  0 Loss =  13.863033811164252\n",
      "Epoch =  57 Batch =  0 Loss =  13.863033609862354\n",
      "Epoch =  58 Batch =  0 Loss =  13.863033406386005\n",
      "Epoch =  59 Batch =  0 Loss =  13.863033203060356\n",
      "Epoch =  60 Batch =  0 Loss =  13.863032999715585\n",
      "Epoch =  61 Batch =  0 Loss =  13.863032796293801\n",
      "Epoch =  62 Batch =  0 Loss =  13.863032593089363\n",
      "Epoch =  63 Batch =  0 Loss =  13.863032390104578\n",
      "Epoch =  64 Batch =  0 Loss =  13.863032187327743\n",
      "Epoch =  65 Batch =  0 Loss =  13.863031984406993\n",
      "Epoch =  66 Batch =  0 Loss =  13.863031781309799\n",
      "Epoch =  67 Batch =  0 Loss =  13.863031577896763\n",
      "Epoch =  68 Batch =  0 Loss =  13.86303137419232\n",
      "Epoch =  69 Batch =  0 Loss =  13.863031170192368\n",
      "Epoch =  70 Batch =  0 Loss =  13.863030965992948\n",
      "Epoch =  71 Batch =  0 Loss =  13.863030761760415\n",
      "Epoch =  72 Batch =  0 Loss =  13.863030557660451\n",
      "Epoch =  73 Batch =  0 Loss =  13.863030353329778\n",
      "Epoch =  74 Batch =  0 Loss =  13.863030149144636\n",
      "Epoch =  75 Batch =  0 Loss =  13.8630299451211\n",
      "Epoch =  76 Batch =  0 Loss =  13.863029740951514\n",
      "Epoch =  77 Batch =  0 Loss =  13.863029536870268\n",
      "Epoch =  78 Batch =  0 Loss =  13.863029333253621\n",
      "Epoch =  79 Batch =  0 Loss =  13.86302913009078\n",
      "Epoch =  80 Batch =  0 Loss =  13.863028926874524\n",
      "Epoch =  81 Batch =  0 Loss =  13.86302872410484\n",
      "Epoch =  82 Batch =  0 Loss =  13.863028521519597\n",
      "Epoch =  83 Batch =  0 Loss =  13.863028318544096\n",
      "Epoch =  84 Batch =  0 Loss =  13.863028116847266\n",
      "Epoch =  85 Batch =  0 Loss =  13.863027916808614\n",
      "Epoch =  86 Batch =  0 Loss =  13.863027716921465\n",
      "Epoch =  87 Batch =  0 Loss =  13.86302751681977\n",
      "Epoch =  88 Batch =  0 Loss =  13.863027316652762\n",
      "Epoch =  89 Batch =  0 Loss =  13.8630271165013\n",
      "Epoch =  90 Batch =  0 Loss =  13.863026916481795\n",
      "Epoch =  91 Batch =  0 Loss =  13.863026716841121\n",
      "Epoch =  92 Batch =  0 Loss =  13.863026517517742\n",
      "Epoch =  93 Batch =  0 Loss =  13.86302631788159\n",
      "Epoch =  94 Batch =  0 Loss =  13.863026118202399\n",
      "Epoch =  95 Batch =  0 Loss =  13.863025918369992\n",
      "Epoch =  96 Batch =  0 Loss =  13.863025718441218\n",
      "Epoch =  97 Batch =  0 Loss =  13.863025518395471\n",
      "Epoch =  98 Batch =  0 Loss =  13.863025315593532\n",
      "Epoch =  99 Batch =  0 Loss =  13.863025110478905\n",
      "Epoch =  100 Batch =  0 Loss =  13.86302492161498\n",
      "Epoch =  101 Batch =  0 Loss =  13.86302473735035\n",
      "Epoch =  102 Batch =  0 Loss =  13.86302455294634\n",
      "Epoch =  103 Batch =  0 Loss =  13.863024369244881\n",
      "Epoch =  104 Batch =  0 Loss =  13.863024185819286\n",
      "Epoch =  105 Batch =  0 Loss =  13.86302400215904\n",
      "Epoch =  106 Batch =  0 Loss =  13.863023818779945\n",
      "Epoch =  107 Batch =  0 Loss =  13.863023637467887\n",
      "Epoch =  108 Batch =  0 Loss =  13.863023456465262\n",
      "Epoch =  109 Batch =  0 Loss =  13.863023271151029\n",
      "Epoch =  110 Batch =  0 Loss =  13.863023085634326\n",
      "Epoch =  111 Batch =  0 Loss =  13.863022899684943\n",
      "Epoch =  112 Batch =  0 Loss =  13.863022711378203\n",
      "Epoch =  113 Batch =  0 Loss =  13.86302251567175\n",
      "Epoch =  114 Batch =  0 Loss =  13.863022319469952\n",
      "Epoch =  115 Batch =  0 Loss =  13.863022123361143\n",
      "Epoch =  116 Batch =  0 Loss =  13.863021928251674\n",
      "Epoch =  117 Batch =  0 Loss =  13.863021733280366\n",
      "Epoch =  118 Batch =  0 Loss =  13.863021538347864\n",
      "Epoch =  119 Batch =  0 Loss =  13.863021343746924\n",
      "Epoch =  120 Batch =  0 Loss =  13.863021148999591\n",
      "Epoch =  121 Batch =  0 Loss =  13.863020954144933\n",
      "Epoch =  122 Batch =  0 Loss =  13.863020759900138\n",
      "Epoch =  123 Batch =  0 Loss =  13.86302056559298\n",
      "Epoch =  124 Batch =  0 Loss =  13.863020371040113\n",
      "Epoch =  125 Batch =  0 Loss =  13.863020175961587\n",
      "Epoch =  126 Batch =  0 Loss =  13.863019981243822\n",
      "Epoch =  127 Batch =  0 Loss =  13.863019786260196\n",
      "Epoch =  128 Batch =  0 Loss =  13.863019590942582\n",
      "Epoch =  129 Batch =  0 Loss =  13.863019395727665\n",
      "Epoch =  130 Batch =  0 Loss =  13.863019200797723\n",
      "Epoch =  131 Batch =  0 Loss =  13.863019008784914\n",
      "Epoch =  132 Batch =  0 Loss =  13.863018831430857\n",
      "Epoch =  133 Batch =  0 Loss =  13.863018653918045\n",
      "Epoch =  134 Batch =  0 Loss =  13.863018476883186\n",
      "Epoch =  135 Batch =  0 Loss =  13.863018299615199\n",
      "Epoch =  136 Batch =  0 Loss =  13.863018122494113\n",
      "Epoch =  137 Batch =  0 Loss =  13.86301794602817\n",
      "Epoch =  138 Batch =  0 Loss =  13.86301776933369\n",
      "Epoch =  139 Batch =  0 Loss =  13.863017592237911\n",
      "Epoch =  140 Batch =  0 Loss =  13.863017415065512\n",
      "Epoch =  141 Batch =  0 Loss =  13.86301723770441\n",
      "Epoch =  142 Batch =  0 Loss =  13.863017060671627\n",
      "Epoch =  143 Batch =  0 Loss =  13.863016883474888\n",
      "Epoch =  144 Batch =  0 Loss =  13.863016706742346\n",
      "Epoch =  145 Batch =  0 Loss =  13.863016530828725\n",
      "Epoch =  146 Batch =  0 Loss =  13.863016355325161\n",
      "Epoch =  147 Batch =  0 Loss =  13.863016179581063\n",
      "Epoch =  148 Batch =  0 Loss =  13.863016004001555\n",
      "Epoch =  149 Batch =  0 Loss =  13.863015828033229\n",
      "Epoch =  150 Batch =  0 Loss =  13.863015651911235\n",
      "Epoch =  151 Batch =  0 Loss =  13.863015476029304\n",
      "Epoch =  152 Batch =  0 Loss =  13.863015300288325\n",
      "Epoch =  153 Batch =  0 Loss =  13.86301512494695\n",
      "Epoch =  154 Batch =  0 Loss =  13.863014949338288\n",
      "Epoch =  155 Batch =  0 Loss =  13.863014773484618\n",
      "Epoch =  156 Batch =  0 Loss =  13.86301459742815\n",
      "Epoch =  157 Batch =  0 Loss =  13.863014421550893\n",
      "Epoch =  158 Batch =  0 Loss =  13.863014245806788\n",
      "Epoch =  159 Batch =  0 Loss =  13.863014069847429\n",
      "Epoch =  160 Batch =  0 Loss =  13.863013893909281\n",
      "Epoch =  161 Batch =  0 Loss =  13.86301371818196\n",
      "Epoch =  162 Batch =  0 Loss =  13.863013542071279\n",
      "Epoch =  163 Batch =  0 Loss =  13.86301336562932\n",
      "Epoch =  164 Batch =  0 Loss =  13.863013188941776\n",
      "Epoch =  165 Batch =  0 Loss =  13.86301301213257\n",
      "Epoch =  166 Batch =  0 Loss =  13.863012835158356\n",
      "Epoch =  167 Batch =  0 Loss =  13.863012658277382\n",
      "Epoch =  168 Batch =  0 Loss =  13.863012484036453\n",
      "Epoch =  169 Batch =  0 Loss =  13.863012326707647\n",
      "Epoch =  170 Batch =  0 Loss =  13.863012169154628\n",
      "Epoch =  171 Batch =  0 Loss =  13.86301201135835\n",
      "Epoch =  172 Batch =  0 Loss =  13.86301185375457\n",
      "Epoch =  173 Batch =  0 Loss =  13.86301169567121\n",
      "Epoch =  174 Batch =  0 Loss =  13.86301153733861\n",
      "Epoch =  175 Batch =  0 Loss =  13.863011378416845\n",
      "Epoch =  176 Batch =  0 Loss =  13.863011219266655\n",
      "Epoch =  177 Batch =  0 Loss =  13.863011060612997\n",
      "Epoch =  178 Batch =  0 Loss =  13.863010901991197\n",
      "Epoch =  179 Batch =  0 Loss =  13.863010743586923\n",
      "Epoch =  180 Batch =  0 Loss =  13.863010584859445\n",
      "Epoch =  181 Batch =  0 Loss =  13.863010425969685\n",
      "Epoch =  182 Batch =  0 Loss =  13.863010266837039\n",
      "Epoch =  183 Batch =  0 Loss =  13.86301010809856\n",
      "Epoch =  184 Batch =  0 Loss =  13.863009949439771\n",
      "Epoch =  185 Batch =  0 Loss =  13.86300979060257\n",
      "Epoch =  186 Batch =  0 Loss =  13.863009631880548\n",
      "Epoch =  187 Batch =  0 Loss =  13.863009473750349\n",
      "Epoch =  188 Batch =  0 Loss =  13.863009317415106\n",
      "Epoch =  189 Batch =  0 Loss =  13.863009161599257\n",
      "Epoch =  190 Batch =  0 Loss =  13.863009005876727\n",
      "Epoch =  191 Batch =  0 Loss =  13.863008849981027\n",
      "Epoch =  192 Batch =  0 Loss =  13.86300869383704\n",
      "Epoch =  193 Batch =  0 Loss =  13.86300853753008\n",
      "Epoch =  194 Batch =  0 Loss =  13.863008381867964\n",
      "Epoch =  195 Batch =  0 Loss =  13.863008226056554\n",
      "Epoch =  196 Batch =  0 Loss =  13.863008070143254\n",
      "Epoch =  197 Batch =  0 Loss =  13.86300791372601\n",
      "Epoch =  198 Batch =  0 Loss =  13.863007756961872\n",
      "Epoch =  199 Batch =  0 Loss =  13.863007599857836\n",
      "Epoch =  200 Batch =  0 Loss =  13.863007442380566\n",
      "Epoch =  201 Batch =  0 Loss =  13.863007284712582\n",
      "Epoch =  202 Batch =  0 Loss =  13.863007127387407\n",
      "Epoch =  203 Batch =  0 Loss =  13.863006969881486\n",
      "Epoch =  204 Batch =  0 Loss =  13.863006812291284\n",
      "Epoch =  205 Batch =  0 Loss =  13.8630066543728\n",
      "Epoch =  206 Batch =  0 Loss =  13.863006496123049\n",
      "Epoch =  207 Batch =  0 Loss =  13.863006337695523\n",
      "Epoch =  208 Batch =  0 Loss =  13.863006179090046\n",
      "Epoch =  209 Batch =  0 Loss =  13.863006020323363\n",
      "Epoch =  210 Batch =  0 Loss =  13.863005861366732\n",
      "Epoch =  211 Batch =  0 Loss =  13.863005702219862\n",
      "Epoch =  212 Batch =  0 Loss =  13.863005543534154\n",
      "Epoch =  213 Batch =  0 Loss =  13.86300538465715\n",
      "Epoch =  214 Batch =  0 Loss =  13.863005225967662\n",
      "Epoch =  215 Batch =  0 Loss =  13.86300506685711\n",
      "Epoch =  216 Batch =  0 Loss =  13.86300490744872\n",
      "Epoch =  217 Batch =  0 Loss =  13.8630047481449\n",
      "Epoch =  218 Batch =  0 Loss =  13.863004588635134\n",
      "Epoch =  219 Batch =  0 Loss =  13.863004429575618\n",
      "Epoch =  220 Batch =  0 Loss =  13.863004270200339\n",
      "Epoch =  221 Batch =  0 Loss =  13.863004111039418\n",
      "Epoch =  222 Batch =  0 Loss =  13.863003951462314\n",
      "Epoch =  223 Batch =  0 Loss =  13.863003792021882\n",
      "Epoch =  224 Batch =  0 Loss =  13.863003632310134\n",
      "Epoch =  225 Batch =  0 Loss =  13.863003472275414\n",
      "Epoch =  226 Batch =  0 Loss =  13.86300331307261\n",
      "Epoch =  227 Batch =  0 Loss =  13.863003153646096\n",
      "Epoch =  228 Batch =  0 Loss =  13.863002993951051\n",
      "Epoch =  229 Batch =  0 Loss =  13.863002833841584\n",
      "Epoch =  230 Batch =  0 Loss =  13.863002673519382\n",
      "Epoch =  231 Batch =  0 Loss =  13.86300251284696\n",
      "Epoch =  232 Batch =  0 Loss =  13.863002352039887\n",
      "Epoch =  233 Batch =  0 Loss =  13.863002191241495\n",
      "Epoch =  234 Batch =  0 Loss =  13.863002030351492\n",
      "Epoch =  235 Batch =  0 Loss =  13.863001869162956\n",
      "Epoch =  236 Batch =  0 Loss =  13.863001708036613\n",
      "Epoch =  237 Batch =  0 Loss =  13.8630015467846\n",
      "Epoch =  238 Batch =  0 Loss =  13.863001385262955\n",
      "Epoch =  239 Batch =  0 Loss =  13.863001223515223\n",
      "Epoch =  240 Batch =  0 Loss =  13.86300106167951\n",
      "Epoch =  241 Batch =  0 Loss =  13.863000899651794\n",
      "Epoch =  242 Batch =  0 Loss =  13.863000737420412\n",
      "Epoch =  243 Batch =  0 Loss =  13.863000575061964\n",
      "Epoch =  244 Batch =  0 Loss =  13.863000412089328\n",
      "Epoch =  245 Batch =  0 Loss =  13.863000248913094\n",
      "Epoch =  246 Batch =  0 Loss =  13.863000085544677\n",
      "Epoch =  247 Batch =  0 Loss =  13.862999922072737\n",
      "Epoch =  248 Batch =  0 Loss =  13.86299975831801\n",
      "Epoch =  249 Batch =  0 Loss =  13.862999595156914\n",
      "Epoch =  250 Batch =  0 Loss =  13.8629994317898\n",
      "Epoch =  251 Batch =  0 Loss =  13.862999267972897\n",
      "Epoch =  252 Batch =  0 Loss =  13.862999104613815\n",
      "Epoch =  253 Batch =  0 Loss =  13.862998940937572\n",
      "Epoch =  254 Batch =  0 Loss =  13.862998777107324\n",
      "Epoch =  255 Batch =  0 Loss =  13.862998613370966\n",
      "Epoch =  256 Batch =  0 Loss =  13.862998449586723\n",
      "Epoch =  257 Batch =  0 Loss =  13.862998285640328\n",
      "Epoch =  258 Batch =  0 Loss =  13.862998122864722\n",
      "Epoch =  259 Batch =  0 Loss =  13.862997961125735\n",
      "Epoch =  260 Batch =  0 Loss =  13.86299779958287\n",
      "Epoch =  261 Batch =  0 Loss =  13.862997637661175\n",
      "Epoch =  262 Batch =  0 Loss =  13.862997476901263\n",
      "Epoch =  263 Batch =  0 Loss =  13.862997315884817\n",
      "Epoch =  264 Batch =  0 Loss =  13.862997155077801\n",
      "Epoch =  265 Batch =  0 Loss =  13.862996994390022\n",
      "Epoch =  266 Batch =  0 Loss =  13.862996833700175\n",
      "Epoch =  267 Batch =  0 Loss =  13.862996672505679\n",
      "Epoch =  268 Batch =  0 Loss =  13.862996511056158\n",
      "Epoch =  269 Batch =  0 Loss =  13.862996349325012\n",
      "Epoch =  270 Batch =  0 Loss =  13.862996187375497\n",
      "Epoch =  271 Batch =  0 Loss =  13.8629960251976\n",
      "Epoch =  272 Batch =  0 Loss =  13.862995862800686\n",
      "Epoch =  273 Batch =  0 Loss =  13.862995699778214\n",
      "Epoch =  274 Batch =  0 Loss =  13.862995537050864\n",
      "Epoch =  275 Batch =  0 Loss =  13.86299537386637\n",
      "Epoch =  276 Batch =  0 Loss =  13.86299521047177\n",
      "Epoch =  277 Batch =  0 Loss =  13.86299504677343\n",
      "Epoch =  278 Batch =  0 Loss =  13.862994883256322\n",
      "Epoch =  279 Batch =  0 Loss =  13.862994720215147\n",
      "Epoch =  280 Batch =  0 Loss =  13.862994556769532\n",
      "Epoch =  281 Batch =  0 Loss =  13.862994393455088\n",
      "Epoch =  282 Batch =  0 Loss =  13.862994229713372\n",
      "Epoch =  283 Batch =  0 Loss =  13.862994065601482\n",
      "Epoch =  284 Batch =  0 Loss =  13.862993901532375\n",
      "Epoch =  285 Batch =  0 Loss =  13.862993738009527\n",
      "Epoch =  286 Batch =  0 Loss =  13.862993574036565\n",
      "Epoch =  287 Batch =  0 Loss =  13.862993425279877\n",
      "Epoch =  288 Batch =  0 Loss =  13.862993283265288\n",
      "Epoch =  289 Batch =  0 Loss =  13.862993140946532\n",
      "Epoch =  290 Batch =  0 Loss =  13.86299299830556\n",
      "Epoch =  291 Batch =  0 Loss =  13.862992855444038\n",
      "Epoch =  292 Batch =  0 Loss =  13.862992712714812\n",
      "Epoch =  293 Batch =  0 Loss =  13.862992570109254\n",
      "Epoch =  294 Batch =  0 Loss =  13.862992427281783\n",
      "Epoch =  295 Batch =  0 Loss =  13.86299228447374\n",
      "Epoch =  296 Batch =  0 Loss =  13.862992141822165\n",
      "Epoch =  297 Batch =  0 Loss =  13.862991998852475\n",
      "Epoch =  298 Batch =  0 Loss =  13.86299185571723\n",
      "Epoch =  299 Batch =  0 Loss =  13.862991713849421\n",
      "Epoch =  300 Batch =  0 Loss =  13.862991572477712\n",
      "Epoch =  301 Batch =  0 Loss =  13.862991430694569\n",
      "Epoch =  302 Batch =  0 Loss =  13.862991288577609\n",
      "Epoch =  303 Batch =  0 Loss =  13.862991146176288\n",
      "Epoch =  304 Batch =  0 Loss =  13.862991003558427\n",
      "Epoch =  305 Batch =  0 Loss =  13.86299086064786\n",
      "Epoch =  306 Batch =  0 Loss =  13.86299071773186\n",
      "Epoch =  307 Batch =  0 Loss =  13.862990574618808\n",
      "Epoch =  308 Batch =  0 Loss =  13.862990431587486\n",
      "Epoch =  309 Batch =  0 Loss =  13.862990288609845\n",
      "Epoch =  310 Batch =  0 Loss =  13.862990145660364\n",
      "Epoch =  311 Batch =  0 Loss =  13.862990002357956\n",
      "Epoch =  312 Batch =  0 Loss =  13.862989858625763\n",
      "Epoch =  313 Batch =  0 Loss =  13.862989714676402\n",
      "Epoch =  314 Batch =  0 Loss =  13.862989570875541\n",
      "Epoch =  315 Batch =  0 Loss =  13.862989427166898\n",
      "Epoch =  316 Batch =  0 Loss =  13.862989283228552\n",
      "Epoch =  317 Batch =  0 Loss =  13.86298913917251\n",
      "Epoch =  318 Batch =  0 Loss =  13.862988994697824\n",
      "Epoch =  319 Batch =  0 Loss =  13.862988849918008\n",
      "Epoch =  320 Batch =  0 Loss =  13.862988705221447\n",
      "Epoch =  321 Batch =  0 Loss =  13.862988560221833\n",
      "Epoch =  322 Batch =  0 Loss =  13.862988414398629\n",
      "Epoch =  323 Batch =  0 Loss =  13.862988268202098\n",
      "Epoch =  324 Batch =  0 Loss =  13.862988121811414\n",
      "Epoch =  325 Batch =  0 Loss =  13.862987975133302\n",
      "Epoch =  326 Batch =  0 Loss =  13.862987828132326\n",
      "Epoch =  327 Batch =  0 Loss =  13.86298768097328\n",
      "Epoch =  328 Batch =  0 Loss =  13.862987533593438\n",
      "Epoch =  329 Batch =  0 Loss =  13.862987385992996\n",
      "Epoch =  330 Batch =  0 Loss =  13.862987238338505\n",
      "Epoch =  331 Batch =  0 Loss =  13.86298709119255\n",
      "Epoch =  332 Batch =  0 Loss =  13.862986944241266\n",
      "Epoch =  333 Batch =  0 Loss =  13.862986796946679\n",
      "Epoch =  334 Batch =  0 Loss =  13.862986649713811\n",
      "Epoch =  335 Batch =  0 Loss =  13.862986502918046\n",
      "Epoch =  336 Batch =  0 Loss =  13.862986356175004\n",
      "Epoch =  337 Batch =  0 Loss =  13.862986209027058\n",
      "Epoch =  338 Batch =  0 Loss =  13.862986062270393\n",
      "Epoch =  339 Batch =  0 Loss =  13.86298591523266\n",
      "Epoch =  340 Batch =  0 Loss =  13.862985767957726\n",
      "Epoch =  341 Batch =  0 Loss =  13.862985620792234\n",
      "Epoch =  342 Batch =  0 Loss =  13.862985473976476\n",
      "Epoch =  343 Batch =  0 Loss =  13.862985326829866\n",
      "Epoch =  344 Batch =  0 Loss =  13.86298517973849\n",
      "Epoch =  345 Batch =  0 Loss =  13.862985032998562\n",
      "Epoch =  346 Batch =  0 Loss =  13.86298488647417\n",
      "Epoch =  347 Batch =  0 Loss =  13.86298473972025\n",
      "Epoch =  348 Batch =  0 Loss =  13.862984592785251\n",
      "Epoch =  349 Batch =  0 Loss =  13.862984445925573\n",
      "Epoch =  350 Batch =  0 Loss =  13.8629842987851\n",
      "Epoch =  351 Batch =  0 Loss =  13.86298415131821\n",
      "Epoch =  352 Batch =  0 Loss =  13.862984003546055\n",
      "Epoch =  353 Batch =  0 Loss =  13.862983855869262\n",
      "Epoch =  354 Batch =  0 Loss =  13.862983707707427\n",
      "Epoch =  355 Batch =  0 Loss =  13.862983559341215\n",
      "Epoch =  356 Batch =  0 Loss =  13.862983410877204\n",
      "Epoch =  357 Batch =  0 Loss =  13.862983262236868\n",
      "Epoch =  358 Batch =  0 Loss =  13.862983113353101\n",
      "Epoch =  359 Batch =  0 Loss =  13.862982964610335\n",
      "Epoch =  360 Batch =  0 Loss =  13.86298281566596\n",
      "Epoch =  361 Batch =  0 Loss =  13.862982666828978\n",
      "Epoch =  362 Batch =  0 Loss =  13.86298251811801\n",
      "Epoch =  363 Batch =  0 Loss =  13.862982369026872\n",
      "Epoch =  364 Batch =  0 Loss =  13.862982220088448\n",
      "Epoch =  365 Batch =  0 Loss =  13.862982070830325\n",
      "Epoch =  366 Batch =  0 Loss =  13.862981921958806\n",
      "Epoch =  367 Batch =  0 Loss =  13.862981772840888\n",
      "Epoch =  368 Batch =  0 Loss =  13.862981623323263\n",
      "Epoch =  369 Batch =  0 Loss =  13.862981473530075\n",
      "Epoch =  370 Batch =  0 Loss =  13.862981323667793\n",
      "Epoch =  371 Batch =  0 Loss =  13.862981173571756\n",
      "Epoch =  372 Batch =  0 Loss =  13.862981022088416\n",
      "Epoch =  373 Batch =  0 Loss =  13.862980868491455\n",
      "Epoch =  374 Batch =  0 Loss =  13.862980714572021\n",
      "Epoch =  375 Batch =  0 Loss =  13.862980560664672\n",
      "Epoch =  376 Batch =  0 Loss =  13.862980406509383\n",
      "Epoch =  377 Batch =  0 Loss =  13.862980252105967\n",
      "Epoch =  378 Batch =  0 Loss =  13.86298009745424\n",
      "Epoch =  379 Batch =  0 Loss =  13.86297994255402\n",
      "Epoch =  380 Batch =  0 Loss =  13.862979787422322\n",
      "Epoch =  381 Batch =  0 Loss =  13.862979632066795\n",
      "Epoch =  382 Batch =  0 Loss =  13.862979476462279\n",
      "Epoch =  383 Batch =  0 Loss =  13.862979321037418\n",
      "Epoch =  384 Batch =  0 Loss =  13.862979165225187\n",
      "Epoch =  385 Batch =  0 Loss =  13.86297900960913\n",
      "Epoch =  386 Batch =  0 Loss =  13.862978853742877\n",
      "Epoch =  387 Batch =  0 Loss =  13.862978697554901\n",
      "Epoch =  388 Batch =  0 Loss =  13.862978541051326\n",
      "Epoch =  389 Batch =  0 Loss =  13.862978384717696\n",
      "Epoch =  390 Batch =  0 Loss =  13.862978227842126\n",
      "Epoch =  391 Batch =  0 Loss =  13.8629780710419\n",
      "Epoch =  392 Batch =  0 Loss =  13.862977913864366\n",
      "Epoch =  393 Batch =  0 Loss =  13.862977757213692\n",
      "Epoch =  394 Batch =  0 Loss =  13.862977614441169\n",
      "Epoch =  395 Batch =  0 Loss =  13.862977483819595\n",
      "Epoch =  396 Batch =  0 Loss =  13.862977353020609\n",
      "Epoch =  397 Batch =  0 Loss =  13.862977223041915\n",
      "Epoch =  398 Batch =  0 Loss =  13.862977092874358\n",
      "Epoch =  399 Batch =  0 Loss =  13.862976962738395\n",
      "Epoch =  400 Batch =  0 Loss =  13.862976833256809\n",
      "Epoch =  401 Batch =  0 Loss =  13.862976703534105\n",
      "Epoch =  402 Batch =  0 Loss =  13.862976573488112\n",
      "Epoch =  403 Batch =  0 Loss =  13.862976443200806\n",
      "Epoch =  404 Batch =  0 Loss =  13.862976312314025\n",
      "Epoch =  405 Batch =  0 Loss =  13.862976181442484\n",
      "Epoch =  406 Batch =  0 Loss =  13.86297605094345\n",
      "Epoch =  407 Batch =  0 Loss =  13.862975919861995\n",
      "Epoch =  408 Batch =  0 Loss =  13.862975788538376\n",
      "Epoch =  409 Batch =  0 Loss =  13.862975657002721\n",
      "Epoch =  410 Batch =  0 Loss =  13.862975525224554\n",
      "Epoch =  411 Batch =  0 Loss =  13.862975393203739\n",
      "Epoch =  412 Batch =  0 Loss =  13.862975261359534\n",
      "Epoch =  413 Batch =  0 Loss =  13.86297512975361\n",
      "Epoch =  414 Batch =  0 Loss =  13.862974997929816\n",
      "Epoch =  415 Batch =  0 Loss =  13.862974865862201\n",
      "Epoch =  416 Batch =  0 Loss =  13.862974730243181\n",
      "Epoch =  417 Batch =  0 Loss =  13.862974594231472\n",
      "Epoch =  418 Batch =  0 Loss =  13.862974458177774\n",
      "Epoch =  419 Batch =  0 Loss =  13.86297432188036\n",
      "Epoch =  420 Batch =  0 Loss =  13.86297418521099\n",
      "Epoch =  421 Batch =  0 Loss =  13.862974048559602\n",
      "Epoch =  422 Batch =  0 Loss =  13.862973911411316\n",
      "Epoch =  423 Batch =  0 Loss =  13.86297377487603\n",
      "Epoch =  424 Batch =  0 Loss =  13.862973638027364\n",
      "Epoch =  425 Batch =  0 Loss =  13.86297350093397\n",
      "Epoch =  426 Batch =  0 Loss =  13.86297336359572\n",
      "Epoch =  427 Batch =  0 Loss =  13.86297322601249\n",
      "Epoch =  428 Batch =  0 Loss =  13.862973088113643\n",
      "Epoch =  429 Batch =  0 Loss =  13.862972949969624\n",
      "Epoch =  430 Batch =  0 Loss =  13.862972811580306\n",
      "Epoch =  431 Batch =  0 Loss =  13.862972673324455\n",
      "Epoch =  432 Batch =  0 Loss =  13.8629725345452\n",
      "Epoch =  433 Batch =  0 Loss =  13.862972396345803\n",
      "Epoch =  434 Batch =  0 Loss =  13.862972258351888\n",
      "Epoch =  435 Batch =  0 Loss =  13.86297212003631\n",
      "Epoch =  436 Batch =  0 Loss =  13.862971981473812\n",
      "Epoch =  437 Batch =  0 Loss =  13.862971842726981\n",
      "Epoch =  438 Batch =  0 Loss =  13.862971703934974\n",
      "Epoch =  439 Batch =  0 Loss =  13.862971565350472\n",
      "Epoch =  440 Batch =  0 Loss =  13.86297142644902\n",
      "Epoch =  441 Batch =  0 Loss =  13.862971287069044\n",
      "Epoch =  442 Batch =  0 Loss =  13.862971147451647\n",
      "Epoch =  443 Batch =  0 Loss =  13.862971007586115\n",
      "Epoch =  444 Batch =  0 Loss =  13.862970867433724\n",
      "Epoch =  445 Batch =  0 Loss =  13.862970726600357\n",
      "Epoch =  446 Batch =  0 Loss =  13.86297058551852\n",
      "Epoch =  447 Batch =  0 Loss =  13.862970444153895\n",
      "Epoch =  448 Batch =  0 Loss =  13.862970302519441\n",
      "Epoch =  449 Batch =  0 Loss =  13.862970160636845\n",
      "Epoch =  450 Batch =  0 Loss =  13.862970018505091\n",
      "Epoch =  451 Batch =  0 Loss =  13.862969876499896\n",
      "Epoch =  452 Batch =  0 Loss =  13.862969734245135\n",
      "Epoch =  453 Batch =  0 Loss =  13.862969592338397\n",
      "Epoch =  454 Batch =  0 Loss =  13.86296945022663\n",
      "Epoch =  455 Batch =  0 Loss =  13.862969307894799\n",
      "Epoch =  456 Batch =  0 Loss =  13.862969165312464\n",
      "Epoch =  457 Batch =  0 Loss =  13.862969022374497\n",
      "Epoch =  458 Batch =  0 Loss =  13.862968879185804\n",
      "Epoch =  459 Batch =  0 Loss =  13.862968736154619\n",
      "Epoch =  460 Batch =  0 Loss =  13.862968592955848\n",
      "Epoch =  461 Batch =  0 Loss =  13.862968449771394\n",
      "Epoch =  462 Batch =  0 Loss =  13.862968306413821\n",
      "Epoch =  463 Batch =  0 Loss =  13.862968162839659\n",
      "Epoch =  464 Batch =  0 Loss =  13.862968019022743\n",
      "Epoch =  465 Batch =  0 Loss =  13.862967875818823\n",
      "Epoch =  466 Batch =  0 Loss =  13.862967732832754\n",
      "Epoch =  467 Batch =  0 Loss =  13.862967589467324\n",
      "Epoch =  468 Batch =  0 Loss =  13.862967445858082\n",
      "Epoch =  469 Batch =  0 Loss =  13.862967302396893\n",
      "Epoch =  470 Batch =  0 Loss =  13.862967158692873\n",
      "Epoch =  471 Batch =  0 Loss =  13.86296701554283\n",
      "Epoch =  472 Batch =  0 Loss =  13.862966872082316\n",
      "Epoch =  473 Batch =  0 Loss =  13.862966728367315\n",
      "Epoch =  474 Batch =  0 Loss =  13.862966584397734\n",
      "Epoch =  475 Batch =  0 Loss =  13.862966440598429\n",
      "Epoch =  476 Batch =  0 Loss =  13.862966296515403\n",
      "Epoch =  477 Batch =  0 Loss =  13.8629661526066\n",
      "Epoch =  478 Batch =  0 Loss =  13.862966008398251\n",
      "Epoch =  479 Batch =  0 Loss =  13.862965864784774\n",
      "Epoch =  480 Batch =  0 Loss =  13.862965720926422\n",
      "Epoch =  481 Batch =  0 Loss =  13.862965576811812\n",
      "Epoch =  482 Batch =  0 Loss =  13.862965432381776\n",
      "Epoch =  483 Batch =  0 Loss =  13.862965287704853\n",
      "Epoch =  484 Batch =  0 Loss =  13.862965143187926\n",
      "Epoch =  485 Batch =  0 Loss =  13.862964998410156\n",
      "Epoch =  486 Batch =  0 Loss =  13.862964853404632\n",
      "Epoch =  487 Batch =  0 Loss =  13.862964708082284\n",
      "Epoch =  488 Batch =  0 Loss =  13.86296456293874\n",
      "Epoch =  489 Batch =  0 Loss =  13.86296441753761\n",
      "Epoch =  490 Batch =  0 Loss =  13.862964271912714\n",
      "Epoch =  491 Batch =  0 Loss =  13.862964126029947\n",
      "Epoch =  492 Batch =  0 Loss =  13.862963980358714\n",
      "Epoch =  493 Batch =  0 Loss =  13.86296383446236\n",
      "Epoch =  494 Batch =  0 Loss =  13.862963688706506\n",
      "Epoch =  495 Batch =  0 Loss =  13.862963542691793\n",
      "Epoch =  496 Batch =  0 Loss =  13.862963396439905\n",
      "Epoch =  497 Batch =  0 Loss =  13.862963249949233\n",
      "Epoch =  498 Batch =  0 Loss =  13.8629631032618\n",
      "Epoch =  499 Batch =  0 Loss =  13.86296295623455\n",
      "Epoch =  500 Batch =  0 Loss =  13.862962809290389\n",
      "Epoch =  501 Batch =  0 Loss =  13.862962662534306\n",
      "Epoch =  502 Batch =  0 Loss =  13.862962515518216\n",
      "Epoch =  503 Batch =  0 Loss =  13.86296236828093\n",
      "Epoch =  504 Batch =  0 Loss =  13.862962220831605\n",
      "Epoch =  505 Batch =  0 Loss =  13.86296207302045\n",
      "Epoch =  506 Batch =  0 Loss =  13.862961924969573\n",
      "Epoch =  507 Batch =  0 Loss =  13.862961776657915\n",
      "Epoch =  508 Batch =  0 Loss =  13.862961628784674\n",
      "Epoch =  509 Batch =  0 Loss =  13.862961481475786\n",
      "Epoch =  510 Batch =  0 Loss =  13.862961334824211\n",
      "Epoch =  511 Batch =  0 Loss =  13.862961187920709\n",
      "Epoch =  512 Batch =  0 Loss =  13.86296104075467\n",
      "Epoch =  513 Batch =  0 Loss =  13.862960893326001\n",
      "Epoch =  514 Batch =  0 Loss =  13.862960745643292\n",
      "Epoch =  515 Batch =  0 Loss =  13.862960598165701\n",
      "Epoch =  516 Batch =  0 Loss =  13.86296045080977\n",
      "Epoch =  517 Batch =  0 Loss =  13.86296030319043\n",
      "Epoch =  518 Batch =  0 Loss =  13.862960155307604\n",
      "Epoch =  519 Batch =  0 Loss =  13.862960007091758\n",
      "Epoch =  520 Batch =  0 Loss =  13.862959858478465\n",
      "Epoch =  521 Batch =  0 Loss =  13.862959709601475\n",
      "Epoch =  522 Batch =  0 Loss =  13.862959561834794\n",
      "Epoch =  523 Batch =  0 Loss =  13.86295941382767\n",
      "Epoch =  524 Batch =  0 Loss =  13.86295926555582\n",
      "Epoch =  525 Batch =  0 Loss =  13.862959117443378\n",
      "Epoch =  526 Batch =  0 Loss =  13.862958969084172\n",
      "Epoch =  527 Batch =  0 Loss =  13.862958820403842\n",
      "Epoch =  528 Batch =  0 Loss =  13.862958671469093\n",
      "Epoch =  529 Batch =  0 Loss =  13.862958522718463\n",
      "Epoch =  530 Batch =  0 Loss =  13.862958374181973\n",
      "Epoch =  531 Batch =  0 Loss =  13.8629582253995\n",
      "Epoch =  532 Batch =  0 Loss =  13.862958076836575\n",
      "Epoch =  533 Batch =  0 Loss =  13.862957928007312\n",
      "Epoch =  534 Batch =  0 Loss =  13.862957778852037\n",
      "Epoch =  535 Batch =  0 Loss =  13.862957629765514\n",
      "Epoch =  536 Batch =  0 Loss =  13.862957480465617\n",
      "Epoch =  537 Batch =  0 Loss =  13.862957330898936\n",
      "Epoch =  538 Batch =  0 Loss =  13.862957181065411\n",
      "Epoch =  539 Batch =  0 Loss =  13.862957031422773\n",
      "Epoch =  540 Batch =  0 Loss =  13.862956881439606\n",
      "Epoch =  541 Batch =  0 Loss =  13.862956731189254\n",
      "Epoch =  542 Batch =  0 Loss =  13.862956580671657\n",
      "Epoch =  543 Batch =  0 Loss =  13.862956429816\n",
      "Epoch =  544 Batch =  0 Loss =  13.862956278693046\n",
      "Epoch =  545 Batch =  0 Loss =  13.862956127302727\n",
      "Epoch =  546 Batch =  0 Loss =  13.862955975644988\n",
      "Epoch =  547 Batch =  0 Loss =  13.862955823719767\n",
      "Epoch =  548 Batch =  0 Loss =  13.862955671527011\n",
      "Epoch =  549 Batch =  0 Loss =  13.862955519083794\n",
      "Epoch =  550 Batch =  0 Loss =  13.862955366407334\n",
      "Epoch =  551 Batch =  0 Loss =  13.862955213260053\n",
      "Epoch =  552 Batch =  0 Loss =  13.862955059855334\n",
      "Epoch =  553 Batch =  0 Loss =  13.86295490611083\n",
      "Epoch =  554 Batch =  0 Loss =  13.86295475213615\n",
      "Epoch =  555 Batch =  0 Loss =  13.86295459886189\n",
      "Epoch =  556 Batch =  0 Loss =  13.862954445319058\n",
      "Epoch =  557 Batch =  0 Loss =  13.862954291431295\n",
      "Epoch =  558 Batch =  0 Loss =  13.86295413730126\n",
      "Epoch =  559 Batch =  0 Loss =  13.862953982902479\n",
      "Epoch =  560 Batch =  0 Loss =  13.862953829573321\n",
      "Epoch =  561 Batch =  0 Loss =  13.862953675974728\n",
      "Epoch =  562 Batch =  0 Loss =  13.862953522036051\n",
      "Epoch =  563 Batch =  0 Loss =  13.86295336778008\n",
      "Epoch =  564 Batch =  0 Loss =  13.86295321384188\n",
      "Epoch =  565 Batch =  0 Loss =  13.862953059655768\n",
      "Epoch =  566 Batch =  0 Loss =  13.862952905217782\n",
      "Epoch =  567 Batch =  0 Loss =  13.862952750518238\n",
      "Epoch =  568 Batch =  0 Loss =  13.862952595548625\n",
      "Epoch =  569 Batch =  0 Loss =  13.86295244022057\n",
      "Epoch =  570 Batch =  0 Loss =  13.86295228464981\n",
      "Epoch =  571 Batch =  0 Loss =  13.862952128734241\n",
      "Epoch =  572 Batch =  0 Loss =  13.862951973102355\n",
      "Epoch =  573 Batch =  0 Loss =  13.862951817209986\n",
      "Epoch =  574 Batch =  0 Loss =  13.862951660991211\n",
      "Epoch =  575 Batch =  0 Loss =  13.862951504501615\n",
      "Epoch =  576 Batch =  0 Loss =  13.862951347741145\n",
      "Epoch =  577 Batch =  0 Loss =  13.862951190652913\n",
      "Epoch =  578 Batch =  0 Loss =  13.862951033191251\n",
      "Epoch =  579 Batch =  0 Loss =  13.862950875469306\n",
      "Epoch =  580 Batch =  0 Loss =  13.86295071748576\n",
      "Epoch =  581 Batch =  0 Loss =  13.862950559231175\n",
      "Epoch =  582 Batch =  0 Loss =  13.862950400637521\n",
      "Epoch =  583 Batch =  0 Loss =  13.862950241720322\n",
      "Epoch =  584 Batch =  0 Loss =  13.862950083033805\n",
      "Epoch =  585 Batch =  0 Loss =  13.862949924094552\n",
      "Epoch =  586 Batch =  0 Loss =  13.862949764828983\n",
      "Epoch =  587 Batch =  0 Loss =  13.86294960531138\n",
      "Epoch =  588 Batch =  0 Loss =  13.862949445955532\n",
      "Epoch =  589 Batch =  0 Loss =  13.862949286800992\n",
      "Epoch =  590 Batch =  0 Loss =  13.862949127374439\n",
      "Epoch =  591 Batch =  0 Loss =  13.862948967541547\n",
      "Epoch =  592 Batch =  0 Loss =  13.862948807946204\n",
      "Epoch =  593 Batch =  0 Loss =  13.86294864801908\n",
      "Epoch =  594 Batch =  0 Loss =  13.862948488233899\n",
      "Epoch =  595 Batch =  0 Loss =  13.862948328185178\n",
      "Epoch =  596 Batch =  0 Loss =  13.86294816779851\n",
      "Epoch =  597 Batch =  0 Loss =  13.862948007091774\n",
      "Epoch =  598 Batch =  0 Loss =  13.862947846487657\n",
      "Epoch =  599 Batch =  0 Loss =  13.862947685483032\n",
      "Epoch =  600 Batch =  0 Loss =  13.862947525199779\n",
      "Epoch =  601 Batch =  0 Loss =  13.862947364677638\n",
      "Epoch =  602 Batch =  0 Loss =  13.86294720437867\n",
      "Epoch =  603 Batch =  0 Loss =  13.862947043805974\n",
      "Epoch =  604 Batch =  0 Loss =  13.862946882976221\n",
      "Epoch =  605 Batch =  0 Loss =  13.862946722360528\n",
      "Epoch =  606 Batch =  0 Loss =  13.862946562917726\n",
      "Epoch =  607 Batch =  0 Loss =  13.862946403079166\n",
      "Epoch =  608 Batch =  0 Loss =  13.862946242904359\n",
      "Epoch =  609 Batch =  0 Loss =  13.862946082454785\n",
      "Epoch =  610 Batch =  0 Loss =  13.8629459217495\n",
      "Epoch =  611 Batch =  0 Loss =  13.862945761208673\n",
      "Epoch =  612 Batch =  0 Loss =  13.86294560082192\n",
      "Epoch =  613 Batch =  0 Loss =  13.862945440159981\n",
      "Epoch =  614 Batch =  0 Loss =  13.862945279582842\n",
      "Epoch =  615 Batch =  0 Loss =  13.862945118613153\n",
      "Epoch =  616 Batch =  0 Loss =  13.862944957378176\n",
      "Epoch =  617 Batch =  0 Loss =  13.86294479573081\n",
      "Epoch =  618 Batch =  0 Loss =  13.862944633808173\n",
      "Epoch =  619 Batch =  0 Loss =  13.862944471610255\n",
      "Epoch =  620 Batch =  0 Loss =  13.862944309153132\n",
      "Epoch =  621 Batch =  0 Loss =  13.862944146420643\n",
      "Epoch =  622 Batch =  0 Loss =  13.86294398341278\n",
      "Epoch =  623 Batch =  0 Loss =  13.862943830254123\n",
      "Epoch =  624 Batch =  0 Loss =  13.862943698544834\n",
      "Epoch =  625 Batch =  0 Loss =  13.862943566563317\n",
      "Epoch =  626 Batch =  0 Loss =  13.862943434329686\n",
      "Epoch =  627 Batch =  0 Loss =  13.862943301843982\n",
      "Epoch =  628 Batch =  0 Loss =  13.862943169037436\n",
      "Epoch =  629 Batch =  0 Loss =  13.86294303594078\n",
      "Epoch =  630 Batch =  0 Loss =  13.862942902592204\n",
      "Epoch =  631 Batch =  0 Loss =  13.862942768991742\n",
      "Epoch =  632 Batch =  0 Loss =  13.862942635148876\n",
      "Epoch =  633 Batch =  0 Loss =  13.862942500982161\n",
      "Epoch =  634 Batch =  0 Loss =  13.862942366563724\n",
      "Epoch =  635 Batch =  0 Loss =  13.862942231893603\n",
      "Epoch =  636 Batch =  0 Loss =  13.862942098016756\n",
      "Epoch =  637 Batch =  0 Loss =  13.862941963744499\n",
      "Epoch =  638 Batch =  0 Loss =  13.862941828940826\n",
      "Epoch =  639 Batch =  0 Loss =  13.862941694317643\n",
      "Epoch =  640 Batch =  0 Loss =  13.862941559925758\n",
      "Epoch =  641 Batch =  0 Loss =  13.862941425168035\n",
      "Epoch =  642 Batch =  0 Loss =  13.862941290158428\n",
      "Epoch =  643 Batch =  0 Loss =  13.862941154896985\n",
      "Epoch =  644 Batch =  0 Loss =  13.862941019383753\n",
      "Epoch =  645 Batch =  0 Loss =  13.862940884055151\n",
      "Epoch =  646 Batch =  0 Loss =  13.86294074851265\n",
      "Epoch =  647 Batch =  0 Loss =  13.862940613670052\n",
      "Epoch =  648 Batch =  0 Loss =  13.862940479083376\n",
      "Epoch =  649 Batch =  0 Loss =  13.862940344244414\n",
      "Epoch =  650 Batch =  0 Loss =  13.862940209644057\n",
      "Epoch =  651 Batch =  0 Loss =  13.86294007479137\n",
      "Epoch =  652 Batch =  0 Loss =  13.862939940165333\n",
      "Epoch =  653 Batch =  0 Loss =  13.862939805542263\n",
      "Epoch =  654 Batch =  0 Loss =  13.862939671597493\n",
      "Epoch =  655 Batch =  0 Loss =  13.862939537398287\n",
      "Epoch =  656 Batch =  0 Loss =  13.862939402953124\n",
      "Epoch =  657 Batch =  0 Loss =  13.86293926810736\n",
      "Epoch =  658 Batch =  0 Loss =  13.862939133007442\n",
      "Epoch =  659 Batch =  0 Loss =  13.86293899761062\n",
      "Epoch =  660 Batch =  0 Loss =  13.86293886189221\n",
      "Epoch =  661 Batch =  0 Loss =  13.862938725919905\n",
      "Epoch =  662 Batch =  0 Loss =  13.862938589693778\n",
      "Epoch =  663 Batch =  0 Loss =  13.862938453223064\n",
      "Epoch =  664 Batch =  0 Loss =  13.862938316284326\n",
      "Epoch =  665 Batch =  0 Loss =  13.862938179638979\n",
      "Epoch =  666 Batch =  0 Loss =  13.862938042749171\n",
      "Epoch =  667 Batch =  0 Loss =  13.862937905605794\n",
      "Epoch =  668 Batch =  0 Loss =  13.862937768229388\n",
      "Epoch =  669 Batch =  0 Loss =  13.862937630618003\n",
      "Epoch =  670 Batch =  0 Loss =  13.862937492766216\n",
      "Epoch =  671 Batch =  0 Loss =  13.862937354660978\n",
      "Epoch =  672 Batch =  0 Loss =  13.862937214673478\n",
      "Epoch =  673 Batch =  0 Loss =  13.862937073739856\n",
      "Epoch =  674 Batch =  0 Loss =  13.86293693255338\n",
      "Epoch =  675 Batch =  0 Loss =  13.862936791738639\n",
      "Epoch =  676 Batch =  0 Loss =  13.862936651291562\n",
      "Epoch =  677 Batch =  0 Loss =  13.862936511755256\n",
      "Epoch =  678 Batch =  0 Loss =  13.862936371965404\n",
      "Epoch =  679 Batch =  0 Loss =  13.862936231922092\n",
      "Epoch =  680 Batch =  0 Loss =  13.862936091647022\n",
      "Epoch =  681 Batch =  0 Loss =  13.862935951162553\n",
      "Epoch =  682 Batch =  0 Loss =  13.862935810376738\n",
      "Epoch =  683 Batch =  0 Loss =  13.862935669213138\n",
      "Epoch =  684 Batch =  0 Loss =  13.862935527796324\n",
      "Epoch =  685 Batch =  0 Loss =  13.862935386126383\n",
      "Epoch =  686 Batch =  0 Loss =  13.862935244203392\n",
      "Epoch =  687 Batch =  0 Loss =  13.862935102027429\n",
      "Epoch =  688 Batch =  0 Loss =  13.862934959598583\n",
      "Epoch =  689 Batch =  0 Loss =  13.862934816948552\n",
      "Epoch =  690 Batch =  0 Loss =  13.862934674057696\n",
      "Epoch =  691 Batch =  0 Loss =  13.862934531429685\n",
      "Epoch =  692 Batch =  0 Loss =  13.862934388475889\n",
      "Epoch =  693 Batch =  0 Loss =  13.862934243965867\n",
      "Epoch =  694 Batch =  0 Loss =  13.862934098067036\n",
      "Epoch =  695 Batch =  0 Loss =  13.862933951871197\n",
      "Epoch =  696 Batch =  0 Loss =  13.862933805397818\n",
      "Epoch =  697 Batch =  0 Loss =  13.862933658626256\n",
      "Epoch =  698 Batch =  0 Loss =  13.862933511602815\n",
      "Epoch =  699 Batch =  0 Loss =  13.862933364327585\n",
      "Epoch =  700 Batch =  0 Loss =  13.862933216871008\n",
      "Epoch =  701 Batch =  0 Loss =  13.862933069170836\n",
      "Epoch =  702 Batch =  0 Loss =  13.862932921853371\n",
      "Epoch =  703 Batch =  0 Loss =  13.862932774574153\n",
      "Epoch =  704 Batch =  0 Loss =  13.86293262704302\n",
      "Epoch =  705 Batch =  0 Loss =  13.862932479270503\n",
      "Epoch =  706 Batch =  0 Loss =  13.86293233119585\n",
      "Epoch =  707 Batch =  0 Loss =  13.862932182869557\n",
      "Epoch =  708 Batch =  0 Loss =  13.862932034309395\n",
      "Epoch =  709 Batch =  0 Loss =  13.862931885508498\n",
      "Epoch =  710 Batch =  0 Loss =  13.862931737197963\n",
      "Epoch =  711 Batch =  0 Loss =  13.8629315887616\n",
      "Epoch =  712 Batch =  0 Loss =  13.862931440070867\n",
      "Epoch =  713 Batch =  0 Loss =  13.862931291735604\n",
      "Epoch =  714 Batch =  0 Loss =  13.862931143163397\n",
      "Epoch =  715 Batch =  0 Loss =  13.862930994347368\n",
      "Epoch =  716 Batch =  0 Loss =  13.862930845277091\n",
      "Epoch =  717 Batch =  0 Loss =  13.86293069596262\n",
      "Epoch =  718 Batch =  0 Loss =  13.862930546404657\n",
      "Epoch =  719 Batch =  0 Loss =  13.862930396592686\n",
      "Epoch =  720 Batch =  0 Loss =  13.862930246548993\n",
      "Epoch =  721 Batch =  0 Loss =  13.862930096251425\n",
      "Epoch =  722 Batch =  0 Loss =  13.862929945646549\n",
      "Epoch =  723 Batch =  0 Loss =  13.862929794788045\n",
      "Epoch =  724 Batch =  0 Loss =  13.862929643611267\n",
      "Epoch =  725 Batch =  0 Loss =  13.862929492189593\n",
      "Epoch =  726 Batch =  0 Loss =  13.862929340514643\n",
      "Epoch =  727 Batch =  0 Loss =  13.862929188586522\n",
      "Epoch =  728 Batch =  0 Loss =  13.862929036405335\n",
      "Epoch =  729 Batch =  0 Loss =  13.862928883852708\n",
      "Epoch =  730 Batch =  0 Loss =  13.86292873098796\n",
      "Epoch =  731 Batch =  0 Loss =  13.862928577870608\n",
      "Epoch =  732 Batch =  0 Loss =  13.862928424441456\n",
      "Epoch =  733 Batch =  0 Loss =  13.862928270759957\n",
      "Epoch =  734 Batch =  0 Loss =  13.862928116842765\n",
      "Epoch =  735 Batch =  0 Loss =  13.862927962673384\n",
      "Epoch =  736 Batch =  0 Loss =  13.862927808251916\n",
      "Epoch =  737 Batch =  0 Loss =  13.862927653597486\n",
      "Epoch =  738 Batch =  0 Loss =  13.862927498691127\n",
      "Epoch =  739 Batch =  0 Loss =  13.862927343549039\n",
      "Epoch =  740 Batch =  0 Loss =  13.862927188155174\n",
      "Epoch =  741 Batch =  0 Loss =  13.862927032509637\n",
      "Epoch =  742 Batch =  0 Loss =  13.862926876612537\n",
      "Epoch =  743 Batch =  0 Loss =  13.862926720409055\n",
      "Epoch =  744 Batch =  0 Loss =  13.862926563954277\n",
      "Epoch =  745 Batch =  0 Loss =  13.862926407248299\n",
      "Epoch =  746 Batch =  0 Loss =  13.862926250232803\n",
      "Epoch =  747 Batch =  0 Loss =  13.862926092983953\n",
      "Epoch =  748 Batch =  0 Loss =  13.862925936001757\n",
      "Epoch =  749 Batch =  0 Loss =  13.86292577872355\n",
      "Epoch =  750 Batch =  0 Loss =  13.862925621212842\n",
      "Epoch =  751 Batch =  0 Loss =  13.862925463459788\n",
      "Epoch =  752 Batch =  0 Loss =  13.862925305464955\n",
      "Epoch =  753 Batch =  0 Loss =  13.86292514817218\n",
      "Epoch =  754 Batch =  0 Loss =  13.862924990576014\n",
      "Epoch =  755 Batch =  0 Loss =  13.862924832729341\n",
      "Epoch =  756 Batch =  0 Loss =  13.862924674640286\n",
      "Epoch =  757 Batch =  0 Loss =  13.862924516300907\n",
      "Epoch =  758 Batch =  0 Loss =  13.862924357727529\n",
      "Epoch =  759 Batch =  0 Loss =  13.862924198904004\n",
      "Epoch =  760 Batch =  0 Loss =  13.862924039830466\n",
      "Epoch =  761 Batch =  0 Loss =  13.862923880522214\n",
      "Epoch =  762 Batch =  0 Loss =  13.862923721537761\n",
      "Epoch =  763 Batch =  0 Loss =  13.862923562303417\n",
      "Epoch =  764 Batch =  0 Loss =  13.862923402819302\n",
      "Epoch =  765 Batch =  0 Loss =  13.862923243085545\n",
      "Epoch =  766 Batch =  0 Loss =  13.862923083045398\n",
      "Epoch =  767 Batch =  0 Loss =  13.862922923346668\n",
      "Epoch =  768 Batch =  0 Loss =  13.862922763349747\n",
      "Epoch =  769 Batch =  0 Loss =  13.862922603113146\n",
      "Epoch =  770 Batch =  0 Loss =  13.862922442627385\n",
      "Epoch =  771 Batch =  0 Loss =  13.862922281892596\n",
      "Epoch =  772 Batch =  0 Loss =  13.862922120875421\n",
      "Epoch =  773 Batch =  0 Loss =  13.86292195966535\n",
      "Epoch =  774 Batch =  0 Loss =  13.862921798511156\n",
      "Epoch =  775 Batch =  0 Loss =  13.862921637108279\n",
      "Epoch =  776 Batch =  0 Loss =  13.862921475463942\n",
      "Epoch =  777 Batch =  0 Loss =  13.862921313586739\n",
      "Epoch =  778 Batch =  0 Loss =  13.86292115216153\n",
      "Epoch =  779 Batch =  0 Loss =  13.862920990503735\n",
      "Epoch =  780 Batch =  0 Loss =  13.86292082859754\n",
      "Epoch =  781 Batch =  0 Loss =  13.862920666465762\n",
      "Epoch =  782 Batch =  0 Loss =  13.862920504032708\n",
      "Epoch =  783 Batch =  0 Loss =  13.862920341351687\n",
      "Epoch =  784 Batch =  0 Loss =  13.862920178603835\n",
      "Epoch =  785 Batch =  0 Loss =  13.8629200156996\n",
      "Epoch =  786 Batch =  0 Loss =  13.862919852547693\n",
      "Epoch =  787 Batch =  0 Loss =  13.862919689148251\n",
      "Epoch =  788 Batch =  0 Loss =  13.862919525501406\n",
      "Epoch =  789 Batch =  0 Loss =  13.86291936160731\n",
      "Epoch =  790 Batch =  0 Loss =  13.862919197466091\n",
      "Epoch =  791 Batch =  0 Loss =  13.862919033026834\n",
      "Epoch =  792 Batch =  0 Loss =  13.862918868353317\n",
      "Epoch =  793 Batch =  0 Loss =  13.86291870343313\n",
      "Epoch =  794 Batch =  0 Loss =  13.86291853829147\n",
      "Epoch =  795 Batch =  0 Loss =  13.862918372911453\n",
      "Epoch =  796 Batch =  0 Loss =  13.862918207238376\n",
      "Epoch =  797 Batch =  0 Loss =  13.862918041795517\n",
      "Epoch =  798 Batch =  0 Loss =  13.862917876065929\n",
      "Epoch =  799 Batch =  0 Loss =  13.86291771000398\n",
      "Epoch =  800 Batch =  0 Loss =  13.862917543644688\n",
      "Epoch =  801 Batch =  0 Loss =  13.86291737705782\n",
      "Epoch =  802 Batch =  0 Loss =  13.862917210176928\n",
      "Epoch =  803 Batch =  0 Loss =  13.862917043050706\n",
      "Epoch =  804 Batch =  0 Loss =  13.862916876344006\n",
      "Epoch =  805 Batch =  0 Loss =  13.862916709341327\n",
      "Epoch =  806 Batch =  0 Loss =  13.862916542093622\n",
      "Epoch =  807 Batch =  0 Loss =  13.862916375249434\n",
      "Epoch =  808 Batch =  0 Loss =  13.862916208105847\n",
      "Epoch =  809 Batch =  0 Loss =  13.862916040717405\n",
      "Epoch =  810 Batch =  0 Loss =  13.862915873025555\n",
      "Epoch =  811 Batch =  0 Loss =  13.86291570509624\n",
      "Epoch =  812 Batch =  0 Loss =  13.862915536860152\n",
      "Epoch =  813 Batch =  0 Loss =  13.86291536832854\n",
      "Epoch =  814 Batch =  0 Loss =  13.862915199552951\n",
      "Epoch =  815 Batch =  0 Loss =  13.862915030541243\n",
      "Epoch =  816 Batch =  0 Loss =  13.862914861285873\n",
      "Epoch =  817 Batch =  0 Loss =  13.862914691731026\n",
      "Epoch =  818 Batch =  0 Loss =  13.862914521932874\n",
      "Epoch =  819 Batch =  0 Loss =  13.862914351891579\n",
      "Epoch =  820 Batch =  0 Loss =  13.86291418162699\n",
      "Epoch =  821 Batch =  0 Loss =  13.862914011046518\n",
      "Epoch =  822 Batch =  0 Loss =  13.862913840165987\n",
      "Epoch =  823 Batch =  0 Loss =  13.862913669042957\n",
      "Epoch =  824 Batch =  0 Loss =  13.862913497677585\n",
      "Epoch =  825 Batch =  0 Loss =  13.862913326762467\n",
      "Epoch =  826 Batch =  0 Loss =  13.862913155605154\n",
      "Epoch =  827 Batch =  0 Loss =  13.86291298422461\n",
      "Epoch =  828 Batch =  0 Loss =  13.862912812614052\n",
      "Epoch =  829 Batch =  0 Loss =  13.862912640706028\n",
      "Epoch =  830 Batch =  0 Loss =  13.862912468556349\n",
      "Epoch =  831 Batch =  0 Loss =  13.862912296165192\n",
      "Epoch =  832 Batch =  0 Loss =  13.862912123532723\n",
      "Epoch =  833 Batch =  0 Loss =  13.862911950659123\n",
      "Epoch =  834 Batch =  0 Loss =  13.862911777544554\n",
      "Epoch =  835 Batch =  0 Loss =  13.862911604189195\n",
      "Epoch =  836 Batch =  0 Loss =  13.862911430593218\n",
      "Epoch =  837 Batch =  0 Loss =  13.862911256704969\n",
      "Epoch =  838 Batch =  0 Loss =  13.862911082576492\n",
      "Epoch =  839 Batch =  0 Loss =  13.862910908151726\n",
      "Epoch =  840 Batch =  0 Loss =  13.862910733434688\n",
      "Epoch =  841 Batch =  0 Loss =  13.862910558478028\n",
      "Epoch =  842 Batch =  0 Loss =  13.862910383281921\n",
      "Epoch =  843 Batch =  0 Loss =  13.86291020786228\n",
      "Epoch =  844 Batch =  0 Loss =  13.862910030817773\n",
      "Epoch =  845 Batch =  0 Loss =  13.862909852979975\n",
      "Epoch =  846 Batch =  0 Loss =  13.862909674903698\n",
      "Epoch =  847 Batch =  0 Loss =  13.862909496538553\n",
      "Epoch =  848 Batch =  0 Loss =  13.862909317942933\n",
      "Epoch =  849 Batch =  0 Loss =  13.862909139776361\n",
      "Epoch =  850 Batch =  0 Loss =  13.862908961371899\n",
      "Epoch =  851 Batch =  0 Loss =  13.862908782736744\n",
      "Epoch =  852 Batch =  0 Loss =  13.862908603731249\n",
      "Epoch =  853 Batch =  0 Loss =  13.862908424449415\n",
      "Epoch =  854 Batch =  0 Loss =  13.862908244937257\n",
      "Epoch =  855 Batch =  0 Loss =  13.862908065188215\n",
      "Epoch =  856 Batch =  0 Loss =  13.862907885158041\n",
      "Epoch =  857 Batch =  0 Loss =  13.862907704844991\n",
      "Epoch =  858 Batch =  0 Loss =  13.8629075243026\n",
      "Epoch =  859 Batch =  0 Loss =  13.862907343524135\n",
      "Epoch =  860 Batch =  0 Loss =  13.862907162509796\n",
      "Epoch =  861 Batch =  0 Loss =  13.862906981269111\n",
      "Epoch =  862 Batch =  0 Loss =  13.862906800037372\n",
      "Epoch =  863 Batch =  0 Loss =  13.862906618648294\n",
      "Epoch =  864 Batch =  0 Loss =  13.862906437116536\n",
      "Epoch =  865 Batch =  0 Loss =  13.862906255606248\n",
      "Epoch =  866 Batch =  0 Loss =  13.862906073877575\n",
      "Epoch =  867 Batch =  0 Loss =  13.862905891875767\n",
      "Epoch =  868 Batch =  0 Loss =  13.862905709639264\n",
      "Epoch =  869 Batch =  0 Loss =  13.862905527193382\n",
      "Epoch =  870 Batch =  0 Loss =  13.862905344473571\n",
      "Epoch =  871 Batch =  0 Loss =  13.862905161524951\n",
      "Epoch =  872 Batch =  0 Loss =  13.86290497834664\n",
      "Epoch =  873 Batch =  0 Loss =  13.862904794934531\n",
      "Epoch =  874 Batch =  0 Loss =  13.862904611293343\n",
      "Epoch =  875 Batch =  0 Loss =  13.862904427418712\n",
      "Epoch =  876 Batch =  0 Loss =  13.862904243310846\n",
      "Epoch =  877 Batch =  0 Loss =  13.862904058969939\n",
      "Epoch =  878 Batch =  0 Loss =  13.862903874396192\n",
      "Epoch =  879 Batch =  0 Loss =  13.862903689596127\n",
      "Epoch =  880 Batch =  0 Loss =  13.86290350456362\n",
      "Epoch =  881 Batch =  0 Loss =  13.862903319298876\n",
      "Epoch =  882 Batch =  0 Loss =  13.862903134335486\n",
      "Epoch =  883 Batch =  0 Loss =  13.862902949063002\n",
      "Epoch =  884 Batch =  0 Loss =  13.862902763558834\n",
      "Epoch =  885 Batch =  0 Loss =  13.862902577823196\n",
      "Epoch =  886 Batch =  0 Loss =  13.862902391856295\n",
      "Epoch =  887 Batch =  0 Loss =  13.862902205615235\n",
      "Epoch =  888 Batch =  0 Loss =  13.862902019057676\n",
      "Epoch =  889 Batch =  0 Loss =  13.862901832269596\n",
      "Epoch =  890 Batch =  0 Loss =  13.862901645207117\n",
      "Epoch =  891 Batch =  0 Loss =  13.86290145791931\n",
      "Epoch =  892 Batch =  0 Loss =  13.862901270401615\n",
      "Epoch =  893 Batch =  0 Loss =  13.862901082654226\n",
      "Epoch =  894 Batch =  0 Loss =  13.862900894682754\n",
      "Epoch =  895 Batch =  0 Loss =  13.862900706482025\n",
      "Epoch =  896 Batch =  0 Loss =  13.862900518052252\n",
      "Epoch =  897 Batch =  0 Loss =  13.86290032939365\n",
      "Epoch =  898 Batch =  0 Loss =  13.862900140506445\n",
      "Epoch =  899 Batch =  0 Loss =  13.86289995139085\n",
      "Epoch =  900 Batch =  0 Loss =  13.862899762047084\n",
      "Epoch =  901 Batch =  0 Loss =  13.86289957241265\n",
      "Epoch =  902 Batch =  0 Loss =  13.862899382568246\n",
      "Epoch =  903 Batch =  0 Loss =  13.86289919249633\n",
      "Epoch =  904 Batch =  0 Loss =  13.862899002197132\n",
      "Epoch =  905 Batch =  0 Loss =  13.862898811670865\n",
      "Epoch =  906 Batch =  0 Loss =  13.862898620917766\n",
      "Epoch =  907 Batch =  0 Loss =  13.86289842989106\n",
      "Epoch =  908 Batch =  0 Loss =  13.862898238638\n",
      "Epoch =  909 Batch =  0 Loss =  13.86289804715882\n",
      "Epoch =  910 Batch =  0 Loss =  13.862897855453742\n",
      "Epoch =  911 Batch =  0 Loss =  13.862897663522995\n",
      "Epoch =  912 Batch =  0 Loss =  13.862897471366814\n",
      "Epoch =  913 Batch =  0 Loss =  13.862897278985418\n",
      "Epoch =  914 Batch =  0 Loss =  13.862897086379045\n",
      "Epoch =  915 Batch =  0 Loss =  13.862896893547923\n",
      "Epoch =  916 Batch =  0 Loss =  13.86289670049229\n",
      "Epoch =  917 Batch =  0 Loss =  13.862896507114883\n",
      "Epoch =  918 Batch =  0 Loss =  13.862896313519729\n",
      "Epoch =  919 Batch =  0 Loss =  13.86289611966053\n",
      "Epoch =  920 Batch =  0 Loss =  13.86289592568518\n",
      "Epoch =  921 Batch =  0 Loss =  13.862895731730745\n",
      "Epoch =  922 Batch =  0 Loss =  13.862895537553213\n",
      "Epoch =  923 Batch =  0 Loss =  13.862895343152815\n",
      "Epoch =  924 Batch =  0 Loss =  13.862895148529796\n",
      "Epoch =  925 Batch =  0 Loss =  13.862894954347968\n",
      "Epoch =  926 Batch =  0 Loss =  13.862894759943893\n",
      "Epoch =  927 Batch =  0 Loss =  13.86289456527936\n",
      "Epoch =  928 Batch =  0 Loss =  13.862894370397813\n",
      "Epoch =  929 Batch =  0 Loss =  13.862894175294768\n",
      "Epoch =  930 Batch =  0 Loss =  13.862893979880374\n",
      "Epoch =  931 Batch =  0 Loss =  13.862893784192394\n",
      "Epoch =  932 Batch =  0 Loss =  13.86289358828377\n",
      "Epoch =  933 Batch =  0 Loss =  13.862893392164434\n",
      "Epoch =  934 Batch =  0 Loss =  13.862893195824913\n",
      "Epoch =  935 Batch =  0 Loss =  13.862892999265457\n",
      "Epoch =  936 Batch =  0 Loss =  13.862892802486314\n",
      "Epoch =  937 Batch =  0 Loss =  13.862892605487742\n",
      "Epoch =  938 Batch =  0 Loss =  13.862892408229975\n",
      "Epoch =  939 Batch =  0 Loss =  13.862892210753305\n",
      "Epoch =  940 Batch =  0 Loss =  13.862892013084046\n",
      "Epoch =  941 Batch =  0 Loss =  13.86289181519632\n",
      "Epoch =  942 Batch =  0 Loss =  13.862891617090387\n",
      "Epoch =  943 Batch =  0 Loss =  13.862891418766502\n",
      "Epoch =  944 Batch =  0 Loss =  13.862891220224927\n",
      "Epoch =  945 Batch =  0 Loss =  13.862891021465925\n",
      "Epoch =  946 Batch =  0 Loss =  13.862890822489751\n",
      "Epoch =  947 Batch =  0 Loss =  13.862890623307008\n",
      "Epoch =  948 Batch =  0 Loss =  13.862890423912356\n",
      "Epoch =  949 Batch =  0 Loss =  13.862890224301266\n",
      "Epoch =  950 Batch =  0 Loss =  13.862890024474007\n",
      "Epoch =  951 Batch =  0 Loss =  13.862889824430843\n",
      "Epoch =  952 Batch =  0 Loss =  13.862889624122703\n",
      "Epoch =  953 Batch =  0 Loss =  13.862889424334394\n",
      "Epoch =  954 Batch =  0 Loss =  13.862889224330859\n",
      "Epoch =  955 Batch =  0 Loss =  13.862889024062564\n",
      "Epoch =  956 Batch =  0 Loss =  13.86288882359115\n",
      "Epoch =  957 Batch =  0 Loss =  13.862888622905317\n",
      "Epoch =  958 Batch =  0 Loss =  13.862888422005334\n",
      "Epoch =  959 Batch =  0 Loss =  13.862888220891481\n",
      "Epoch =  960 Batch =  0 Loss =  13.862888019564025\n",
      "Epoch =  961 Batch =  0 Loss =  13.862887818023262\n",
      "Epoch =  962 Batch =  0 Loss =  13.86288761626945\n",
      "Epoch =  963 Batch =  0 Loss =  13.862887414302891\n",
      "Epoch =  964 Batch =  0 Loss =  13.86288721212385\n",
      "Epoch =  965 Batch =  0 Loss =  13.862887009676657\n",
      "Epoch =  966 Batch =  0 Loss =  13.862886807017594\n",
      "Epoch =  967 Batch =  0 Loss =  13.86288660415588\n",
      "Epoch =  968 Batch =  0 Loss =  13.86288640108285\n",
      "Epoch =  969 Batch =  0 Loss =  13.862886197798789\n",
      "Epoch =  970 Batch =  0 Loss =  13.862885994247268\n",
      "Epoch =  971 Batch =  0 Loss =  13.862885790435207\n",
      "Epoch =  972 Batch =  0 Loss =  13.86288558641306\n",
      "Epoch =  973 Batch =  0 Loss =  13.86288538212869\n",
      "Epoch =  974 Batch =  0 Loss =  13.862885177594073\n",
      "Epoch =  975 Batch =  0 Loss =  13.862884972850315\n",
      "Epoch =  976 Batch =  0 Loss =  13.862884767897702\n",
      "Epoch =  977 Batch =  0 Loss =  13.862884562736532\n",
      "Epoch =  978 Batch =  0 Loss =  13.862884357373403\n",
      "Epoch =  979 Batch =  0 Loss =  13.862884151802294\n",
      "Epoch =  980 Batch =  0 Loss =  13.86288394597632\n",
      "Epoch =  981 Batch =  0 Loss =  13.862883739942985\n",
      "Epoch =  982 Batch =  0 Loss =  13.862883533648182\n",
      "Epoch =  983 Batch =  0 Loss =  13.862883327146633\n",
      "Epoch =  984 Batch =  0 Loss =  13.862883120438626\n",
      "Epoch =  985 Batch =  0 Loss =  13.86288291352447\n",
      "Epoch =  986 Batch =  0 Loss =  13.862882706404465\n",
      "Epoch =  987 Batch =  0 Loss =  13.862882499037568\n",
      "Epoch =  988 Batch =  0 Loss =  13.862882291465462\n",
      "Epoch =  989 Batch =  0 Loss =  13.862882083688458\n",
      "Epoch =  990 Batch =  0 Loss =  13.862881875706853\n",
      "Epoch =  991 Batch =  0 Loss =  13.862881668341819\n",
      "Epoch =  992 Batch =  0 Loss =  13.862881460729623\n",
      "Epoch =  993 Batch =  0 Loss =  13.862881252919736\n",
      "Epoch =  994 Batch =  0 Loss =  13.862881044859899\n",
      "Epoch =  995 Batch =  0 Loss =  13.86288083654372\n",
      "Epoch =  996 Batch =  0 Loss =  13.862880628031066\n",
      "Epoch =  997 Batch =  0 Loss =  13.862880419334457\n",
      "Epoch =  998 Batch =  0 Loss =  13.86288021043561\n",
      "Epoch =  999 Batch =  0 Loss =  13.862880001334844\n"
     ]
    }
   ],
   "source": [
    "NN = TwoLayerNet(input_dim = x_train.shape[1],hidden_dim= 10 , output_dim=2)\n",
    "sgd_solver = GeneralSolver(NN, x_train, y_train, lr = 1.9e-5, batch_size = 20, num_epochs=1000, print_every=1000)\n",
    "sgd_solver.train('sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(0,sgd_solver.num_epochs)\n",
    "test_epochs  = range(0,sgd_solver.loss_history.shape[0])\n",
    "plt.plot(test_epochs, sgd_solver.loss_history, label = 'Loss Batch 0 ', color = 'red')\n",
    "plt.plot(test_epochs, sgd_solver.loss_final_history, label = 'Loss Batch 35', color = 'blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0 Batch =  0 Loss =  13.866404713377248 Gradient_max =  0.012282357653694285 learning rate ratio =  0.0008070375991460248\n",
      "Epoch =  1 Batch =  0 Loss =  13.86640237859875 Gradient_max =  0.012297596794739574 learning rate ratio =  0.0008224088133958646\n",
      "Epoch =  2 Batch =  0 Loss =  13.866400044901193 Gradient_max =  0.012312854937309773 learning rate ratio =  0.0008383780116628888\n",
      "Epoch =  3 Batch =  0 Loss =  13.866397712290292 Gradient_max =  0.012328132105021964 learning rate ratio =  0.000854980778712902\n",
      "Epoch =  4 Batch =  0 Loss =  13.866395380766264 Gradient_max =  0.012343428321517792 learning rate ratio =  0.0008722555798834764\n",
      "Epoch =  5 Batch =  0 Loss =  13.86639305032152 Gradient_max =  0.012358743610460984 learning rate ratio =  0.000890244058581184\n",
      "Epoch =  6 Batch =  0 Loss =  13.86639072092224 Gradient_max =  0.012374077995511231 learning rate ratio =  0.0009088539934397749\n",
      "Epoch =  7 Batch =  0 Loss =  13.866388392899264 Gradient_max =  0.012389431500747021 learning rate ratio =  0.0009282598952149746\n",
      "Epoch =  8 Batch =  0 Loss =  13.86638606640877 Gradient_max =  0.012404804150087599 learning rate ratio =  0.0009485139435142642\n",
      "Epoch =  9 Batch =  0 Loss =  13.866383741244213 Gradient_max =  0.012420195967027186 learning rate ratio =  0.0009692654900802639\n",
      "Epoch =  10 Batch =  0 Loss =  13.866381417192338 Gradient_max =  0.012435606975237198 learning rate ratio =  0.0009909467866841106\n",
      "Epoch =  11 Batch =  0 Loss =  13.866379094213988 Gradient_max =  0.012451037198534846 learning rate ratio =  0.001013621746596646\n",
      "Epoch =  12 Batch =  0 Loss =  13.866376772308636 Gradient_max =  0.012466486660797813 learning rate ratio =  0.0010373602785388766\n",
      "Epoch =  13 Batch =  0 Loss =  13.86637445147574 Gradient_max =  0.012481955385933564 learning rate ratio =  0.0010622390065773651\n",
      "Epoch =  14 Batch =  0 Loss =  13.866372131714785 Gradient_max =  0.01249744339787934 learning rate ratio =  0.001088342096299348\n",
      "Epoch =  15 Batch =  0 Loss =  13.86636981302524 Gradient_max =  0.012512950720602265 learning rate ratio =  0.0011157622060358622\n",
      "Epoch =  16 Batch =  0 Loss =  13.866367495406582 Gradient_max =  0.012528477378099334 learning rate ratio =  0.0011446015857858468\n",
      "Epoch =  17 Batch =  0 Loss =  13.866365178858283 Gradient_max =  0.012544023394397471 learning rate ratio =  0.001174973351308637\n",
      "Epoch =  18 Batch =  0 Loss =  13.866362863379825 Gradient_max =  0.01255958879355354 learning rate ratio =  0.0012070029668493723\n",
      "Epoch =  19 Batch =  0 Loss =  13.866360548976981 Gradient_max =  0.012575173599660408 learning rate ratio =  0.0012408299774718167\n",
      "Epoch =  20 Batch =  0 Loss =  13.86635823564293 Gradient_max =  0.012590777836828998 learning rate ratio =  0.0012766100414444265\n",
      "Epoch =  21 Batch =  0 Loss =  13.866355923377151 Gradient_max =  0.012606401529206288 learning rate ratio =  0.0013145173250994591\n",
      "Epoch =  22 Batch =  0 Loss =  13.866353612135468 Gradient_max =  0.01262204470091912 learning rate ratio =  0.001354583816575984\n",
      "Epoch =  23 Batch =  0 Loss =  13.866351301981325 Gradient_max =  0.012637707376248971 learning rate ratio =  0.0013971723154884831\n",
      "Epoch =  24 Batch =  0 Loss =  13.86634899285623 Gradient_max =  0.012653389579365884 learning rate ratio =  0.0014420714393334226\n",
      "Epoch =  25 Batch =  0 Loss =  13.86634668479728 Gradient_max =  0.012669091334580797 learning rate ratio =  0.0014899552030982282\n",
      "Epoch =  26 Batch =  0 Loss =  13.866344377706113 Gradient_max =  0.012684812666078361 learning rate ratio =  0.0015408246073904254\n",
      "Epoch =  27 Batch =  0 Loss =  13.866342071875714 Gradient_max =  0.012700553598503327 learning rate ratio =  0.0015952938239329187\n",
      "Epoch =  28 Batch =  0 Loss =  13.866339767247062 Gradient_max =  0.012716314156154134 learning rate ratio =  0.0016537589761336574\n",
      "Epoch =  29 Batch =  0 Loss =  13.866337463682477 Gradient_max =  0.012732094363276419 learning rate ratio =  0.001716676519659183\n",
      "Epoch =  30 Batch =  0 Loss =  13.866335161181452 Gradient_max =  0.012747894244289478 learning rate ratio =  0.0017845751832184175\n",
      "Epoch =  31 Batch =  0 Loss =  13.866332859743492 Gradient_max =  0.012763713823643087 learning rate ratio =  0.0018580708624494572\n",
      "Epoch =  32 Batch =  0 Loss =  13.866330559368096 Gradient_max =  0.012779553125817479 learning rate ratio =  0.0019378853551714214\n",
      "Epoch =  33 Batch =  0 Loss =  13.866328260054766 Gradient_max =  0.01279541217532342 learning rate ratio =  0.0020248701459315054\n",
      "Epoch =  34 Batch =  0 Loss =  13.86632596180301 Gradient_max =  0.012811290996702234 learning rate ratio =  0.002120036902142238\n",
      "Epoch =  35 Batch =  0 Loss =  13.866323664612322 Gradient_max =  0.012827189614525844 learning rate ratio =  0.002224597000397838\n",
      "Epoch =  36 Batch =  0 Loss =  13.86632136843407 Gradient_max =  0.012843108053341058 learning rate ratio =  0.002338940192154679\n",
      "Epoch =  37 Batch =  0 Loss =  13.866319073321245 Gradient_max =  0.012859046337841903 learning rate ratio =  0.0024656827685837807\n",
      "Epoch =  38 Batch =  0 Loss =  13.866316779222393 Gradient_max =  0.01287500449263451 learning rate ratio =  0.002606533254490626\n",
      "Epoch =  39 Batch =  0 Loss =  13.866314486182713 Gradient_max =  0.012890982542466433 learning rate ratio =  0.002764460773166432\n",
      "Epoch =  40 Batch =  0 Loss =  13.866312194207007 Gradient_max =  0.012906980512068116 learning rate ratio =  0.0029427714069014494\n",
      "Epoch =  41 Batch =  0 Loss =  13.86630990328949 Gradient_max =  0.012922998426190917 learning rate ratio =  0.0031456835114540005\n",
      "Epoch =  42 Batch =  0 Loss =  13.866307613429687 Gradient_max =  0.012939036309622017 learning rate ratio =  0.0033786656948906098\n",
      "Epoch =  43 Batch =  0 Loss =  13.866305324627113 Gradient_max =  0.012955094187179494 learning rate ratio =  0.0036489370780229857\n",
      "Epoch =  44 Batch =  0 Loss =  13.86630303688129 Gradient_max =  0.012971172083712398 learning rate ratio =  0.003966228650779945\n",
      "Epoch =  45 Batch =  0 Loss =  13.866300750191742 Gradient_max =  0.012987270024100743 learning rate ratio =  0.0043439798545716465\n",
      "Epoch =  46 Batch =  0 Loss =  13.86629846455799 Gradient_max =  0.013003388033255537 learning rate ratio =  0.004801291218085459\n",
      "Epoch =  47 Batch =  0 Loss =  13.86629617997956 Gradient_max =  0.013019526136118871 learning rate ratio =  0.005366255970845468\n",
      "Epoch =  48 Batch =  0 Loss =  13.866293896455975 Gradient_max =  0.013035684357663956 learning rate ratio =  0.006081958364785388\n",
      "Epoch =  49 Batch =  0 Loss =  13.866291613996184 Gradient_max =  0.013051862722906232 learning rate ratio =  0.007018012305277937\n",
      "Epoch =  50 Batch =  0 Loss =  13.866289332597713 Gradient_max =  0.013068061256878824 learning rate ratio =  0.008294696708233382\n",
      "Epoch =  51 Batch =  0 Loss =  13.8662870522526 Gradient_max =  0.013084279984639572 learning rate ratio =  0.010139299861291154\n",
      "Epoch =  52 Batch =  0 Loss =  13.866284772916531 Gradient_max =  0.013100518931234699 learning rate ratio =  0.01303007583050624\n",
      "Epoch =  53 Batch =  0 Loss =  13.86628249464726 Gradient_max =  0.013116778121862174 learning rate ratio =  0.018227011773455426\n",
      "Epoch =  54 Batch =  0 Loss =  13.866280217429907 Gradient_max =  0.01313305758166556 learning rate ratio =  0.030320989088303822\n",
      "Epoch =  55 Batch =  0 Loss =  13.866277941264011 Gradient_max =  0.013149357335836801 learning rate ratio =  0.09012252379021073\n",
      "Epoch =  56 Batch =  0 Loss =  13.866275666159648 Gradient_max =  0.013165677409609064 learning rate ratio =  0.001273054726615864\n",
      "Epoch =  57 Batch =  0 Loss =  13.866273392105807 Gradient_max =  0.01318201782822731 learning rate ratio =  0.0013907187683649255\n",
      "Epoch =  58 Batch =  0 Loss =  13.866271119111586 Gradient_max =  0.013198378616986637 learning rate ratio =  0.001532168333278107\n",
      "Epoch =  59 Batch =  0 Loss =  13.866268847166964 Gradient_max =  0.013214759801196026 learning rate ratio =  0.0017054264480517918\n",
      "Epoch =  60 Batch =  0 Loss =  13.866266576228812 Gradient_max =  0.013231161406155373 learning rate ratio =  0.0019225815338902849\n",
      "Epoch =  61 Batch =  0 Loss =  13.866264306347619 Gradient_max =  0.013247583457304946 learning rate ratio =  0.0022027359208588917\n",
      "Epoch =  62 Batch =  0 Loss =  13.86626203745975 Gradient_max =  0.013264025979984015 learning rate ratio =  0.0025779559184653934\n",
      "Epoch =  63 Batch =  0 Loss =  13.866259770517994 Gradient_max =  0.013280489000633805 learning rate ratio =  0.0031065166627653107\n",
      "Epoch =  64 Batch =  0 Loss =  13.86625750462345 Gradient_max =  0.013296972543796796 learning rate ratio =  0.0039065551985091145\n",
      "Epoch =  65 Batch =  0 Loss =  13.866255239775677 Gradient_max =  0.013313476634980765 learning rate ratio =  0.005259495681796971\n",
      "Epoch =  66 Batch =  0 Loss =  13.866252975979624 Gradient_max =  0.013330001299730342 learning rate ratio =  0.008041120138855358\n",
      "Epoch =  67 Batch =  0 Loss =  13.866250713229448 Gradient_max =  0.01334654656361191 learning rate ratio =  0.01704582511898202\n",
      "Epoch =  68 Batch =  0 Loss =  13.866248451524736 Gradient_max =  0.013363112452228775 learning rate ratio =  0.0008227294088081385\n",
      "Epoch =  69 Batch =  0 Loss =  13.866246190865043 Gradient_max =  0.013379698991216164 learning rate ratio =  0.0008659001809634348\n",
      "Epoch =  70 Batch =  0 Loss =  13.866243931206574 Gradient_max =  0.013396306206190948 learning rate ratio =  0.0009137908501909616\n",
      "Epoch =  71 Batch =  0 Loss =  13.866241672600019 Gradient_max =  0.01341293412291178 learning rate ratio =  0.000967220176420247\n",
      "Epoch =  72 Batch =  0 Loss =  13.866239415037189 Gradient_max =  0.013429582767100707 learning rate ratio =  0.0010272079153838176\n",
      "Epoch =  73 Batch =  0 Loss =  13.866237158517658 Gradient_max =  0.013446252164521033 learning rate ratio =  0.0010950405289968021\n",
      "Epoch =  74 Batch =  0 Loss =  13.866234903041006 Gradient_max =  0.013462942340968183 learning rate ratio =  0.0011723644779553597\n",
      "Epoch =  75 Batch =  0 Loss =  13.866232648548687 Gradient_max =  0.01347965332220252 learning rate ratio =  0.0012613216199886972\n",
      "Epoch =  76 Batch =  0 Loss =  13.866230395042402 Gradient_max =  0.013496385134086292 learning rate ratio =  0.0013647506879243833\n",
      "Epoch =  77 Batch =  0 Loss =  13.86622814257782 Gradient_max =  0.013513137802576325 learning rate ratio =  0.0014864958034121795\n",
      "Epoch =  78 Batch =  0 Loss =  13.866225891154528 Gradient_max =  0.013529911353596902 learning rate ratio =  0.0016318948108941488\n",
      "Epoch =  79 Batch =  0 Loss =  13.86622364077587 Gradient_max =  0.01354670581310792 learning rate ratio =  0.0018085828668096442\n",
      "Epoch =  80 Batch =  0 Loss =  13.866221391441497 Gradient_max =  0.01356352120709847 learning rate ratio =  0.0020278773519995163\n",
      "Epoch =  81 Batch =  0 Loss =  13.866219143147156 Gradient_max =  0.013580357561586482 learning rate ratio =  0.0023073019937424203\n",
      "Epoch =  82 Batch =  0 Loss =  13.866216895897006 Gradient_max =  0.013597214902629964 learning rate ratio =  0.0026755171785415124\n",
      "Epoch =  83 Batch =  0 Loss =  13.866214649686068 Gradient_max =  0.013614093256310966 learning rate ratio =  0.0031828380033581457\n",
      "Epoch =  84 Batch =  0 Loss =  13.866212404467399 Gradient_max =  0.013630992648694327 learning rate ratio =  0.003926448946162688\n",
      "Epoch =  85 Batch =  0 Loss =  13.866210160238253 Gradient_max =  0.013647913105928798 learning rate ratio =  0.005121545926716023\n",
      "Epoch =  86 Batch =  0 Loss =  13.866207917047177 Gradient_max =  0.01366485465425496 learning rate ratio =  0.007358546109270758\n",
      "Epoch =  87 Batch =  0 Loss =  13.866205674893783 Gradient_max =  0.01368181731988941 learning rate ratio =  0.013052860086932891\n",
      "Epoch =  88 Batch =  0 Loss =  13.866203433785973 Gradient_max =  0.01369880112909126 learning rate ratio =  0.05747484110559382\n",
      "Epoch =  89 Batch =  0 Loss =  13.866201193731204 Gradient_max =  0.013715806108150328 learning rate ratio =  6.61635600515449e-05\n",
      "Epoch =  90 Batch =  0 Loss =  13.866198954615895 Gradient_max =  0.013732832283251364 learning rate ratio =  6.657677193150783e-05\n",
      "Epoch =  91 Batch =  0 Loss =  13.866196716545044 Gradient_max =  0.013749879680864172 learning rate ratio =  6.699467640734039e-05\n",
      "Epoch =  92 Batch =  0 Loss =  13.866194479509847 Gradient_max =  0.013766948327359186 learning rate ratio =  6.741735352665925e-05\n",
      "Epoch =  93 Batch =  0 Loss =  13.86619224350991 Gradient_max =  0.013784038249149685 learning rate ratio =  6.784488516814498e-05\n",
      "Epoch =  94 Batch =  0 Loss =  13.866190008498497 Gradient_max =  0.013801149472628161 learning rate ratio =  6.827735509397637e-05\n",
      "Epoch =  95 Batch =  0 Loss =  13.866187774521613 Gradient_max =  0.01381828202432756 learning rate ratio =  6.871484900483862e-05\n",
      "Epoch =  96 Batch =  0 Loss =  13.866185540011063 Gradient_max =  0.013835435929455723 learning rate ratio =  6.915745458915845e-05\n",
      "Epoch =  97 Batch =  0 Loss =  13.866183306460838 Gradient_max =  0.01385261121578739 learning rate ratio =  6.960526160115233e-05\n",
      "Epoch =  98 Batch =  0 Loss =  13.866181073944315 Gradient_max =  0.013869807909976 learning rate ratio =  7.005836190240355e-05\n",
      "Epoch =  99 Batch =  0 Loss =  13.866178842461125 Gradient_max =  0.013887026038633012 learning rate ratio =  7.051684953010616e-05\n",
      "Epoch =  100 Batch =  0 Loss =  13.866176612015552 Gradient_max =  0.013904265628407313 learning rate ratio =  7.098082076218642e-05\n",
      "Epoch =  101 Batch =  0 Loss =  13.866174382568346 Gradient_max =  0.013921526705930777 learning rate ratio =  7.145037418425399e-05\n",
      "Epoch =  102 Batch =  0 Loss =  13.866172154153366 Gradient_max =  0.013938809297956207 learning rate ratio =  7.192561075962469e-05\n",
      "Epoch =  103 Batch =  0 Loss =  13.866169926770247 Gradient_max =  0.013956113431227992 learning rate ratio =  7.240663390100433e-05\n",
      "Epoch =  104 Batch =  0 Loss =  13.866167700418618 Gradient_max =  0.013973439132523862 learning rate ratio =  7.289354954536067e-05\n",
      "Epoch =  105 Batch =  0 Loss =  13.866165475098107 Gradient_max =  0.013990786428654905 learning rate ratio =  7.338646623141696e-05\n",
      "Epoch =  106 Batch =  0 Loss =  13.866163250808363 Gradient_max =  0.014008155346465653 learning rate ratio =  7.388549518001785e-05\n",
      "Epoch =  107 Batch =  0 Loss =  13.866161027549015 Gradient_max =  0.014025545912834073 learning rate ratio =  7.439075037749217e-05\n",
      "Epoch =  108 Batch =  0 Loss =  13.866158805319714 Gradient_max =  0.014042958154671648 learning rate ratio =  7.490234866214521e-05\n",
      "Epoch =  109 Batch =  0 Loss =  13.866156584046704 Gradient_max =  0.014060392098830183 learning rate ratio =  7.542040981373348e-05\n",
      "Epoch =  110 Batch =  0 Loss =  13.866154363803032 Gradient_max =  0.01407784777238154 learning rate ratio =  7.594505664747852e-05\n",
      "Epoch =  111 Batch =  0 Loss =  13.866152144588352 Gradient_max =  0.014095325202337963 learning rate ratio =  7.647641510996248e-05\n",
      "Epoch =  112 Batch =  0 Loss =  13.866149926406653 Gradient_max =  0.014112824415750175 learning rate ratio =  7.70146143798862e-05\n",
      "Epoch =  113 Batch =  0 Loss =  13.866147709253267 Gradient_max =  0.014130345439693033 learning rate ratio =  7.755978697234767e-05\n",
      "Epoch =  114 Batch =  0 Loss =  13.866145493127847 Gradient_max =  0.014147888301279889 learning rate ratio =  7.81120688473295e-05\n",
      "Epoch =  115 Batch =  0 Loss =  13.866143278030044 Gradient_max =  0.014165453027657917 learning rate ratio =  7.86715995223493e-05\n",
      "Epoch =  116 Batch =  0 Loss =  13.86614106395952 Gradient_max =  0.014183039646008134 learning rate ratio =  7.923852218958435e-05\n",
      "Epoch =  117 Batch =  0 Loss =  13.866138850915926 Gradient_max =  0.014200648183545434 learning rate ratio =  7.981298383765407e-05\n",
      "Epoch =  118 Batch =  0 Loss =  13.86613663885563 Gradient_max =  0.014218278667466057 learning rate ratio =  8.039513537809413e-05\n",
      "Epoch =  119 Batch =  0 Loss =  13.86613442782163 Gradient_max =  0.014235931125105373 learning rate ratio =  8.098513177766888e-05\n",
      "Epoch =  120 Batch =  0 Loss =  13.866132217813584 Gradient_max =  0.014253605583780173 learning rate ratio =  8.158313219493465e-05\n",
      "Epoch =  121 Batch =  0 Loss =  13.86613000883116 Gradient_max =  0.014271302070841275 learning rate ratio =  8.218930012313124e-05\n",
      "Epoch =  122 Batch =  0 Loss =  13.866127800874024 Gradient_max =  0.014289020613673589 learning rate ratio =  8.28038035387628e-05\n",
      "Epoch =  123 Batch =  0 Loss =  13.86612559391332 Gradient_max =  0.014306761239658546 learning rate ratio =  8.342681505620707e-05\n",
      "Epoch =  124 Batch =  0 Loss =  13.866123387899163 Gradient_max =  0.014324523976192109 learning rate ratio =  8.405851208892991e-05\n",
      "Epoch =  125 Batch =  0 Loss =  13.866121182857972 Gradient_max =  0.014342308850794462 learning rate ratio =  8.469907701787633e-05\n",
      "Epoch =  126 Batch =  0 Loss =  13.866118978851855 Gradient_max =  0.014360115891061304 learning rate ratio =  8.534869736636784e-05\n",
      "Epoch =  127 Batch =  0 Loss =  13.866116775869468 Gradient_max =  0.01437794512453689 learning rate ratio =  8.60075659820992e-05\n",
      "Epoch =  128 Batch =  0 Loss =  13.866114573910485 Gradient_max =  0.014395796578811577 learning rate ratio =  8.667588122789284e-05\n",
      "Epoch =  129 Batch =  0 Loss =  13.866112372984855 Gradient_max =  0.01441367028152063 learning rate ratio =  8.73538471802623e-05\n",
      "Epoch =  130 Batch =  0 Loss =  13.866110173086755 Gradient_max =  0.01443156626031711 learning rate ratio =  8.804167383648251e-05\n",
      "Epoch =  131 Batch =  0 Loss =  13.866107974211133 Gradient_max =  0.01444948454289027 learning rate ratio =  8.873957733098084e-05\n",
      "Epoch =  132 Batch =  0 Loss =  13.866105776357676 Gradient_max =  0.014467425156968274 learning rate ratio =  8.944778016119617e-05\n",
      "Epoch =  133 Batch =  0 Loss =  13.866103579530725 Gradient_max =  0.014485388130318123 learning rate ratio =  9.016651142347322e-05\n",
      "Epoch =  134 Batch =  0 Loss =  13.866101383725313 Gradient_max =  0.014503373490732847 learning rate ratio =  9.089600705946386e-05\n",
      "Epoch =  135 Batch =  0 Loss =  13.866099188941142 Gradient_max =  0.014521381266044431 learning rate ratio =  9.163651011378856e-05\n",
      "Epoch =  136 Batch =  0 Loss =  13.866096995177902 Gradient_max =  0.014539411484119538 learning rate ratio =  9.238827100333281e-05\n",
      "Epoch =  137 Batch =  0 Loss =  13.8660948024353 Gradient_max =  0.014557464172859559 learning rate ratio =  9.315154779891062e-05\n",
      "Epoch =  138 Batch =  0 Loss =  13.866092610713032 Gradient_max =  0.014575539360200642 learning rate ratio =  9.392660651994288e-05\n",
      "Epoch =  139 Batch =  0 Loss =  13.866090420063937 Gradient_max =  0.014593637074186804 learning rate ratio =  9.471372144354409e-05\n",
      "Epoch =  140 Batch =  0 Loss =  13.866088230834716 Gradient_max =  0.014611757343297215 learning rate ratio =  9.551317543047357e-05\n",
      "Epoch =  141 Batch =  0 Loss =  13.866086042627883 Gradient_max =  0.014629900195030306 learning rate ratio =  9.632526025001026e-05\n",
      "Epoch =  142 Batch =  0 Loss =  13.866083855448831 Gradient_max =  0.014648065657467105 learning rate ratio =  9.715027694875809e-05\n",
      "Epoch =  143 Batch =  0 Loss =  13.866081669291578 Gradient_max =  0.014666253758712952 learning rate ratio =  9.798853621680757e-05\n",
      "Epoch =  144 Batch =  0 Loss =  13.866079484155833 Gradient_max =  0.014684464526913571 learning rate ratio =  9.88403587770826e-05\n",
      "Epoch =  145 Batch =  0 Loss =  13.866077300046149 Gradient_max =  0.014702697990254253 learning rate ratio =  9.97060757936083e-05\n",
      "Epoch =  146 Batch =  0 Loss =  13.866075116957397 Gradient_max =  0.014720954176946422 learning rate ratio =  0.00010058602929980755\n",
      "Epoch =  147 Batch =  0 Loss =  13.866072934900128 Gradient_max =  0.014739233115252439 learning rate ratio =  0.0001014805726482797\n",
      "Epoch =  148 Batch =  0 Loss =  13.866070753863264 Gradient_max =  0.014757534833447376 learning rate ratio =  0.00010239007098271079\n",
      "Epoch =  149 Batch =  0 Loss =  13.866068573846531 Gradient_max =  0.014775859359852797 learning rate ratio =  0.00010331490173405129\n",
      "Epoch =  150 Batch =  0 Loss =  13.86606639484966 Gradient_max =  0.014794206722825536 learning rate ratio =  0.00010425545514166884\n",
      "Epoch =  151 Batch =  0 Loss =  13.866064216878321 Gradient_max =  0.014812576950763464 learning rate ratio =  0.0001052121348013599\n",
      "Epoch =  152 Batch =  0 Loss =  13.866062039933237 Gradient_max =  0.01483097007209631 learning rate ratio =  0.00010618535824166061\n",
      "Epoch =  153 Batch =  0 Loss =  13.866059864007225 Gradient_max =  0.01484938611527929 learning rate ratio =  0.00010717555753024562\n",
      "Epoch =  154 Batch =  0 Loss =  13.866057689060064 Gradient_max =  0.01486782510876229 learning rate ratio =  0.00010818317991224422\n",
      "Epoch =  155 Batch =  0 Loss =  13.866055515131478 Gradient_max =  0.014886287081127941 learning rate ratio =  0.00010920868848320074\n",
      "Epoch =  156 Batch =  0 Loss =  13.866053342221223 Gradient_max =  0.014904772060945815 learning rate ratio =  0.0001102525628968348\n",
      "Epoch =  157 Batch =  0 Loss =  13.866051170329031 Gradient_max =  0.014923280076821074 learning rate ratio =  0.0001113153001120082\n",
      "Epoch =  158 Batch =  0 Loss =  13.866048999454655 Gradient_max =  0.014941811157394543 learning rate ratio =  0.00011239741518025583\n",
      "Epoch =  159 Batch =  0 Loss =  13.86604682959784 Gradient_max =  0.014960365331342722 learning rate ratio =  0.00011349944207671874\n",
      "Epoch =  160 Batch =  0 Loss =  13.86604466071863 Gradient_max =  0.014978942627329586 learning rate ratio =  0.00011462193457706576\n",
      "Epoch =  161 Batch =  0 Loss =  13.866042492856513 Gradient_max =  0.01499754307415147 learning rate ratio =  0.00011576546718457876\n",
      "Epoch =  162 Batch =  0 Loss =  13.866040326011245 Gradient_max =  0.01501616670059226 learning rate ratio =  0.00011693063610837141\n",
      "Epoch =  163 Batch =  0 Loss =  13.866038160182578 Gradient_max =  0.015034813535471656 learning rate ratio =  0.00011811806029852186\n",
      "Epoch =  164 Batch =  0 Loss =  13.86603599537028 Gradient_max =  0.015053483607645326 learning rate ratio =  0.0001193283825407618\n",
      "Epoch =  165 Batch =  0 Loss =  13.866033831574098 Gradient_max =  0.015072176946004868 learning rate ratio =  0.00012056227061501303\n",
      "Epoch =  166 Batch =  0 Loss =  13.866031668755765 Gradient_max =  0.015090893579430418 learning rate ratio =  0.00012182041852192832\n",
      "Epoch =  167 Batch =  0 Loss =  13.866029506953115 Gradient_max =  0.01510963353693315 learning rate ratio =  0.0001231035477833317\n",
      "Epoch =  168 Batch =  0 Loss =  13.866027346165902 Gradient_max =  0.015128396847512835 learning rate ratio =  0.0001244124088193623\n",
      "Epoch =  169 Batch =  0 Loss =  13.866025186393916 Gradient_max =  0.015147183540205379 learning rate ratio =  0.00012574778241015023\n",
      "Epoch =  170 Batch =  0 Loss =  13.866023027615881 Gradient_max =  0.015165993644055492 learning rate ratio =  0.00012711048124675243\n",
      "Epoch =  171 Batch =  0 Loss =  13.866020869862698 Gradient_max =  0.015184827188208305 learning rate ratio =  0.00012850135157864927\n",
      "Epoch =  172 Batch =  0 Loss =  13.866018713123987 Gradient_max =  0.015203684201798894 learning rate ratio =  0.0001299212749634336\n",
      "Epoch =  173 Batch =  0 Loss =  13.866016557357417 Gradient_max =  0.015222564713957039 learning rate ratio =  0.00013137117012771863\n",
      "Epoch =  174 Batch =  0 Loss =  13.866014402604915 Gradient_max =  0.015241468753951186 learning rate ratio =  0.00013285199494772785\n",
      "Epoch =  175 Batch =  0 Loss =  13.866012248873416 Gradient_max =  0.015260396351043126 learning rate ratio =  0.00013436474855629796\n",
      "Epoch =  176 Batch =  0 Loss =  13.866010096155582 Gradient_max =  0.015279347534514941 learning rate ratio =  0.00013591047358834537\n",
      "Epoch =  177 Batch =  0 Loss =  13.866007944455331 Gradient_max =  0.015298322333697035 learning rate ratio =  0.0001374902585747553\n",
      "Epoch =  178 Batch =  0 Loss =  13.86600579376832 Gradient_max =  0.015317320777948822 learning rate ratio =  0.00013910524049570383\n",
      "Epoch =  179 Batch =  0 Loss =  13.86600364409434 Gradient_max =  0.015336342896670116 learning rate ratio =  0.00014075660750660832\n",
      "Epoch =  180 Batch =  0 Loss =  13.866001495433188 Gradient_max =  0.01535538871929735 learning rate ratio =  0.00014244560185009891\n",
      "Epoch =  181 Batch =  0 Loss =  13.865999347784662 Gradient_max =  0.015374458275303596 learning rate ratio =  0.00014417352296914517\n",
      "Epoch =  182 Batch =  0 Loss =  13.86599720111686 Gradient_max =  0.01539355159415996 learning rate ratio =  0.00014594173083747806\n",
      "Epoch =  183 Batch =  0 Loss =  13.865995055461315 Gradient_max =  0.01541266870545183 learning rate ratio =  0.00014775164952639342\n",
      "Epoch =  184 Batch =  0 Loss =  13.865992910817836 Gradient_max =  0.015431809638762652 learning rate ratio =  0.0001496047710253112\n",
      "Epoch =  185 Batch =  0 Loss =  13.865990767186217 Gradient_max =  0.015450974423712703 learning rate ratio =  0.00015150265933990405\n",
      "Epoch =  186 Batch =  0 Loss =  13.865988624576058 Gradient_max =  0.01547016308996823 learning rate ratio =  0.00015344695489040436\n",
      "Epoch =  187 Batch =  0 Loss =  13.865986482977375 Gradient_max =  0.015489375667214342 learning rate ratio =  0.0001554393792359322\n",
      "Epoch =  188 Batch =  0 Loss =  13.865984342345497 Gradient_max =  0.01550861218512819 learning rate ratio =  0.0001574817401538416\n",
      "Epoch =  189 Batch =  0 Loss =  13.865982202730425 Gradient_max =  0.015527872673537443 learning rate ratio =  0.00015957593710659823\n",
      "Epoch =  190 Batch =  0 Loss =  13.865980064130659 Gradient_max =  0.015547157162246021 learning rate ratio =  0.00016172396712772953\n",
      "Epoch =  191 Batch =  0 Loss =  13.865977926541657 Gradient_max =  0.015566465681092401 learning rate ratio =  0.0001639279311687413\n",
      "Epoch =  192 Batch =  0 Loss =  13.865975789963253 Gradient_max =  0.015585798259956218 learning rate ratio =  0.0001661900409476375\n",
      "Epoch =  193 Batch =  0 Loss =  13.865973654395267 Gradient_max =  0.015605154928754365 learning rate ratio =  0.00016851262634618152\n",
      "Epoch =  194 Batch =  0 Loss =  13.865971519837533 Gradient_max =  0.015624535717441003 learning rate ratio =  0.00017089814340803081\n",
      "Epoch =  195 Batch =  0 Loss =  13.865969386294164 Gradient_max =  0.0156439406560115 learning rate ratio =  0.00017334918299546505\n",
      "Epoch =  196 Batch =  0 Loss =  13.865967253760704 Gradient_max =  0.015663369774490826 learning rate ratio =  0.00017586848016866937\n",
      "Epoch =  197 Batch =  0 Loss =  13.865965122236995 Gradient_max =  0.015682823102945262 learning rate ratio =  0.0001784589243593156\n",
      "Epoch =  198 Batch =  0 Loss =  13.865962991722876 Gradient_max =  0.01570230067147857 learning rate ratio =  0.00018112357041764275\n",
      "Epoch =  199 Batch =  0 Loss =  13.86596086221819 Gradient_max =  0.01572180251023201 learning rate ratio =  0.00018386565062200925\n",
      "Epoch =  200 Batch =  0 Loss =  13.865958733722781 Gradient_max =  0.01574132864938441 learning rate ratio =  0.00018668858775014826\n",
      "Epoch =  201 Batch =  0 Loss =  13.8659566062365 Gradient_max =  0.015760879119152183 learning rate ratio =  0.0001895960093232662\n",
      "Epoch =  202 Batch =  0 Loss =  13.86595447975919 Gradient_max =  0.015780453949789417 learning rate ratio =  0.0001925917631476239\n",
      "Epoch =  203 Batch =  0 Loss =  13.865952354239129 Gradient_max =  0.015800053171525434 learning rate ratio =  0.00019567993429311809\n",
      "Epoch =  204 Batch =  0 Loss =  13.865950229749178 Gradient_max =  0.015819676814775398 learning rate ratio =  0.00019886486366909562\n",
      "Epoch =  205 Batch =  0 Loss =  13.865948106276667 Gradient_max =  0.015839324909893544 learning rate ratio =  0.00020215116836902445\n",
      "Epoch =  206 Batch =  0 Loss =  13.865945983812669 Gradient_max =  0.015858997487275078 learning rate ratio =  0.00020554376399130903\n",
      "Epoch =  207 Batch =  0 Loss =  13.865943862357042 Gradient_max =  0.01587869457736304 learning rate ratio =  0.00020904788915942135\n",
      "Epoch =  208 Batch =  0 Loss =  13.86594174190966 Gradient_max =  0.015898416210638446 learning rate ratio =  0.00021266913249917574\n",
      "Epoch =  209 Batch =  0 Loss =  13.865939622470384 Gradient_max =  0.015918162417620244 learning rate ratio =  0.0002164134623656546\n",
      "Epoch =  210 Batch =  0 Loss =  13.865937504043849 Gradient_max =  0.01593793322886984 learning rate ratio =  0.00022028725965231758\n",
      "Epoch =  211 Batch =  0 Loss =  13.865935386625155 Gradient_max =  0.015957728674977897 learning rate ratio =  0.00022429735406141757\n",
      "Epoch =  212 Batch =  0 Loss =  13.865933270214182 Gradient_max =  0.015977548786577624 learning rate ratio =  0.00022845106426990994\n",
      "Epoch =  213 Batch =  0 Loss =  13.865931154810808 Gradient_max =  0.01599739359434036 learning rate ratio =  0.00023275624248730726\n",
      "Epoch =  214 Batch =  0 Loss =  13.865929040414908 Gradient_max =  0.016017263128975716 learning rate ratio =  0.00023722132397644436\n",
      "Epoch =  215 Batch =  0 Loss =  13.865926927026367 Gradient_max =  0.01603715742123154 learning rate ratio =  0.00024185538219419886\n",
      "Epoch =  216 Batch =  0 Loss =  13.865924814602247 Gradient_max =  0.016057076501841946 learning rate ratio =  0.00024666819031009756\n",
      "Epoch =  217 Batch =  0 Loss =  13.865922703137105 Gradient_max =  0.016077020401625085 learning rate ratio =  0.00025167028998241645\n",
      "Epoch =  218 Batch =  0 Loss =  13.865920592642674 Gradient_max =  0.016096989151458022 learning rate ratio =  0.0002568730684077944\n",
      "Epoch =  219 Batch =  0 Loss =  13.865918483155257 Gradient_max =  0.016116982782286574 learning rate ratio =  0.0002622888448310298\n",
      "Epoch =  220 Batch =  0 Loss =  13.86591637467476 Gradient_max =  0.016137001325050637 learning rate ratio =  0.0002679309678971658\n",
      "Epoch =  221 Batch =  0 Loss =  13.865914267201065 Gradient_max =  0.016157044810728674 learning rate ratio =  0.00027381392546734927\n",
      "Epoch =  222 Batch =  0 Loss =  13.86591216073409 Gradient_max =  0.016177113270337724 learning rate ratio =  0.00027995346879874873\n",
      "Epoch =  223 Batch =  0 Loss =  13.865910055273732 Gradient_max =  0.01619720673493351 learning rate ratio =  0.0002863667533299214\n",
      "Epoch =  224 Batch =  0 Loss =  13.865907950782823 Gradient_max =  0.01621732523556521 learning rate ratio =  0.00029307249872074153\n",
      "Epoch =  225 Batch =  0 Loss =  13.865905847307847 Gradient_max =  0.01623746880342193 learning rate ratio =  0.000300091171293715\n",
      "Epoch =  226 Batch =  0 Loss =  13.865903744848755 Gradient_max =  0.01625763746967565 learning rate ratio =  0.00030744519261572415\n",
      "Epoch =  227 Batch =  0 Loss =  13.865901643367776 Gradient_max =  0.016277831265491274 learning rate ratio =  0.0003151591787035579\n",
      "Epoch =  228 Batch =  0 Loss =  13.86589954286014 Gradient_max =  0.016298050222113347 learning rate ratio =  0.0003232602152284871\n",
      "Epoch =  229 Batch =  0 Loss =  13.865897443358739 Gradient_max =  0.016318294370870835 learning rate ratio =  0.0003317781752011406\n",
      "Epoch =  230 Batch =  0 Loss =  13.865895344820222 Gradient_max =  0.01633856374303876 learning rate ratio =  0.0003407460869879742\n",
      "Epoch =  231 Batch =  0 Loss =  13.865893247287818 Gradient_max =  0.016358858370036346 learning rate ratio =  0.0003502005622216574\n",
      "Epoch =  232 Batch =  0 Loss =  13.86589115072458 Gradient_max =  0.016379178283224414 learning rate ratio =  0.0003601822952837208\n",
      "Epoch =  233 Batch =  0 Loss =  13.865889055167314 Gradient_max =  0.0163995235140927 learning rate ratio =  0.0003707366487384767\n",
      "Epoch =  234 Batch =  0 Loss =  13.865886960615951 Gradient_max =  0.016419894094125226 learning rate ratio =  0.00038191434247208467\n",
      "Epoch =  235 Batch =  0 Loss =  13.865884867070424 Gradient_max =  0.01644029005484523 learning rate ratio =  0.0003937722686293786\n",
      "Epoch =  236 Batch =  0 Loss =  13.865882774530679 Gradient_max =  0.01646071142781526 learning rate ratio =  0.00040637445997599965\n",
      "Epoch =  237 Batch =  0 Loss =  13.865880682996654 Gradient_max =  0.016481158244637157 learning rate ratio =  0.00041979324647730966\n",
      "Epoch =  238 Batch =  0 Loss =  13.865878592468297 Gradient_max =  0.016501630536952167 learning rate ratio =  0.00043411064420309357\n",
      "Epoch =  239 Batch =  0 Loss =  13.865876502955372 Gradient_max =  0.016522128336452065 learning rate ratio =  0.00044942003289032346\n",
      "Epoch =  240 Batch =  0 Loss =  13.865874414448033 Gradient_max =  0.016542651674845893 learning rate ratio =  0.00046582819466470374\n",
      "Epoch =  241 Batch =  0 Loss =  13.865872326946242 Gradient_max =  0.016563200583893302 learning rate ratio =  0.0004834578080123281\n",
      "Epoch =  242 Batch =  0 Loss =  13.865870240449954 Gradient_max =  0.01658377509539352 learning rate ratio =  0.00050245052018353\n",
      "Epoch =  243 Batch =  0 Loss =  13.865868154971059 Gradient_max =  0.0166043752411989 learning rate ratio =  0.0005229707608413694\n",
      "Epoch =  244 Batch =  0 Loss =  13.86586606997645 Gradient_max =  0.016625001052499703 learning rate ratio =  0.0005452105143145964\n",
      "Epoch =  245 Batch =  0 Loss =  13.865863984902504 Gradient_max =  0.01664565256048799 learning rate ratio =  0.0005693953439192135\n",
      "Epoch =  246 Batch =  0 Loss =  13.865861900834197 Gradient_max =  0.016666329798523054 learning rate ratio =  0.0005957920691110487\n",
      "Epoch =  247 Batch =  0 Loss =  13.8658598177715 Gradient_max =  0.016687032798602706 learning rate ratio =  0.0006247186496719346\n",
      "Epoch =  248 Batch =  0 Loss =  13.865857735714386 Gradient_max =  0.01670776159276466 learning rate ratio =  0.0006565570547485862\n",
      "Epoch =  249 Batch =  0 Loss =  13.865855654675618 Gradient_max =  0.016728516213100994 learning rate ratio =  0.0006917702238476907\n",
      "Epoch =  250 Batch =  0 Loss =  13.86585357464711 Gradient_max =  0.016749296691719127 learning rate ratio =  0.0007309247227571249\n",
      "Epoch =  251 Batch =  0 Loss =  13.865851495640165 Gradient_max =  0.016770103060789462 learning rate ratio =  0.0007747214574981566\n",
      "Epoch =  252 Batch =  0 Loss =  13.865849417604576 Gradient_max =  0.01679093535245045 learning rate ratio =  0.0008240380006601906\n",
      "Epoch =  253 Batch =  0 Loss =  13.865847340574547 Gradient_max =  0.016811793598982067 learning rate ratio =  0.0008799879967770294\n",
      "Epoch =  254 Batch =  0 Loss =  13.865845264550071 Gradient_max =  0.016832677832662235 learning rate ratio =  0.0009440062665441427\n",
      "Epoch =  255 Batch =  0 Loss =  13.865843189495832 Gradient_max =  0.01685358808576688 learning rate ratio =  0.001017973587964657\n",
      "Epoch =  256 Batch =  0 Loss =  13.865841115451905 Gradient_max =  0.016874524390701046 learning rate ratio =  0.001104404553612084\n",
      "Epoch =  257 Batch =  0 Loss =  13.865839042413542 Gradient_max =  0.016895486779859158 learning rate ratio =  0.0012067391249593136\n",
      "Epoch =  258 Batch =  0 Loss =  13.865836970380743 Gradient_max =  0.016916475285680306 learning rate ratio =  0.0013298114309532926\n",
      "Epoch =  259 Batch =  0 Loss =  13.865834899353512 Gradient_max =  0.016937489940644035 learning rate ratio =  0.0014806356945445988\n",
      "Epoch =  260 Batch =  0 Loss =  13.865832829336172 Gradient_max =  0.01695853077727428 learning rate ratio =  0.001669791372877465\n",
      "Epoch =  261 Batch =  0 Loss =  13.865830760324418 Gradient_max =  0.016979597828127604 learning rate ratio =  0.0019140181010565535\n",
      "Epoch =  262 Batch =  0 Loss =  13.865828692318253 Gradient_max =  0.017000691125805087 learning rate ratio =  0.002241463238913838\n",
      "Epoch =  263 Batch =  0 Loss =  13.8658266253177 Gradient_max =  0.017021810702948423 learning rate ratio =  0.0027033951268651945\n",
      "Epoch =  264 Batch =  0 Loss =  13.865824559322785 Gradient_max =  0.017042956592240012 learning rate ratio =  0.003404078404579833\n",
      "Epoch =  265 Batch =  0 Loss =  13.865822494333518 Gradient_max =  0.017064128826402916 learning rate ratio =  0.004593115607078224\n",
      "Epoch =  266 Batch =  0 Loss =  13.865820429215624 Gradient_max =  0.017085327437086198 learning rate ratio =  0.007054138689866668\n",
      "Epoch =  267 Batch =  0 Loss =  13.86581836499473 Gradient_max =  0.01710655245809721 learning rate ratio =  0.015175429862823555\n",
      "Epoch =  268 Batch =  0 Loss =  13.865816301779816 Gradient_max =  0.017127803922393877 learning rate ratio =  0.0002357229656557001\n",
      "Epoch =  269 Batch =  0 Loss =  13.865814239582917 Gradient_max =  0.01714908186287624 learning rate ratio =  0.00024023508100018596\n",
      "Epoch =  270 Batch =  0 Loss =  13.865812178392094 Gradient_max =  0.017170386312458215 learning rate ratio =  0.0002449175547592511\n",
      "Epoch =  271 Batch =  0 Loss =  13.865810118212295 Gradient_max =  0.017191717304112848 learning rate ratio =  0.00024978021657856396\n",
      "Epoch =  272 Batch =  0 Loss =  13.865808059031718 Gradient_max =  0.01721307487083326 learning rate ratio =  0.0002548336671581537\n",
      "Epoch =  273 Batch =  0 Loss =  13.865806000857388 Gradient_max =  0.017234459045682028 learning rate ratio =  0.00026008935536860203\n",
      "Epoch =  274 Batch =  0 Loss =  13.865803943689343 Gradient_max =  0.017255869861750997 learning rate ratio =  0.0002655596648107594\n",
      "Epoch =  275 Batch =  0 Loss =  13.865801887527644 Gradient_max =  0.017277307352173258 learning rate ratio =  0.0002712580111972112\n",
      "Epoch =  276 Batch =  0 Loss =  13.865799832377382 Gradient_max =  0.017298771550127844 learning rate ratio =  0.0002771989521672314\n",
      "Epoch =  277 Batch =  0 Loss =  13.86579777825358 Gradient_max =  0.017320262488848246 learning rate ratio =  0.0002833983114275642\n",
      "Epoch =  278 Batch =  0 Loss =  13.865795725148182 Gradient_max =  0.017341780201582536 learning rate ratio =  0.00028987331944706955\n",
      "Epoch =  279 Batch =  0 Loss =  13.865793673049426 Gradient_max =  0.01736332472161581 learning rate ratio =  0.0002966427733409144\n",
      "Epoch =  280 Batch =  0 Loss =  13.865791621957381 Gradient_max =  0.017384896082288076 learning rate ratio =  0.0003037272190675992\n",
      "Epoch =  281 Batch =  0 Loss =  13.865789571872124 Gradient_max =  0.017406494316980842 learning rate ratio =  0.00031114915965781777\n",
      "Epoch =  282 Batch =  0 Loss =  13.865787522793722 Gradient_max =  0.017428119459117267 learning rate ratio =  0.00031893329392106466\n",
      "Epoch =  283 Batch =  0 Loss =  13.865785474733343 Gradient_max =  0.01744977154217461 learning rate ratio =  0.00032710679096448984\n",
      "Epoch =  284 Batch =  0 Loss =  13.865783427697336 Gradient_max =  0.017471450599665362 learning rate ratio =  0.00033569960695217663\n",
      "Epoch =  285 Batch =  0 Loss =  13.865781381668466 Gradient_max =  0.01749315666511922 learning rate ratio =  0.000344744851887917\n",
      "Epoch =  286 Batch =  0 Loss =  13.865779336651924 Gradient_max =  0.017514889772130924 learning rate ratio =  0.0003542792158883577\n",
      "Epoch =  287 Batch =  0 Loss =  13.865777291670838 Gradient_max =  0.017536649952978912 learning rate ratio =  0.0003643434664933943\n",
      "Epoch =  288 Batch =  0 Loss =  13.86577524731739 Gradient_max =  0.01755843724216167 learning rate ratio =  0.00037498303132159214\n",
      "Epoch =  289 Batch =  0 Loss =  13.865773203971667 Gradient_max =  0.01758025167391908 learning rate ratio =  0.00038624868347280993\n",
      "Epoch =  290 Batch =  0 Loss =  13.865771161633756 Gradient_max =  0.017602093282008884 learning rate ratio =  0.0003981973516115658\n",
      "Epoch =  291 Batch =  0 Loss =  13.865769120335143 Gradient_max =  0.01762396210026527 learning rate ratio =  0.0004108930819862683\n",
      "Epoch =  292 Batch =  0 Loss =  13.865767080049746 Gradient_max =  0.017645858162500513 learning rate ratio =  0.0004244081866742796\n",
      "Epoch =  293 Batch =  0 Loss =  13.865765040751027 Gradient_max =  0.017667781502568057 learning rate ratio =  0.0004388246215313353\n",
      "Epoch =  294 Batch =  0 Loss =  13.86576300246062 Gradient_max =  0.01768973215441994 learning rate ratio =  0.0004542356492997026\n",
      "Epoch =  295 Batch =  0 Loss =  13.865760965192631 Gradient_max =  0.01771171015204047 learning rate ratio =  0.0004707478591804012\n",
      "Epoch =  296 Batch =  0 Loss =  13.865758928933218 Gradient_max =  0.01773371552942471 learning rate ratio =  0.0004884836353118246\n",
      "Epoch =  297 Batch =  0 Loss =  13.865756893682503 Gradient_max =  0.017755748320625847 learning rate ratio =  0.0005075841950470128\n",
      "Epoch =  298 Batch =  0 Loss =  13.865754859440607 Gradient_max =  0.01777780855973952 learning rate ratio =  0.0005282133566126012\n",
      "Epoch =  299 Batch =  0 Loss =  13.865752826193082 Gradient_max =  0.017799896280884752 learning rate ratio =  0.0005505622489473206\n",
      "Epoch =  300 Batch =  0 Loss =  13.865750793960766 Gradient_max =  0.017822011518267043 learning rate ratio =  0.0005748552505560889\n",
      "Epoch =  301 Batch =  0 Loss =  13.86574876271376 Gradient_max =  0.017844154306075305 learning rate ratio =  0.0006013575485445364\n",
      "Epoch =  302 Batch =  0 Loss =  13.86574673247615 Gradient_max =  0.01786632467860393 learning rate ratio =  0.0006303848580770663\n",
      "Epoch =  303 Batch =  0 Loss =  13.865744703248073 Gradient_max =  0.017888522670161437 learning rate ratio =  0.000662316058650425\n",
      "Epoch =  304 Batch =  0 Loss =  13.865742675029677 Gradient_max =  0.01791074831509913 learning rate ratio =  0.0006976098221895651\n",
      "Epoch =  305 Batch =  0 Loss =  13.865740647821104 Gradient_max =  0.017933001647811076 learning rate ratio =  0.0007368267859189347\n",
      "Epoch =  306 Batch =  0 Loss =  13.865738621622514 Gradient_max =  0.017955282702734183 learning rate ratio =  0.0007806595540589576\n",
      "Epoch =  307 Batch =  0 Loss =  13.865736596434058 Gradient_max =  0.017977591514348287 learning rate ratio =  0.0008299739549031015\n",
      "Epoch =  308 Batch =  0 Loss =  13.865734572255898 Gradient_max =  0.01799992811717617 learning rate ratio =  0.0008858668081491955\n",
      "Epoch =  309 Batch =  0 Loss =  13.865732549096133 Gradient_max =  0.018022292545790693 learning rate ratio =  0.0009497484615912647\n",
      "Epoch =  310 Batch =  0 Loss =  13.865730526951799 Gradient_max =  0.018044684834798073 learning rate ratio =  0.00102346344144103\n",
      "Epoch =  311 Batch =  0 Loss =  13.865728505818236 Gradient_max =  0.018067105018845814 learning rate ratio =  0.0011094714617353879\n",
      "Epoch =  312 Batch =  0 Loss =  13.86572648570032 Gradient_max =  0.018089553132633312 learning rate ratio =  0.0012111272267205685\n",
      "Epoch =  313 Batch =  0 Loss =  13.865724466593509 Gradient_max =  0.018112029210894637 learning rate ratio =  0.0013331282279666918\n",
      "Epoch =  314 Batch =  0 Loss =  13.865722448473822 Gradient_max =  0.018134533288382628 learning rate ratio =  0.0014822612967724837\n",
      "Epoch =  315 Batch =  0 Loss =  13.865720431377854 Gradient_max =  0.018157065399964898 learning rate ratio =  0.0016687095433126227\n",
      "Epoch =  316 Batch =  0 Loss =  13.8657184152936 Gradient_max =  0.018179625580496024 learning rate ratio =  0.001908480609689892\n",
      "Epoch =  317 Batch =  0 Loss =  13.865716400202212 Gradient_max =  0.018202213864863752 learning rate ratio =  0.002228265908633432\n",
      "Epoch =  318 Batch =  0 Loss =  13.865714386122932 Gradient_max =  0.018224830288047406 learning rate ratio =  0.0026761388246673264\n",
      "Epoch =  319 Batch =  0 Loss =  13.86571237305978 Gradient_max =  0.018247474885049186 learning rate ratio =  0.003348327319969859\n",
      "Epoch =  320 Batch =  0 Loss =  13.865710361009128 Gradient_max =  0.01827014769090814 learning rate ratio =  0.004469652399521311\n",
      "Epoch =  321 Batch =  0 Loss =  13.865708349971177 Gradient_max =  0.018292848740710313 learning rate ratio =  0.006716098485893966\n",
      "Epoch =  322 Batch =  0 Loss =  13.865706339946135 Gradient_max =  0.018315578069585527 learning rate ratio =  0.013485947889688758\n",
      "Epoch =  323 Batch =  0 Loss =  13.865704330939689 Gradient_max =  0.018338335712712463 learning rate ratio =  0.00018014250161311858\n",
      "Epoch =  324 Batch =  0 Loss =  13.865702322952036 Gradient_max =  0.018361121705308448 learning rate ratio =  0.0001827762764839345\n",
      "Epoch =  325 Batch =  0 Loss =  13.86570031598208 Gradient_max =  0.018383936082633372 learning rate ratio =  0.00018548513972941607\n",
      "Epoch =  326 Batch =  0 Loss =  13.86569831002587 Gradient_max =  0.018406778879988643 learning rate ratio =  0.0001882723458378739\n",
      "Epoch =  327 Batch =  0 Loss =  13.865696305083627 Gradient_max =  0.018429650132723383 learning rate ratio =  0.00019114134013141422\n",
      "Epoch =  328 Batch =  0 Loss =  13.865694301168533 Gradient_max =  0.018452549876245488 learning rate ratio =  0.00019409577296100423\n",
      "Epoch =  329 Batch =  0 Loss =  13.865692298267893 Gradient_max =  0.018475478145977546 learning rate ratio =  0.0001971395151902888\n",
      "Epoch =  330 Batch =  0 Loss =  13.865690296381933 Gradient_max =  0.018498434977400998 learning rate ratio =  0.00020027667509985104\n",
      "Epoch =  331 Batch =  0 Loss =  13.865688295510898 Gradient_max =  0.018521420406041454 learning rate ratio =  0.00020351161687367106\n",
      "Epoch =  332 Batch =  0 Loss =  13.865686295655028 Gradient_max =  0.018544434467468788 learning rate ratio =  0.00020684898083948318\n",
      "Epoch =  333 Batch =  0 Loss =  13.86568429681939 Gradient_max =  0.018567477197301594 learning rate ratio =  0.0002102937056608146\n",
      "Epoch =  334 Batch =  0 Loss =  13.8656822989994 Gradient_max =  0.018590548631193995 learning rate ratio =  0.00021385105270518888\n",
      "Epoch =  335 Batch =  0 Loss =  13.865680302195319 Gradient_max =  0.018613648804849 learning rate ratio =  0.00021752663283881697\n",
      "Epoch =  336 Batch =  0 Loss =  13.865678306407393 Gradient_max =  0.018636777754014042 learning rate ratio =  0.00022132643593729512\n",
      "Epoch =  337 Batch =  0 Loss =  13.865676311635884 Gradient_max =  0.018659935514481103 learning rate ratio =  0.00022525686343752936\n",
      "Epoch =  338 Batch =  0 Loss =  13.86567431788106 Gradient_max =  0.018683122122086768 learning rate ratio =  0.00022932476430323668\n",
      "Epoch =  339 Batch =  0 Loss =  13.865672325147129 Gradient_max =  0.018706337612715715 learning rate ratio =  0.00023353747482884434\n",
      "Epoch =  340 Batch =  0 Loss =  13.865670333430405 Gradient_max =  0.018729582022290373 learning rate ratio =  0.0002379028627695766\n",
      "Epoch =  341 Batch =  0 Loss =  13.865668342731176 Gradient_max =  0.018752855386781394 learning rate ratio =  0.00024242937635391462\n",
      "Epoch =  342 Batch =  0 Loss =  13.865666353035447 Gradient_max =  0.018776157742186727 learning rate ratio =  0.00024712609882384734\n",
      "Epoch =  343 Batch =  0 Loss =  13.865664364357784 Gradient_max =  0.018799489124584273 learning rate ratio =  0.0002520028092387605\n",
      "Epoch =  344 Batch =  0 Loss =  13.865662376680154 Gradient_max =  0.018822849570057098 learning rate ratio =  0.00025707005040841586\n",
      "Epoch =  345 Batch =  0 Loss =  13.865660390021187 Gradient_max =  0.018846239114777646 learning rate ratio =  0.0002623392049352891\n",
      "Epoch =  346 Batch =  0 Loss =  13.865658404381175 Gradient_max =  0.018869657794941163 learning rate ratio =  0.0002678225805345993\n",
      "Epoch =  347 Batch =  0 Loss =  13.865656419760413 Gradient_max =  0.01889310564678802 learning rate ratio =  0.0002735335059671934\n",
      "Epoch =  348 Batch =  0 Loss =  13.865654436159204 Gradient_max =  0.018916582706603704 learning rate ratio =  0.00027948643916422236\n",
      "Epoch =  349 Batch =  0 Loss =  13.86565245360052 Gradient_max =  0.01894008901074356 learning rate ratio =  0.0002856970893873948\n",
      "Epoch =  350 Batch =  0 Loss =  13.865650472039055 Gradient_max =  0.01896362459553147 learning rate ratio =  0.0002921825556077434\n",
      "Epoch =  351 Batch =  0 Loss =  13.865648491498119 Gradient_max =  0.018987189497416183 learning rate ratio =  0.00029896148364557854\n",
      "Epoch =  352 Batch =  0 Loss =  13.865646511978014 Gradient_max =  0.019010783752864368 learning rate ratio =  0.0003060542451434218\n",
      "Epoch =  353 Batch =  0 Loss =  13.865644533459179 Gradient_max =  0.019034407398364137 learning rate ratio =  0.0003134831419691271\n",
      "Epoch =  354 Batch =  0 Loss =  13.865642555949695 Gradient_max =  0.019058060470481725 learning rate ratio =  0.00032127264037135246\n",
      "Epoch =  355 Batch =  0 Loss =  13.865640579475489 Gradient_max =  0.019081743005850704 learning rate ratio =  0.0003294496400741664\n",
      "Epoch =  356 Batch =  0 Loss =  13.865638604029055 Gradient_max =  0.019105455041109828 learning rate ratio =  0.00033804378454541345\n",
      "Epoch =  357 Batch =  0 Loss =  13.865636629634981 Gradient_max =  0.019129196612981944 learning rate ratio =  0.00034708781996842394\n",
      "Epoch =  358 Batch =  0 Loss =  13.865634656284481 Gradient_max =  0.019152967758192783 learning rate ratio =  0.00035661801209924707\n",
      "Epoch =  359 Batch =  0 Loss =  13.865632683957227 Gradient_max =  0.019176768513508767 learning rate ratio =  0.0003666746321765733\n",
      "Epoch =  360 Batch =  0 Loss =  13.865630712607382 Gradient_max =  0.019200598915706316 learning rate ratio =  0.0003773025256228972\n",
      "Epoch =  361 Batch =  0 Loss =  13.86562874228153 Gradient_max =  0.019224459001718243 learning rate ratio =  0.00038855178046025773\n",
      "Epoch =  362 Batch =  0 Loss =  13.865626772984454 Gradient_max =  0.01924834880847196 learning rate ratio =  0.0004004785164860356\n",
      "Epoch =  363 Batch =  0 Loss =  13.865624804712075 Gradient_max =  0.019272268372932937 learning rate ratio =  0.0004131458214127848\n",
      "Epoch =  364 Batch =  0 Loss =  13.865622837464755 Gradient_max =  0.019296217732116674 learning rate ratio =  0.00042662486694314256\n",
      "Epoch =  365 Batch =  0 Loss =  13.865620871242855 Gradient_max =  0.019320196923084777 learning rate ratio =  0.000440996246465035\n",
      "Epoch =  366 Batch =  0 Loss =  13.865618906051202 Gradient_max =  0.019344205982949017 learning rate ratio =  0.00045635158746306547\n",
      "Epoch =  367 Batch =  0 Loss =  13.865616941214336 Gradient_max =  0.019368244947990984 learning rate ratio =  0.00047279550682836793\n",
      "Epoch =  368 Batch =  0 Loss =  13.865614976829459 Gradient_max =  0.01939231385553792 learning rate ratio =  0.0004904479971441363\n",
      "Epoch =  369 Batch =  0 Loss =  13.865613013471691 Gradient_max =  0.019416412743578106 learning rate ratio =  0.0005094473591819404\n",
      "Epoch =  370 Batch =  0 Loss =  13.86561105114141 Gradient_max =  0.019440541649404553 learning rate ratio =  0.0005299538322059878\n",
      "Epoch =  371 Batch =  0 Loss =  13.865609089839008 Gradient_max =  0.019464700610356704 learning rate ratio =  0.000552154123673939\n",
      "Epoch =  372 Batch =  0 Loss =  13.865607129564879 Gradient_max =  0.019488889663820577 learning rate ratio =  0.0005762671096271373\n",
      "Epoch =  373 Batch =  0 Loss =  13.865605170340023 Gradient_max =  0.019513108847250958 learning rate ratio =  0.0006025510746054398\n",
      "Epoch =  374 Batch =  0 Loss =  13.865603212136113 Gradient_max =  0.01953735819809425 learning rate ratio =  0.0006313129990520717\n",
      "Epoch =  375 Batch =  0 Loss =  13.865601254948734 Gradient_max =  0.019561637753871836 learning rate ratio =  0.0006629206031291128\n",
      "Epoch =  376 Batch =  0 Loss =  13.865599298777376 Gradient_max =  0.019585947552155328 learning rate ratio =  0.000697818151083527\n",
      "Epoch =  377 Batch =  0 Loss =  13.86559734363638 Gradient_max =  0.01961028763058019 learning rate ratio =  0.0007365474612258634\n",
      "Epoch =  378 Batch =  0 Loss =  13.86559538951477 Gradient_max =  0.01963465802679938 learning rate ratio =  0.0007797762381184716\n",
      "Epoch =  379 Batch =  0 Loss =  13.865593436424392 Gradient_max =  0.019659058778539407 learning rate ratio =  0.0008283368877123188\n",
      "Epoch =  380 Batch =  0 Loss =  13.865591484380642 Gradient_max =  0.01968348992357734 learning rate ratio =  0.0008832806380288928\n",
      "Epoch =  381 Batch =  0 Loss =  13.865589533382456 Gradient_max =  0.019707951499715763 learning rate ratio =  0.0009459545017988472\n",
      "Epoch =  382 Batch =  0 Loss =  13.865587583416787 Gradient_max =  0.01973244354479666 learning rate ratio =  0.0010181131794304108\n",
      "Epoch =  383 Batch =  0 Loss =  13.865585634484068 Gradient_max =  0.019756966096721496 learning rate ratio =  0.0011020859235801512\n",
      "Epoch =  384 Batch =  0 Loss =  13.865583686573892 Gradient_max =  0.019781519193426154 learning rate ratio =  0.0012010326669643868\n",
      "Epoch =  385 Batch =  0 Loss =  13.86558173969758 Gradient_max =  0.019806102872919343 learning rate ratio =  0.001319350575775337\n",
      "Epoch =  386 Batch =  0 Loss =  13.86557979385557 Gradient_max =  0.019830717173244278 learning rate ratio =  0.0014633452827717069\n",
      "Epoch =  387 Batch =  0 Loss =  13.865577849048332 Gradient_max =  0.01985536213249161 learning rate ratio =  0.0016423923026405655\n",
      "Epoch =  388 Batch =  0 Loss =  13.86557590527631 Gradient_max =  0.019880037788799425 learning rate ratio =  0.0018710641308847045\n",
      "Epoch =  389 Batch =  0 Loss =  13.865573962525648 Gradient_max =  0.019904744180336838 learning rate ratio =  0.0021733104859691\n",
      "Epoch =  390 Batch =  0 Loss =  13.865572020811163 Gradient_max =  0.019929481345353495 learning rate ratio =  0.002591446997730543\n",
      "Epoch =  391 Batch =  0 Loss =  13.86557008014344 Gradient_max =  0.019954249322139503 learning rate ratio =  0.003207935770233277\n",
      "Epoch =  392 Batch =  0 Loss =  13.86556814051283 Gradient_max =  0.01997904814901395 learning rate ratio =  0.004207799754504561\n",
      "Epoch =  393 Batch =  0 Loss =  13.865566201910891 Gradient_max =  0.020003877864341647 learning rate ratio =  0.006110019638380678\n",
      "Epoch =  394 Batch =  0 Loss =  13.865564264347038 Gradient_max =  0.02002873850655802 learning rate ratio =  0.011140575151497379\n",
      "Epoch =  395 Batch =  0 Loss =  13.865562327800443 Gradient_max =  0.020053630114109097 learning rate ratio =  0.06272361429903177\n",
      "Epoch =  396 Batch =  0 Loss =  13.865560392278557 Gradient_max =  0.020078552725523998 learning rate ratio =  0.0005814755928175902\n",
      "Epoch =  397 Batch =  0 Loss =  13.865558457796292 Gradient_max =  0.02010350637938705 learning rate ratio =  0.0006131375091663482\n",
      "Epoch =  398 Batch =  0 Loss =  13.865556524358874 Gradient_max =  0.020128491114318345 learning rate ratio =  0.0006484012198975833\n",
      "Epoch =  399 Batch =  0 Loss =  13.865554591962074 Gradient_max =  0.020153506968977455 learning rate ratio =  0.0006879183566942323\n",
      "Epoch =  400 Batch =  0 Loss =  13.865552660625381 Gradient_max =  0.020178553982097988 learning rate ratio =  0.0007325078288611023\n",
      "Epoch =  401 Batch =  0 Loss =  13.865550730330355 Gradient_max =  0.020203632192418716 learning rate ratio =  0.0007832131807324414\n",
      "Epoch =  402 Batch =  0 Loss =  13.865548801065126 Gradient_max =  0.020228741638733803 learning rate ratio =  0.0008413852872840952\n",
      "Epoch =  403 Batch =  0 Loss =  13.865546872842618 Gradient_max =  0.02025388235991456 learning rate ratio =  0.000908804484812632\n",
      "Epoch =  404 Batch =  0 Loss =  13.865544945663355 Gradient_max =  0.02027905439486628 learning rate ratio =  0.0009878659706083499\n",
      "Epoch =  405 Batch =  0 Loss =  13.865543019546338 Gradient_max =  0.02030425778256363 learning rate ratio =  0.0010818703239437709\n",
      "Epoch =  406 Batch =  0 Loss =  13.865541094488574 Gradient_max =  0.020329492562004704 learning rate ratio =  0.001195495942992314\n",
      "Epoch =  407 Batch =  0 Loss =  13.865539170479787 Gradient_max =  0.020354758772227335 learning rate ratio =  0.0013356017786515322\n",
      "Epoch =  408 Batch =  0 Loss =  13.865537247516428 Gradient_max =  0.020380056452327364 learning rate ratio =  0.0015126653066890163\n",
      "Epoch =  409 Batch =  0 Loss =  13.865535325599053 Gradient_max =  0.020405385641452955 learning rate ratio =  0.0017435313141129607\n",
      "Epoch =  410 Batch =  0 Loss =  13.865533404733368 Gradient_max =  0.02043074637880655 learning rate ratio =  0.0020571152891562185\n",
      "Epoch =  411 Batch =  0 Loss =  13.865531484931212 Gradient_max =  0.020456138703647066 learning rate ratio =  0.002507575173186443\n",
      "Epoch =  412 Batch =  0 Loss =  13.86552956617677 Gradient_max =  0.020481562655250623 learning rate ratio =  0.0032095460966720616\n",
      "Epoch =  413 Batch =  0 Loss =  13.865527648470602 Gradient_max =  0.02050701827296084 learning rate ratio =  0.004455251593185195\n",
      "Epoch =  414 Batch =  0 Loss =  13.865525731801126 Gradient_max =  0.02053250559615616 learning rate ratio =  0.007275767224203733\n",
      "Epoch =  415 Batch =  0 Loss =  13.865523816181096 Gradient_max =  0.020558024664292438 learning rate ratio =  0.019788100446905287\n",
      "Epoch =  416 Batch =  0 Loss =  13.865521901611084 Gradient_max =  0.02058357551686046 learning rate ratio =  0.00038379351446784295\n",
      "Epoch =  417 Batch =  0 Loss =  13.865519988091677 Gradient_max =  0.02060915819340024 learning rate ratio =  0.00039371883799782357\n",
      "Epoch =  418 Batch =  0 Loss =  13.865518075642289 Gradient_max =  0.02063477273352226 learning rate ratio =  0.00040415811712922657\n",
      "Epoch =  419 Batch =  0 Loss =  13.865516164244708 Gradient_max =  0.020660419176843844 learning rate ratio =  0.0004151523244681651\n",
      "Epoch =  420 Batch =  0 Loss =  13.865514253899534 Gradient_max =  0.020686097563052903 learning rate ratio =  0.0004267469066324346\n",
      "Epoch =  421 Batch =  0 Loss =  13.865512344596649 Gradient_max =  0.020711807931874184 learning rate ratio =  0.00043899241206519726\n",
      "Epoch =  422 Batch =  0 Loss =  13.86551043633437 Gradient_max =  0.02073755032309209 learning rate ratio =  0.0004519452276195476\n",
      "Epoch =  423 Batch =  0 Loss =  13.865508529145636 Gradient_max =  0.02076332477657992 learning rate ratio =  0.0004656684465524583\n",
      "Epoch =  424 Batch =  0 Loss =  13.865506623011797 Gradient_max =  0.020789131332201928 learning rate ratio =  0.0004802328961148197\n",
      "Epoch =  425 Batch =  0 Loss =  13.865504717906717 Gradient_max =  0.020814970029862885 learning rate ratio =  0.0004957183600610295\n",
      "Epoch =  426 Batch =  0 Loss =  13.865502813857828 Gradient_max =  0.020840840909579213 learning rate ratio =  0.0005122150406192272\n",
      "Epoch =  427 Batch =  0 Loss =  13.865500910855864 Gradient_max =  0.020866744011374443 learning rate ratio =  0.0005298253164866734\n",
      "Epoch =  428 Batch =  0 Loss =  13.865499008911373 Gradient_max =  0.020892679375345412 learning rate ratio =  0.0005486658692312217\n",
      "Epoch =  429 Batch =  0 Loss =  13.865497108024986 Gradient_max =  0.020918647041627162 learning rate ratio =  0.0005688702714222712\n",
      "Epoch =  430 Batch =  0 Loss =  13.865495208202342 Gradient_max =  0.020944647050409354 learning rate ratio =  0.0005905921578580587\n",
      "Epoch =  431 Batch =  0 Loss =  13.865493309439092 Gradient_max =  0.020970679441922425 learning rate ratio =  0.0006140091391305794\n",
      "Epoch =  432 Batch =  0 Loss =  13.865491411735885 Gradient_max =  0.02099674425645156 learning rate ratio =  0.0006393276684930468\n",
      "Epoch =  433 Batch =  0 Loss =  13.865489515093385 Gradient_max =  0.021022841534332152 learning rate ratio =  0.0006667891444085741\n",
      "Epoch =  434 Batch =  0 Loss =  13.865487619556442 Gradient_max =  0.02104897131599872 learning rate ratio =  0.0006966776309802683\n",
      "Epoch =  435 Batch =  0 Loss =  13.865485725081566 Gradient_max =  0.021075133641838266 learning rate ratio =  0.0007293297198016797\n",
      "Epoch =  436 Batch =  0 Loss =  13.865483831669428 Gradient_max =  0.021101328552337113 learning rate ratio =  0.0007651472597929766\n",
      "Epoch =  437 Batch =  0 Loss =  13.865481939306909 Gradient_max =  0.021127556088016013 learning rate ratio =  0.0008046139776365806\n",
      "Epoch =  438 Batch =  0 Loss =  13.865480048012017 Gradient_max =  0.021153816289482225 learning rate ratio =  0.0008483174505393572\n",
      "Epoch =  439 Batch =  0 Loss =  13.865478157788646 Gradient_max =  0.021180109197376167 learning rate ratio =  0.0008969785562590943\n",
      "Epoch =  440 Batch =  0 Loss =  13.86547626863862 Gradient_max =  0.021206434852386335 learning rate ratio =  0.0009514915474635519\n",
      "Epoch =  441 Batch =  0 Loss =  13.865474380554808 Gradient_max =  0.021232793295244853 learning rate ratio =  0.001012979508094585\n",
      "Epoch =  442 Batch =  0 Loss =  13.865472493537922 Gradient_max =  0.02125918456674148 learning rate ratio =  0.0010828725507206114\n",
      "Epoch =  443 Batch =  0 Loss =  13.865470607588664 Gradient_max =  0.02128560870771678 learning rate ratio =  0.0011630204326169228\n",
      "Epoch =  444 Batch =  0 Loss =  13.865468722691984 Gradient_max =  0.02131206575904421 learning rate ratio =  0.001255858664177705\n",
      "Epoch =  445 Batch =  0 Loss =  13.8654668388553 Gradient_max =  0.02133855576167256 learning rate ratio =  0.0013646603016814534\n",
      "Epoch =  446 Batch =  0 Loss =  13.865464956088426 Gradient_max =  0.02136507875660681 learning rate ratio =  0.001493929839954362\n",
      "Epoch =  447 Batch =  0 Loss =  13.86546307439209 Gradient_max =  0.021391634784891388 learning rate ratio =  0.0016500424789511543\n",
      "Epoch =  448 Batch =  0 Loss =  13.865461193767025 Gradient_max =  0.021418223887621844 learning rate ratio =  0.0018423277548507667\n",
      "Epoch =  449 Batch =  0 Loss =  13.865459314213963 Gradient_max =  0.021444846105944892 learning rate ratio =  0.002085005121326012\n",
      "Epoch =  450 Batch =  0 Loss =  13.865457435727793 Gradient_max =  0.02147150148105037 learning rate ratio =  0.0024008707519287144\n",
      "Epoch =  451 Batch =  0 Loss =  13.86545555830226 Gradient_max =  0.021498190054180806 learning rate ratio =  0.002828912287144607\n",
      "Epoch =  452 Batch =  0 Loss =  13.865453681950997 Gradient_max =  0.021524911866651793 learning rate ratio =  0.003441787986249141\n",
      "Epoch =  453 Batch =  0 Loss =  13.86545180667476 Gradient_max =  0.02155166695981544 learning rate ratio =  0.004392183903317663\n",
      "Epoch =  454 Batch =  0 Loss =  13.865449932474299 Gradient_max =  0.021578455375075335 learning rate ratio =  0.006064862522645798\n",
      "Epoch =  455 Batch =  0 Loss =  13.865448059367086 Gradient_max =  0.021605277153905568 learning rate ratio =  0.00978783322854674\n",
      "Epoch =  456 Batch =  0 Loss =  13.86544618733721 Gradient_max =  0.021632132337794048 learning rate ratio =  0.025298869416429223\n",
      "Epoch =  457 Batch =  0 Loss =  13.86544431638544 Gradient_max =  0.02165902096829927 learning rate ratio =  0.00025947498130732624\n",
      "Epoch =  458 Batch =  0 Loss =  13.865442446498923 Gradient_max =  0.021685943087015755 learning rate ratio =  0.000264612496493659\n",
      "Epoch =  459 Batch =  0 Loss =  13.8654405776921 Gradient_max =  0.021712898735621346 learning rate ratio =  0.00026995097719097834\n",
      "Epoch =  460 Batch =  0 Loss =  13.86543870996576 Gradient_max =  0.021739887955830015 learning rate ratio =  0.00027550244588672474\n",
      "Epoch =  461 Batch =  0 Loss =  13.865436843320701 Gradient_max =  0.021766910789407692 learning rate ratio =  0.00028127990355227163\n",
      "Epoch =  462 Batch =  0 Loss =  13.86543497775772 Gradient_max =  0.021793967278172302 learning rate ratio =  0.00028729743125627895\n",
      "Epoch =  463 Batch =  0 Loss =  13.865433113277629 Gradient_max =  0.02182105746399388 learning rate ratio =  0.0002935703047094116\n",
      "Epoch =  464 Batch =  0 Loss =  13.865431249881233 Gradient_max =  0.021848181388794602 learning rate ratio =  0.0003001151237019463\n",
      "Epoch =  465 Batch =  0 Loss =  13.865429387555846 Gradient_max =  0.021875339094533257 learning rate ratio =  0.00030694995874318737\n",
      "Epoch =  466 Batch =  0 Loss =  13.86542752631581 Gradient_max =  0.02190253062325213 learning rate ratio =  0.0003140945176311652\n",
      "Epoch =  467 Batch =  0 Loss =  13.86542566615074 Gradient_max =  0.021929756017017053 learning rate ratio =  0.0003215703351838903\n",
      "Epoch =  468 Batch =  0 Loss =  13.865423807072697 Gradient_max =  0.021957015317972624 learning rate ratio =  0.000329400989981724\n",
      "Epoch =  469 Batch =  0 Loss =  13.865421949082519 Gradient_max =  0.02198430856830272 learning rate ratio =  0.00033761235271330625\n",
      "Epoch =  470 Batch =  0 Loss =  13.865420092181044 Gradient_max =  0.022011635810243797 learning rate ratio =  0.0003462328716372796\n",
      "Epoch =  471 Batch =  0 Loss =  13.865418236443462 Gradient_max =  0.0220389970861688 learning rate ratio =  0.0003552939017970052\n",
      "Epoch =  472 Batch =  0 Loss =  13.865416381782781 Gradient_max =  0.022066392438319895 learning rate ratio =  0.00036483008601000176\n",
      "Epoch =  473 Batch =  0 Loss =  13.865414528213444 Gradient_max =  0.022093821909107185 learning rate ratio =  0.0003748797974035236\n",
      "Epoch =  474 Batch =  0 Loss =  13.865412675736318 Gradient_max =  0.02212128554097794 learning rate ratio =  0.00038548565539826997\n",
      "Epoch =  475 Batch =  0 Loss =  13.865410824352265 Gradient_max =  0.022148783376432227 learning rate ratio =  0.0003966951297778411\n",
      "Epoch =  476 Batch =  0 Loss =  13.865408974062165 Gradient_max =  0.022176315458023083 learning rate ratio =  0.00040856125089827497\n",
      "Epoch =  477 Batch =  0 Loss =  13.865407124871796 Gradient_max =  0.02220388182836106 learning rate ratio =  0.00042114344845242175\n",
      "Epoch =  478 Batch =  0 Loss =  13.86540527679815 Gradient_max =  0.022231482530124493 learning rate ratio =  0.00043450854678169626\n",
      "Epoch =  479 Batch =  0 Loss =  13.86540342982111 Gradient_max =  0.022259117606001834 learning rate ratio =  0.00044873195191562267\n",
      "Epoch =  480 Batch =  0 Loss =  13.865401583941582 Gradient_max =  0.022286787098758497 learning rate ratio =  0.0004638990748611332\n",
      "Epoch =  481 Batch =  0 Loss =  13.865399739160452 Gradient_max =  0.022314491051213175 learning rate ratio =  0.00048010704787433147\n",
      "Epoch =  482 Batch =  0 Loss =  13.86539789547864 Gradient_max =  0.02234222950623783 learning rate ratio =  0.0004974668065792753\n",
      "Epoch =  483 Batch =  0 Loss =  13.865396052906805 Gradient_max =  0.022370002506768575 learning rate ratio =  0.0005161056322677611\n",
      "Epoch =  484 Batch =  0 Loss =  13.865394211436147 Gradient_max =  0.02239781009577359 learning rate ratio =  0.0005361702775748975\n",
      "Epoch =  485 Batch =  0 Loss =  13.865392371067589 Gradient_max =  0.022425652316285315 learning rate ratio =  0.0005578308379215968\n",
      "Epoch =  486 Batch =  0 Loss =  13.86539053184281 Gradient_max =  0.02245352921143582 learning rate ratio =  0.0005812855849092094\n",
      "Epoch =  487 Batch =  0 Loss =  13.865388693769216 Gradient_max =  0.02248144082437206 learning rate ratio =  0.0006067670525669947\n",
      "Epoch =  488 Batch =  0 Loss =  13.865386856800594 Gradient_max =  0.02250938719823405 learning rate ratio =  0.0006345497724390731\n",
      "Epoch =  489 Batch =  0 Loss =  13.865385020943199 Gradient_max =  0.02253736837627393 learning rate ratio =  0.0006649602032955214\n",
      "Epoch =  490 Batch =  0 Loss =  13.865383186197976 Gradient_max =  0.022565384401792725 learning rate ratio =  0.0006983896179562891\n",
      "Epoch =  491 Batch =  0 Loss =  13.86538135256055 Gradient_max =  0.022593435318140367 learning rate ratio =  0.0007353110283108447\n",
      "Epoch =  492 Batch =  0 Loss =  13.865379520031885 Gradient_max =  0.02262152116872581 learning rate ratio =  0.0007763017061852448\n",
      "Epoch =  493 Batch =  0 Loss =  13.865377688612947 Gradient_max =  0.02264964199701207 learning rate ratio =  0.0008220735844205135\n",
      "Epoch =  494 Batch =  0 Loss =  13.865375858309536 Gradient_max =  0.02267779784652069 learning rate ratio =  0.0008735149542829264\n",
      "Epoch =  495 Batch =  0 Loss =  13.865374029117794 Gradient_max =  0.022705988760818568 learning rate ratio =  0.000931748679525879\n",
      "Epoch =  496 Batch =  0 Loss =  13.865372201044039 Gradient_max =  0.022734214783537626 learning rate ratio =  0.0009982150993841374\n",
      "Epoch =  497 Batch =  0 Loss =  13.865370374083948 Gradient_max =  0.022762475958351503 learning rate ratio =  0.001074792765486407\n",
      "Epoch =  498 Batch =  0 Loss =  13.865368548247925 Gradient_max =  0.022790772329004415 learning rate ratio =  0.0011639788146615244\n",
      "Epoch =  499 Batch =  0 Loss =  13.865366723548213 Gradient_max =  0.022819103939298695 learning rate ratio =  0.001269166425915444\n",
      "Epoch =  500 Batch =  0 Loss =  13.865364899965188 Gradient_max =  0.022847470833054283 learning rate ratio =  0.001395086333284783\n",
      "Epoch =  501 Batch =  0 Loss =  13.865363077499868 Gradient_max =  0.022875873054169094 learning rate ratio =  0.0015485379373743984\n",
      "Epoch =  502 Batch =  0 Loss =  13.865361256153271 Gradient_max =  0.022904310646595746 learning rate ratio =  0.0017396588297773888\n",
      "Epoch =  503 Batch =  0 Loss =  13.865359435926411 Gradient_max =  0.022932783654341565 learning rate ratio =  0.0019842600390862024\n",
      "Epoch =  504 Batch =  0 Loss =  13.865357616841209 Gradient_max =  0.022961292121492338 learning rate ratio =  0.0023084405264895687\n",
      "Epoch =  505 Batch =  0 Loss =  13.865355798877827 Gradient_max =  0.022989836092141475 learning rate ratio =  0.0027585805003810834\n",
      "Epoch =  506 Batch =  0 Loss =  13.865353982060888 Gradient_max =  0.023018415610487713 learning rate ratio =  0.0034257919279559717\n",
      "Epoch =  507 Batch =  0 Loss =  13.865352166367876 Gradient_max =  0.023047030720731273 learning rate ratio =  0.004516979810659844\n",
      "Epoch =  508 Batch =  0 Loss =  13.865350351805333 Gradient_max =  0.023075681467159312 learning rate ratio =  0.006624453406689283\n",
      "Epoch =  509 Batch =  0 Loss =  13.865348538391359 Gradient_max =  0.02310436789412941 learning rate ratio =  0.012405382937841766\n",
      "Epoch =  510 Batch =  0 Loss =  13.865346726108639 Gradient_max =  0.023133090046012245 learning rate ratio =  0.09662099173515463\n",
      "Epoch =  511 Batch =  0 Loss =  13.865344914954086 Gradient_max =  0.023161847967251792 learning rate ratio =  0.0003208690439385673\n",
      "Epoch =  512 Batch =  0 Loss =  13.865343104928769 Gradient_max =  0.02319064170235109 learning rate ratio =  0.00033045651122122076\n",
      "Epoch =  513 Batch =  0 Loss =  13.865341296033773 Gradient_max =  0.023219471295868658 learning rate ratio =  0.0003406222212852542\n",
      "Epoch =  514 Batch =  0 Loss =  13.865339488270179 Gradient_max =  0.02324833679241846 learning rate ratio =  0.0003514201037602739\n",
      "Epoch =  515 Batch =  0 Loss =  13.865337681639087 Gradient_max =  0.02327723823667003 learning rate ratio =  0.00036291100976081693\n",
      "Epoch =  516 Batch =  0 Loss =  13.865335876141595 Gradient_max =  0.023306175673348542 learning rate ratio =  0.0003751638590998163\n",
      "Epoch =  517 Batch =  0 Loss =  13.865334071778799 Gradient_max =  0.023335149147234866 learning rate ratio =  0.00038825702350154737\n",
      "Epoch =  518 Batch =  0 Loss =  13.86533226855182 Gradient_max =  0.023364158703165633 learning rate ratio =  0.00040228000446513896\n",
      "Epoch =  519 Batch =  0 Loss =  13.865330466461767 Gradient_max =  0.023393204386033324 learning rate ratio =  0.00041733548170132706\n",
      "Epoch =  520 Batch =  0 Loss =  13.865328665509761 Gradient_max =  0.023422286240786315 learning rate ratio =  0.0004335418312772933\n",
      "Epoch =  521 Batch =  0 Loss =  13.865326865696932 Gradient_max =  0.02345140431242898 learning rate ratio =  0.00045103624411768736\n",
      "Epoch =  522 Batch =  0 Loss =  13.865325067024422 Gradient_max =  0.02348055864602171 learning rate ratio =  0.00046997861875925403\n",
      "Epoch =  523 Batch =  0 Loss =  13.865323269516693 Gradient_max =  0.023509749286706363 learning rate ratio =  0.000490556462308229\n",
      "Epoch =  524 Batch =  0 Loss =  13.86532147313997 Gradient_max =  0.023538976279616736 learning rate ratio =  0.0005129911179802509\n",
      "Epoch =  525 Batch =  0 Loss =  13.865319677907019 Gradient_max =  0.023568239669995425 learning rate ratio =  0.0005375457579434855\n",
      "Epoch =  526 Batch =  0 Loss =  13.865317883819003 Gradient_max =  0.023597539503127724 learning rate ratio =  0.0005645357542139988\n",
      "Epoch =  527 Batch =  0 Loss =  13.865316090881251 Gradient_max =  0.023626875824359087 learning rate ratio =  0.0005943422961199304\n",
      "Epoch =  528 Batch =  0 Loss =  13.865314299090763 Gradient_max =  0.023656248679083905 learning rate ratio =  0.0006274305053157493\n",
      "Epoch =  529 Batch =  0 Loss =  13.865312508453997 Gradient_max =  0.023685658112761774 learning rate ratio =  0.0006643738823164261\n",
      "Epoch =  530 Batch =  0 Loss =  13.865310718966855 Gradient_max =  0.023715104170899087 learning rate ratio =  0.000705887826060605\n",
      "Epoch =  531 Batch =  0 Loss =  13.865308930604145 Gradient_max =  0.02374458689903324 learning rate ratio =  0.0007528764140987887\n",
      "Epoch =  532 Batch =  0 Loss =  13.865307143393473 Gradient_max =  0.02377410634281958 learning rate ratio =  0.0008064989957892123\n",
      "Epoch =  533 Batch =  0 Loss =  13.865305357340288 Gradient_max =  0.023803662547943426 learning rate ratio =  0.0008682671321235992\n",
      "Epoch =  534 Batch =  0 Loss =  13.86530357244155 Gradient_max =  0.023833255560139446 learning rate ratio =  0.0009401893421170556\n",
      "Epoch =  535 Batch =  0 Loss =  13.865301788721334 Gradient_max =  0.023862885425228866 learning rate ratio =  0.0010249936244810947\n",
      "Epoch =  536 Batch =  0 Loss =  13.86530000615801 Gradient_max =  0.023892552189038303 learning rate ratio =  0.0011264813056022977\n",
      "Epoch =  537 Batch =  0 Loss =  13.865298224752811 Gradient_max =  0.02392225589747728 learning rate ratio =  0.001250112501113407\n",
      "Epoch =  538 Batch =  0 Loss =  13.865296444506958 Gradient_max =  0.02395199659651248 learning rate ratio =  0.0014040217208269158\n",
      "Epoch =  539 Batch =  0 Loss =  13.865294665421704 Gradient_max =  0.023981774332167877 learning rate ratio =  0.0016008837663457982\n",
      "Epoch =  540 Batch =  0 Loss =  13.865292887498281 Gradient_max =  0.024011589150524724 learning rate ratio =  0.001861595143215105\n",
      "Epoch =  541 Batch =  0 Loss =  13.865291110737948 Gradient_max =  0.024041441097721685 learning rate ratio =  0.002223230792342252\n",
      "Epoch =  542 Batch =  0 Loss =  13.865289335148361 Gradient_max =  0.024071330219962443 learning rate ratio =  0.0027584589498919274\n",
      "Epoch =  543 Batch =  0 Loss =  13.865287560728513 Gradient_max =  0.024101256563496737 learning rate ratio =  0.003631743896138662\n",
      "Epoch =  544 Batch =  0 Loss =  13.865285787497712 Gradient_max =  0.02413122017465721 learning rate ratio =  0.005311198455466778\n",
      "Epoch =  545 Batch =  0 Loss =  13.865284015435096 Gradient_max =  0.024161221099787716 learning rate ratio =  0.009870111462464597\n",
      "Epoch =  546 Batch =  0 Loss =  13.865282244541959 Gradient_max =  0.02419125938531493 learning rate ratio =  0.06918800012284312\n",
      "Epoch =  547 Batch =  0 Loss =  13.865280474819583 Gradient_max =  0.02422133507772331 learning rate ratio =  0.0005027627259009853\n",
      "Epoch =  548 Batch =  0 Loss =  13.865278706342908 Gradient_max =  0.024251448223638596 learning rate ratio =  0.0005188307635741583\n",
      "Epoch =  549 Batch =  0 Loss =  13.865276939039644 Gradient_max =  0.024281598869577786 learning rate ratio =  0.0005359376666396614\n",
      "Epoch =  550 Batch =  0 Loss =  13.865275172895862 Gradient_max =  0.024311787062181792 learning rate ratio =  0.0005541875381619633\n",
      "Epoch =  551 Batch =  0 Loss =  13.865273407952555 Gradient_max =  0.024342012848212205 learning rate ratio =  0.0005736988712157443\n",
      "Epoch =  552 Batch =  0 Loss =  13.865271644211065 Gradient_max =  0.024372276274443723 learning rate ratio =  0.0005946071242515383\n",
      "Epoch =  553 Batch =  0 Loss =  13.865269881641854 Gradient_max =  0.02440257738767271 learning rate ratio =  0.0006170678701088714\n",
      "Epoch =  554 Batch =  0 Loss =  13.865268120252733 Gradient_max =  0.024432916234799245 learning rate ratio =  0.0006412606734865382\n",
      "Epoch =  555 Batch =  0 Loss =  13.865266360045037 Gradient_max =  0.02446329286277294 learning rate ratio =  0.0006673939013080466\n",
      "Epoch =  556 Batch =  0 Loss =  13.865264600996316 Gradient_max =  0.024493707318574053 learning rate ratio =  0.0006957107388190018\n",
      "Epoch =  557 Batch =  0 Loss =  13.86526284315427 Gradient_max =  0.024524159649322527 learning rate ratio =  0.0007264967793996517\n",
      "Epoch =  558 Batch =  0 Loss =  13.865261086524791 Gradient_max =  0.02455464990214772 learning rate ratio =  0.0007600896901580777\n",
      "Epoch =  559 Batch =  0 Loss =  13.865259331089167 Gradient_max =  0.02458517812421217 learning rate ratio =  0.0007968916473851679\n",
      "Epoch =  560 Batch =  0 Loss =  13.865257576841858 Gradient_max =  0.02461574436275037 learning rate ratio =  0.0008373855143225234\n",
      "Epoch =  561 Batch =  0 Loss =  13.865255823784242 Gradient_max =  0.024646348665063825 learning rate ratio =  0.0008821561445191321\n",
      "Epoch =  562 Batch =  0 Loss =  13.865254071917715 Gradient_max =  0.02467699107851293 learning rate ratio =  0.0009319188110043066\n",
      "Epoch =  563 Batch =  0 Loss =  13.865252321243672 Gradient_max =  0.024707671650517085 learning rate ratio =  0.0009875577062459632\n",
      "Epoch =  564 Batch =  0 Loss =  13.86525057176351 Gradient_max =  0.02473839042855471 learning rate ratio =  0.001050178936107882\n",
      "Epoch =  565 Batch =  0 Loss =  13.86524882347865 Gradient_max =  0.0247691474601634 learning rate ratio =  0.0011211847999213652\n",
      "Epoch =  566 Batch =  0 Loss =  13.865247076390506 Gradient_max =  0.024799942792939875 learning rate ratio =  0.001202380047368388\n",
      "Epoch =  567 Batch =  0 Loss =  13.865245330500496 Gradient_max =  0.024830776474540198 learning rate ratio =  0.0012961274134880487\n",
      "Epoch =  568 Batch =  0 Loss =  13.865243585810054 Gradient_max =  0.02486164855267976 learning rate ratio =  0.001405581327604629\n",
      "Epoch =  569 Batch =  0 Loss =  13.865241842370837 Gradient_max =  0.024892559075190147 learning rate ratio =  0.0015350498269906551\n",
      "Epoch =  570 Batch =  0 Loss =  13.865240100134097 Gradient_max =  0.024923508089848886 learning rate ratio =  0.0016905749818602659\n",
      "Epoch =  571 Batch =  0 Loss =  13.865238359101278 Gradient_max =  0.02495449564454985 learning rate ratio =  0.001880902971512086\n",
      "Epoch =  572 Batch =  0 Loss =  13.865236619282696 Gradient_max =  0.024985521787257003 learning rate ratio =  0.002119187435817704\n",
      "Epoch =  573 Batch =  0 Loss =  13.86523488067098 Gradient_max =  0.02501658656597312 learning rate ratio =  0.0024261659715321417\n",
      "Epoch =  574 Batch =  0 Loss =  13.865233143285105 Gradient_max =  0.025047690028791966 learning rate ratio =  0.0028365465005731234\n",
      "Epoch =  575 Batch =  0 Loss =  13.86523140713276 Gradient_max =  0.02507883222385254 learning rate ratio =  0.0034131541103690497\n",
      "Epoch =  576 Batch =  0 Loss =  13.865229672178152 Gradient_max =  0.0251100131993051 learning rate ratio =  0.004282634364492101\n",
      "Epoch =  577 Batch =  0 Loss =  13.865227938436425 Gradient_max =  0.02514123300341827 learning rate ratio =  0.005744079714365183\n",
      "Epoch =  578 Batch =  0 Loss =  13.865226205909078 Gradient_max =  0.02517249168450497 learning rate ratio =  0.008714027725795588\n",
      "Epoch =  579 Batch =  0 Loss =  13.865224474584403 Gradient_max =  0.025203789290922974 learning rate ratio =  0.018019104103372315\n",
      "Epoch =  580 Batch =  0 Loss =  13.865222744477151 Gradient_max =  0.025235125871120923 learning rate ratio =  0.0013379309729257655\n",
      "Epoch =  581 Batch =  0 Loss =  13.865221015616898 Gradient_max =  0.025266501473623235 learning rate ratio =  0.0014620342278608945\n",
      "Epoch =  582 Batch =  0 Loss =  13.865219287977123 Gradient_max =  0.02529791614695319 learning rate ratio =  0.0016113095374493311\n",
      "Epoch =  583 Batch =  0 Loss =  13.865217561559362 Gradient_max =  0.02532936993972526 learning rate ratio =  0.001794279642885366\n",
      "Epoch =  584 Batch =  0 Loss =  13.86521583636892 Gradient_max =  0.025360862900617728 learning rate ratio =  0.0020238042553749423\n",
      "Epoch =  585 Batch =  0 Loss =  13.865214112436558 Gradient_max =  0.025392395078398286 learning rate ratio =  0.0023202408601011764\n",
      "Epoch =  586 Batch =  0 Loss =  13.865212389755447 Gradient_max =  0.025423966521855718 learning rate ratio =  0.002717841535098574\n",
      "Epoch =  587 Batch =  0 Loss =  13.86521066832509 Gradient_max =  0.02545557727984482 learning rate ratio =  0.003279046185629064\n",
      "Epoch =  588 Batch =  0 Loss =  13.865208948124542 Gradient_max =  0.025487227401258108 learning rate ratio =  0.00413099008240733\n",
      "Epoch =  589 Batch =  0 Loss =  13.865207229205197 Gradient_max =  0.025518916935130788 learning rate ratio =  0.005578585945030019\n",
      "Epoch =  590 Batch =  0 Loss =  13.86520551151885 Gradient_max =  0.025550645930446376 learning rate ratio =  0.008582279324459445\n",
      "Epoch =  591 Batch =  0 Loss =  13.8652037950671 Gradient_max =  0.0255824144363058 learning rate ratio =  0.018566855071013678\n",
      "Epoch =  592 Batch =  0 Loss =  13.86520207985154 Gradient_max =  0.0256142225018711 learning rate ratio =  0.016637019319879133\n",
      "Epoch =  593 Batch =  0 Loss =  13.865200365903618 Gradient_max =  0.025646070176398132 learning rate ratio =  8.994257503992637\n",
      "Epoch =  594 Batch =  0 Loss =  13.865198653195113 Gradient_max =  0.02567795750913893 learning rate ratio =  0.0026039244256685695\n",
      "Epoch =  595 Batch =  0 Loss =  13.865196941704978 Gradient_max =  0.025709884549413704 learning rate ratio =  0.0030821652782801797\n",
      "Epoch =  596 Batch =  0 Loss =  13.865195230932725 Gradient_max =  0.02574185134610779 learning rate ratio =  0.0037745598874391398\n",
      "Epoch =  597 Batch =  0 Loss =  13.865193521058671 Gradient_max =  0.02577385794887652 learning rate ratio =  0.004866450403753967\n",
      "Epoch =  598 Batch =  0 Loss =  13.865191812436361 Gradient_max =  0.025805904407617184 learning rate ratio =  0.006843772073611928\n",
      "Epoch =  599 Batch =  0 Loss =  13.865190105067436 Gradient_max =  0.02583799077192196 learning rate ratio =  0.011518021820551122\n",
      "Epoch =  600 Batch =  0 Loss =  13.865188398974787 Gradient_max =  0.025870117091469955 learning rate ratio =  0.03623826548418921\n",
      "Epoch =  601 Batch =  0 Loss =  13.865186694151264 Gradient_max =  0.025902283415966103 learning rate ratio =  0.00021743317294050404\n",
      "Epoch =  602 Batch =  0 Loss =  13.865184990580582 Gradient_max =  0.0259344897951687 learning rate ratio =  0.0002208068474081774\n",
      "Epoch =  603 Batch =  0 Loss =  13.865183288264415 Gradient_max =  0.02596673627891742 learning rate ratio =  0.00022428273504646802\n",
      "Epoch =  604 Batch =  0 Loss =  13.865181587213204 Gradient_max =  0.025999022917124387 learning rate ratio =  0.00022786554902169437\n",
      "Epoch =  605 Batch =  0 Loss =  13.86517988741992 Gradient_max =  0.02603134975974313 learning rate ratio =  0.00023156029679810317\n",
      "Epoch =  606 Batch =  0 Loss =  13.86517818888628 Gradient_max =  0.026063716856799767 learning rate ratio =  0.00023537230347170926\n",
      "Epoch =  607 Batch =  0 Loss =  13.86517649161398 Gradient_max =  0.02609612425838272 learning rate ratio =  0.0002393072373613374\n",
      "Epoch =  608 Batch =  0 Loss =  13.865174795604755 Gradient_max =  0.026128572014642745 learning rate ratio =  0.00024337113811402568\n",
      "Epoch =  609 Batch =  0 Loss =  13.865173100860327 Gradient_max =  0.026161060175793067 learning rate ratio =  0.0002475704476181635\n",
      "Epoch =  610 Batch =  0 Loss =  13.865171407408484 Gradient_max =  0.02619358879213892 learning rate ratio =  0.0002519120440560514\n",
      "Epoch =  611 Batch =  0 Loss =  13.865169715253598 Gradient_max =  0.02622615791402176 learning rate ratio =  0.00025640327947871403\n",
      "Epoch =  612 Batch =  0 Loss =  13.865168024368764 Gradient_max =  0.026258767591810103 learning rate ratio =  0.00026105202132999214\n",
      "Epoch =  613 Batch =  0 Loss =  13.865166334755742 Gradient_max =  0.026291417875967733 learning rate ratio =  0.00026586669841202403\n",
      "Epoch =  614 Batch =  0 Loss =  13.865164646430955 Gradient_max =  0.026324108817037346 learning rate ratio =  0.00027085635186290484\n",
      "Epoch =  615 Batch =  0 Loss =  13.865162959385895 Gradient_max =  0.0263568404655964 learning rate ratio =  0.0002760306917931995\n",
      "Epoch =  616 Batch =  0 Loss =  13.865161273617986 Gradient_max =  0.02638961287229352 learning rate ratio =  0.0002814001603239895\n",
      "Epoch =  617 Batch =  0 Loss =  13.865159589144396 Gradient_max =  0.026422426087862556 learning rate ratio =  0.00028697600189085904\n",
      "Epoch =  618 Batch =  0 Loss =  13.86515790594502 Gradient_max =  0.02645528016305573 learning rate ratio =  0.0002927703418124009\n",
      "Epoch =  619 Batch =  0 Loss =  13.865156224028242 Gradient_max =  0.026488175148723488 learning rate ratio =  0.00029879627427014223\n",
      "Epoch =  620 Batch =  0 Loss =  13.865154543403861 Gradient_max =  0.026521111095780613 learning rate ratio =  0.0003050679610524509\n",
      "Epoch =  621 Batch =  0 Loss =  13.865152864093249 Gradient_max =  0.026554088055217504 learning rate ratio =  0.0003116007426243764\n",
      "Epoch =  622 Batch =  0 Loss =  13.865151186070731 Gradient_max =  0.026587106078035258 learning rate ratio =  0.0003184112633621908\n",
      "Epoch =  623 Batch =  0 Loss =  13.865149509338131 Gradient_max =  0.026620165215329606 learning rate ratio =  0.00032551761309455114\n",
      "Epoch =  624 Batch =  0 Loss =  13.865147833907072 Gradient_max =  0.026653265518268895 learning rate ratio =  0.0003329394874957076\n",
      "Epoch =  625 Batch =  0 Loss =  13.865146159795927 Gradient_max =  0.026686407038096962 learning rate ratio =  0.00034069837031956333\n",
      "Epoch =  626 Batch =  0 Loss =  13.865144486980272 Gradient_max =  0.026719589826070894 learning rate ratio =  0.0003488177410296456\n",
      "Epoch =  627 Batch =  0 Loss =  13.865142815488829 Gradient_max =  0.02675281393357182 learning rate ratio =  0.00035732331203943123\n",
      "Epoch =  628 Batch =  0 Loss =  13.86514114529663 Gradient_max =  0.02678607941198402 learning rate ratio =  0.00036624330063212215\n",
      "Epoch =  629 Batch =  0 Loss =  13.865139476405552 Gradient_max =  0.02681938631278615 learning rate ratio =  0.0003756087415960905\n",
      "Epoch =  630 Batch =  0 Loss =  13.865137808817504 Gradient_max =  0.02685273468752101 learning rate ratio =  0.00038545384789086295\n",
      "Epoch =  631 Batch =  0 Loss =  13.86513614255516 Gradient_max =  0.026886124587817983 learning rate ratio =  0.0003958164281675969\n",
      "Epoch =  632 Batch =  0 Loss =  13.865134477599684 Gradient_max =  0.02691955606532593 learning rate ratio =  0.00040673837189182996\n",
      "Epoch =  633 Batch =  0 Loss =  13.865132813952988 Gradient_max =  0.026953029171780392 learning rate ratio =  0.0004182662151769252\n",
      "Epoch =  634 Batch =  0 Loss =  13.865131151643372 Gradient_max =  0.026986543959011237 learning rate ratio =  0.0004304518034557365\n",
      "Epoch =  635 Batch =  0 Loss =  13.865129490646405 Gradient_max =  0.027020100478853126 learning rate ratio =  0.0004433530708945728\n",
      "Epoch =  636 Batch =  0 Loss =  13.865127830964044 Gradient_max =  0.027053698783235124 learning rate ratio =  0.0004570349612505218\n",
      "Epoch =  637 Batch =  0 Loss =  13.865126172598233 Gradient_max =  0.027087338924150967 learning rate ratio =  0.00047157052107440415\n",
      "Epoch =  638 Batch =  0 Loss =  13.865124515560016 Gradient_max =  0.027121020953669913 learning rate ratio =  0.0004870422040904787\n",
      "Epoch =  639 Batch =  0 Loss =  13.865122859842305 Gradient_max =  0.02715474492390453 learning rate ratio =  0.0005035434359347547\n",
      "Epoch =  640 Batch =  0 Loss =  13.865121205471933 Gradient_max =  0.027188510887070222 learning rate ratio =  0.0005211805019581489\n",
      "Epoch =  641 Batch =  0 Loss =  13.865119552429793 Gradient_max =  0.027222318895396825 learning rate ratio =  0.0005400748387062482\n",
      "Epoch =  642 Batch =  0 Loss =  13.865117900735134 Gradient_max =  0.027256169001223674 learning rate ratio =  0.0005603658334743375\n",
      "Epoch =  643 Batch =  0 Loss =  13.865116250373287 Gradient_max =  0.027290061256913193 learning rate ratio =  0.0005822142684417567\n",
      "Epoch =  644 Batch =  0 Loss =  13.865114601342 Gradient_max =  0.02732399571491027 learning rate ratio =  0.0006058065894108674\n",
      "Epoch =  645 Batch =  0 Loss =  13.865112953643301 Gradient_max =  0.02735797242772884 learning rate ratio =  0.0006313602390776357\n",
      "Epoch =  646 Batch =  0 Loss =  13.86511130740362 Gradient_max =  0.027391991448109266 learning rate ratio =  0.0006591303779632269\n",
      "Epoch =  647 Batch =  0 Loss =  13.865109662606105 Gradient_max =  0.027426052828669174 learning rate ratio =  0.000689418433380704\n",
      "Epoch =  648 Batch =  0 Loss =  13.865108019147248 Gradient_max =  0.027460156621985095 learning rate ratio =  0.0007225830839834368\n",
      "Epoch =  649 Batch =  0 Loss =  13.865106377029116 Gradient_max =  0.027494302880833016 learning rate ratio =  0.0007590545298185055\n",
      "Epoch =  650 Batch =  0 Loss =  13.865104736283858 Gradient_max =  0.027528491658088648 learning rate ratio =  0.000799353254569486\n",
      "Epoch =  651 Batch =  0 Loss =  13.865103096886545 Gradient_max =  0.02756272300662797 learning rate ratio =  0.000844115021129045\n",
      "Epoch =  652 Batch =  0 Loss =  13.865101458836167 Gradient_max =  0.02759699697942196 learning rate ratio =  0.0008941246581235883\n",
      "Epoch =  653 Batch =  0 Loss =  13.865099822134809 Gradient_max =  0.027631313629509966 learning rate ratio =  0.0009503624691159652\n",
      "Epoch =  654 Batch =  0 Loss =  13.86509818678894 Gradient_max =  0.027665673010001144 learning rate ratio =  0.0010140691316086702\n",
      "Epoch =  655 Batch =  0 Loss =  13.865096552796292 Gradient_max =  0.027700075174063286 learning rate ratio =  0.001086838291344784\n",
      "Epoch =  656 Batch =  0 Loss =  13.865094920428131 Gradient_max =  0.0277345201752145 learning rate ratio =  0.001170751696239763\n",
      "Epoch =  657 Batch =  0 Loss =  13.865093289902761 Gradient_max =  0.027769008066982494 learning rate ratio =  0.0012685815616778522\n",
      "Epoch =  658 Batch =  0 Loss =  13.865091660794814 Gradient_max =  0.027803538902298794 learning rate ratio =  0.0013841027188288089\n",
      "Epoch =  659 Batch =  0 Loss =  13.865090033083433 Gradient_max =  0.02783811273457431 learning rate ratio =  0.0015225909355116969\n",
      "Epoch =  660 Batch =  0 Loss =  13.865088406738117 Gradient_max =  0.027872729617275563 learning rate ratio =  0.001691651253934501\n",
      "Epoch =  661 Batch =  0 Loss =  13.865086781761022 Gradient_max =  0.02790738960397257 learning rate ratio =  0.0019026629618803912\n",
      "Epoch =  662 Batch =  0 Loss =  13.865085158154317 Gradient_max =  0.02794209274830208 learning rate ratio =  0.0021734525963070946\n",
      "Epoch =  663 Batch =  0 Loss =  13.865083535912316 Gradient_max =  0.027976839103957783 learning rate ratio =  0.0025336135141494\n",
      "Epoch =  664 Batch =  0 Loss =  13.865081915045085 Gradient_max =  0.02801162872471993 learning rate ratio =  0.0030361333373384336\n",
      "Epoch =  665 Batch =  0 Loss =  13.865080295562521 Gradient_max =  0.02804646166443219 learning rate ratio =  0.003786205095118256\n",
      "Epoch =  666 Batch =  0 Loss =  13.86507867745911 Gradient_max =  0.028081337976992634 learning rate ratio =  0.005026532027990657\n",
      "Epoch =  667 Batch =  0 Loss =  13.865077060737073 Gradient_max =  0.02811625771637278 learning rate ratio =  0.007471100993447453\n",
      "Epoch =  668 Batch =  0 Loss =  13.865075445398622 Gradient_max =  0.028151220936611283 learning rate ratio =  0.014528357639411594\n",
      "Epoch =  669 Batch =  0 Loss =  13.865073831471099 Gradient_max =  0.028186227691842633 learning rate ratio =  0.2570881695522057\n",
      "Epoch =  670 Batch =  0 Loss =  13.865072219706681 Gradient_max =  0.028221278036642827 learning rate ratio =  0.0004286222199744225\n",
      "Epoch =  671 Batch =  0 Loss =  13.86507061062324 Gradient_max =  0.02825637202555648 learning rate ratio =  0.0004424590624494822\n",
      "Epoch =  672 Batch =  0 Loss =  13.86506900297144 Gradient_max =  0.02829150971219695 learning rate ratio =  0.000457200815931404\n",
      "Epoch =  673 Batch =  0 Loss =  13.86506739671777 Gradient_max =  0.028326691150902222 learning rate ratio =  0.0004729392373434705\n",
      "Epoch =  674 Batch =  0 Loss =  13.865065791864513 Gradient_max =  0.028361916396116026 learning rate ratio =  0.0004897789229706045\n",
      "Epoch =  675 Batch =  0 Loss =  13.865064188418442 Gradient_max =  0.028397185502353633 learning rate ratio =  0.0005078396355991816\n",
      "Epoch =  676 Batch =  0 Loss =  13.86506258637736 Gradient_max =  0.028432498524190657 learning rate ratio =  0.0005272591568498192\n",
      "Epoch =  677 Batch =  0 Loss =  13.865060985743572 Gradient_max =  0.028467855516274374 learning rate ratio =  0.0005481968083883858\n",
      "Epoch =  678 Batch =  0 Loss =  13.865059386552986 Gradient_max =  0.028503256533357273 learning rate ratio =  0.0005708378324538403\n",
      "Epoch =  679 Batch =  0 Loss =  13.865057788774346 Gradient_max =  0.028538701630185806 learning rate ratio =  0.0005953988867310989\n",
      "Epoch =  680 Batch =  0 Loss =  13.865056192439267 Gradient_max =  0.028574190861643943 learning rate ratio =  0.0006221349989411002\n",
      "Epoch =  681 Batch =  0 Loss =  13.865054597520826 Gradient_max =  0.028609724282619476 learning rate ratio =  0.0006513484544958566\n",
      "Epoch =  682 Batch =  0 Loss =  13.865053004024288 Gradient_max =  0.028645301948103042 learning rate ratio =  0.0006834002745401605\n",
      "Epoch =  683 Batch =  0 Loss =  13.865051411949095 Gradient_max =  0.028680923913149314 learning rate ratio =  0.0007187252101635795\n",
      "Epoch =  684 Batch =  0 Loss =  13.865049821297621 Gradient_max =  0.028716590232883685 learning rate ratio =  0.0007578515770647051\n",
      "Epoch =  685 Batch =  0 Loss =  13.865048232072246 Gradient_max =  0.02875230096250023 learning rate ratio =  0.0008014278573639661\n",
      "Epoch =  686 Batch =  0 Loss =  13.86504664427536 Gradient_max =  0.028788056157261786 learning rate ratio =  0.0008502589245678024\n",
      "Epoch =  687 Batch =  0 Loss =  13.865045057909361 Gradient_max =  0.028823855872499908 learning rate ratio =  0.0009053562136064279\n",
      "Epoch =  688 Batch =  0 Loss =  13.865043472976662 Gradient_max =  0.02885970016361516 learning rate ratio =  0.0009680085282952534\n",
      "Epoch =  689 Batch =  0 Loss =  13.865041889525342 Gradient_max =  0.028895589086128083 learning rate ratio =  0.0010398841192664169\n",
      "Epoch =  690 Batch =  0 Loss =  13.86504030750643 Gradient_max =  0.028931522695518637 learning rate ratio =  0.0011231814239818874\n",
      "Epoch =  691 Batch =  0 Loss =  13.865038726928146 Gradient_max =  0.02896750104740216 learning rate ratio =  0.0012208578690450238\n",
      "Epoch =  692 Batch =  0 Loss =  13.865037147792941 Gradient_max =  0.02900352419745556 learning rate ratio =  0.0013369883528800645\n",
      "Epoch =  693 Batch =  0 Loss =  13.865035570101462 Gradient_max =  0.029039592201420942 learning rate ratio =  0.001477348104173165\n",
      "Epoch =  694 Batch =  0 Loss =  13.865033993862351 Gradient_max =  0.02907570511512188 learning rate ratio =  0.0016504028424431164\n",
      "Epoch =  695 Batch =  0 Loss =  13.865032419106551 Gradient_max =  0.029111862994476205 learning rate ratio =  0.0018690820731368345\n",
      "Epoch =  696 Batch =  0 Loss =  13.865030845803735 Gradient_max =  0.029148065895402416 learning rate ratio =  0.002154167863149693\n",
      "Epoch =  697 Batch =  0 Loss =  13.8650292739564 Gradient_max =  0.029184313873924974 learning rate ratio =  0.0025413234598067403\n",
      "Epoch =  698 Batch =  0 Loss =  13.86502770360309 Gradient_max =  0.029220606986177663 learning rate ratio =  0.0030973173921533988\n",
      "Epoch =  699 Batch =  0 Loss =  13.865026134741338 Gradient_max =  0.029256945288319187 learning rate ratio =  0.003963392293653347\n",
      "Epoch =  700 Batch =  0 Loss =  13.865024567342644 Gradient_max =  0.029293328836549355 learning rate ratio =  0.005499252230606427\n",
      "Epoch =  701 Batch =  0 Loss =  13.86502300141278 Gradient_max =  0.029329757687174763 learning rate ratio =  0.00897172279073695\n",
      "Epoch =  702 Batch =  0 Loss =  13.8650214369804 Gradient_max =  0.029366231896599263 learning rate ratio =  0.024292926645801762\n",
      "Epoch =  703 Batch =  0 Loss =  13.865019874018722 Gradient_max =  0.029402751521234866 learning rate ratio =  0.000290613387925835\n",
      "Epoch =  704 Batch =  0 Loss =  13.865018312557611 Gradient_max =  0.029439316617625947 learning rate ratio =  0.0002966296930345159\n",
      "Epoch =  705 Batch =  0 Loss =  13.865016752572343 Gradient_max =  0.029475927242327695 learning rate ratio =  0.0003028924654823011\n",
      "Epoch =  706 Batch =  0 Loss =  13.865015194092368 Gradient_max =  0.02951258345202505 learning rate ratio =  0.000309417162483666\n",
      "Epoch =  707 Batch =  0 Loss =  13.865013637096864 Gradient_max =  0.029549285303417234 learning rate ratio =  0.0003162205613791899\n",
      "Epoch =  708 Batch =  0 Loss =  13.865012081584997 Gradient_max =  0.02958603285329812 learning rate ratio =  0.000323320903647367\n",
      "Epoch =  709 Batch =  0 Loss =  13.8650105275908 Gradient_max =  0.029622826158569534 learning rate ratio =  0.0003307380581845167\n",
      "Epoch =  710 Batch =  0 Loss =  13.86500897507871 Gradient_max =  0.02965966527612641 learning rate ratio =  0.0003384937069256713\n",
      "Epoch =  711 Batch =  0 Loss =  13.865007424058172 Gradient_max =  0.029696550262986506 learning rate ratio =  0.00034661155648100085\n",
      "Epoch =  712 Batch =  0 Loss =  13.86500587452556 Gradient_max =  0.029733481176221686 learning rate ratio =  0.00035511758011224487\n",
      "Epoch =  713 Batch =  0 Loss =  13.865004326489819 Gradient_max =  0.029770458072991165 learning rate ratio =  0.00036404029526963545\n",
      "Epoch =  714 Batch =  0 Loss =  13.86500277998769 Gradient_max =  0.02980748101055457 learning rate ratio =  0.00037341108293005603\n",
      "Epoch =  715 Batch =  0 Loss =  13.86500123500037 Gradient_max =  0.029844550046182788 learning rate ratio =  0.0003832645562651555\n",
      "Epoch =  716 Batch =  0 Loss =  13.864999691517982 Gradient_max =  0.02988166523722589 learning rate ratio =  0.00039363898778448905\n",
      "Epoch =  717 Batch =  0 Loss =  13.86499814954322 Gradient_max =  0.02991882664112019 learning rate ratio =  0.0004045768060501099\n",
      "Epoch =  718 Batch =  0 Loss =  13.864996609078792 Gradient_max =  0.02995603431537346 learning rate ratio =  0.0004161251755555297\n",
      "Epoch =  719 Batch =  0 Loss =  13.864995070132139 Gradient_max =  0.029993288317569068 learning rate ratio =  0.0004283366764834685\n",
      "Epoch =  720 Batch =  0 Loss =  13.864993532701272 Gradient_max =  0.0300305887053541 learning rate ratio =  0.00044127010500291306\n",
      "Epoch =  721 Batch =  0 Loss =  13.864991996788925 Gradient_max =  0.03006793553645139 learning rate ratio =  0.0004549914198059077\n",
      "Epoch =  722 Batch =  0 Loss =  13.864990462397854 Gradient_max =  0.030105328868655686 learning rate ratio =  0.0004695748670359668\n",
      "Epoch =  723 Batch =  0 Loss =  13.864988929528634 Gradient_max =  0.030142768759828985 learning rate ratio =  0.0004851043241063237\n",
      "Epoch =  724 Batch =  0 Loss =  13.864987398186232 Gradient_max =  0.030180255267914763 learning rate ratio =  0.0005016749137659157\n",
      "Epoch =  725 Batch =  0 Loss =  13.864985868373436 Gradient_max =  0.030217788450923905 learning rate ratio =  0.0005193949540129623\n",
      "Epoch =  726 Batch =  0 Loss =  13.86498434012813 Gradient_max =  0.03025536836697793 learning rate ratio =  0.0005383883283226988\n",
      "Epoch =  727 Batch =  0 Loss =  13.86498281348342 Gradient_max =  0.030292995074265743 learning rate ratio =  0.0005587973858140109\n",
      "Epoch =  728 Batch =  0 Loss =  13.864981288376757 Gradient_max =  0.03033066863094338 learning rate ratio =  0.0005807865149407419\n",
      "Epoch =  729 Batch =  0 Loss =  13.864979764810956 Gradient_max =  0.030368389095311168 learning rate ratio =  0.0006045465805821459\n",
      "Epoch =  730 Batch =  0 Loss =  13.864978242788862 Gradient_max =  0.03040615652574207 learning rate ratio =  0.0006303004780361314\n",
      "Epoch =  731 Batch =  0 Loss =  13.864976722313314 Gradient_max =  0.03044397098068173 learning rate ratio =  0.0006583101463287119\n",
      "Epoch =  732 Batch =  0 Loss =  13.864975203387186 Gradient_max =  0.030481832518648545 learning rate ratio =  0.0006888855086581192\n",
      "Epoch =  733 Batch =  0 Loss =  13.864973686013336 Gradient_max =  0.03051974119823376 learning rate ratio =  0.0007223959874249606\n",
      "Epoch =  734 Batch =  0 Loss =  13.864972170198504 Gradient_max =  0.030557697078104795 learning rate ratio =  0.0007592855024292364\n",
      "Epoch =  735 Batch =  0 Loss =  13.864970655941717 Gradient_max =  0.03059570021699573 learning rate ratio =  0.0008000922467146214\n",
      "Epoch =  736 Batch =  0 Loss =  13.864969143259653 Gradient_max =  0.030633750673733046 learning rate ratio =  0.000845475115182776\n",
      "Epoch =  737 Batch =  0 Loss =  13.86496763214791 Gradient_max =  0.030671848507189407 learning rate ratio =  0.0008962495521106358\n",
      "Epoch =  738 Batch =  0 Loss =  13.864966122602963 Gradient_max =  0.030709993776316648 learning rate ratio =  0.0009534369811854833\n",
      "Epoch =  739 Batch =  0 Loss =  13.864964614627745 Gradient_max =  0.030748186540145116 learning rate ratio =  0.001018334226850122\n",
      "Epoch =  740 Batch =  0 Loss =  13.864963108229142 Gradient_max =  0.030786426857781917 learning rate ratio =  0.0010926130414992239\n",
      "Epoch =  741 Batch =  0 Loss =  13.86496160340617 Gradient_max =  0.030824714788401236 learning rate ratio =  0.0011784661571879872\n",
      "Epoch =  742 Batch =  0 Loss =  13.864960100174377 Gradient_max =  0.030863050391268906 learning rate ratio =  0.001278827378478078\n",
      "Epoch =  743 Batch =  0 Loss =  13.864958598524176 Gradient_max =  0.030901433725695114 learning rate ratio =  0.001397713549003877\n",
      "Epoch =  744 Batch =  0 Loss =  13.864957098458554 Gradient_max =  0.03093986485107863 learning rate ratio =  0.00154077513067719\n",
      "Epoch =  745 Batch =  0 Loss =  13.86495559998374 Gradient_max =  0.030978343826894756 learning rate ratio =  0.0017162206715151634\n",
      "Epoch =  746 Batch =  0 Loss =  13.864954103134478 Gradient_max =  0.031016870712725197 learning rate ratio =  0.0019364491850099774\n",
      "Epoch =  747 Batch =  0 Loss =  13.864952607878829 Gradient_max =  0.03105544556815337 learning rate ratio =  0.0022211153938481846\n",
      "Epoch =  748 Batch =  0 Loss =  13.864951114266756 Gradient_max =  0.031094068452926962 learning rate ratio =  0.002603346651035734\n",
      "Epoch =  749 Batch =  0 Loss =  13.864949622286717 Gradient_max =  0.031132739426798322 learning rate ratio =  0.003143674036231394\n",
      "Epoch =  750 Batch =  0 Loss =  13.864948131909465 Gradient_max =  0.031171458549575974 learning rate ratio =  0.0039657566000478725\n",
      "Epoch =  751 Batch =  0 Loss =  13.864946643138065 Gradient_max =  0.031210225881178333 learning rate ratio =  0.005367700415286884\n",
      "Epoch =  752 Batch =  0 Loss =  13.864945156043166 Gradient_max =  0.03124904148167262 learning rate ratio =  0.00829727394067205\n",
      "Epoch =  753 Batch =  0 Loss =  13.864943670560303 Gradient_max =  0.03128790541105236 learning rate ratio =  0.01823999606900922\n",
      "Epoch =  754 Batch =  0 Loss =  13.864942186692588 Gradient_max =  0.03132681772946008 learning rate ratio =  0.000535210603800568\n",
      "Epoch =  755 Batch =  0 Loss =  13.864940704457743 Gradient_max =  0.031365778497130264 learning rate ratio =  0.0005544371343912161\n",
      "Epoch =  756 Batch =  0 Loss =  13.864939223844305 Gradient_max =  0.03140478777433819 learning rate ratio =  0.0005750704058494343\n",
      "Epoch =  757 Batch =  0 Loss =  13.86493774487624 Gradient_max =  0.031443845621475036 learning rate ratio =  0.0005972706520416186\n",
      "Epoch =  758 Batch =  0 Loss =  13.864936267718598 Gradient_max =  0.03148295209918098 learning rate ratio =  0.0006212234023676721\n",
      "Epoch =  759 Batch =  0 Loss =  13.864934792191768 Gradient_max =  0.03152210726775252 learning rate ratio =  0.0006471446784851476\n",
      "Epoch =  760 Batch =  0 Loss =  13.864933318298924 Gradient_max =  0.03156131118778289 learning rate ratio =  0.0006752875271034266\n",
      "Epoch =  761 Batch =  0 Loss =  13.864931846046487 Gradient_max =  0.031600563919943085 learning rate ratio =  0.0007059503074761305\n",
      "Epoch =  762 Batch =  0 Loss =  13.864930375434414 Gradient_max =  0.03163986552497493 learning rate ratio =  0.0007394873092580555\n",
      "Epoch =  763 Batch =  0 Loss =  13.864928906465918 Gradient_max =  0.03167921606369825 learning rate ratio =  0.0007763225027983676\n",
      "Epoch =  764 Batch =  0 Loss =  13.86492743914421 Gradient_max =  0.03171861559700862 learning rate ratio =  0.0008169675557590404\n",
      "Epoch =  765 Batch =  0 Loss =  13.864925973475634 Gradient_max =  0.03175806418587966 learning rate ratio =  0.0008620457446148151\n",
      "Epoch =  766 Batch =  0 Loss =  13.86492450946286 Gradient_max =  0.03179756189135815 learning rate ratio =  0.0009123241410327803\n",
      "Epoch =  767 Batch =  0 Loss =  13.8649230471066 Gradient_max =  0.03183710877456562 learning rate ratio =  0.000968757618519416\n",
      "Epoch =  768 Batch =  0 Loss =  13.864921586410134 Gradient_max =  0.03187670489670154 learning rate ratio =  0.0010325500739391342\n",
      "Epoch =  769 Batch =  0 Loss =  13.864920127446096 Gradient_max =  0.03191635031911768 learning rate ratio =  0.0011052412684953322\n",
      "Epoch =  770 Batch =  0 Loss =  13.864918670178593 Gradient_max =  0.03195604510312174 learning rate ratio =  0.001188832733776248\n",
      "Epoch =  771 Batch =  0 Loss =  13.864917214615597 Gradient_max =  0.03199578931014836 learning rate ratio =  0.0012859749067644652\n",
      "Epoch =  772 Batch =  0 Loss =  13.864915760725617 Gradient_max =  0.03203558300166399 learning rate ratio =  0.0014002532964499394\n",
      "Epoch =  773 Batch =  0 Loss =  13.86491430851199 Gradient_max =  0.03207542623925004 learning rate ratio =  0.001536640738399532\n",
      "Epoch =  774 Batch =  0 Loss =  13.864912857978045 Gradient_max =  0.03211531908456447 learning rate ratio =  0.0017022402423829165\n",
      "Epoch =  775 Batch =  0 Loss =  13.864911409166751 Gradient_max =  0.03215526159938562 learning rate ratio =  0.00190756239770184\n",
      "Epoch =  776 Batch =  0 Loss =  13.864909962041867 Gradient_max =  0.03219525384548166 learning rate ratio =  0.0021688472786439554\n",
      "Epoch =  777 Batch =  0 Loss =  13.864908516671425 Gradient_max =  0.03223529588481256 learning rate ratio =  0.00251258446677553\n",
      "Epoch =  778 Batch =  0 Loss =  13.86490707299417 Gradient_max =  0.03227538777927267 learning rate ratio =  0.0029851137530391633\n",
      "Epoch =  779 Batch =  0 Loss =  13.86490563101352 Gradient_max =  0.03231552959090481 learning rate ratio =  0.003675505141833107\n",
      "Epoch =  780 Batch =  0 Loss =  13.864904190732878 Gradient_max =  0.03235572138182908 learning rate ratio =  0.00477956658356327\n",
      "Epoch =  781 Batch =  0 Loss =  13.864902752155666 Gradient_max =  0.0323959632142429 learning rate ratio =  0.0068281218967911755\n",
      "Epoch =  782 Batch =  0 Loss =  13.864901315322765 Gradient_max =  0.032436255150460974 learning rate ratio =  0.01193903711913783\n",
      "Epoch =  783 Batch =  0 Loss =  13.864899880200191 Gradient_max =  0.03247659725279587 learning rate ratio =  0.04730149701230697\n",
      "Epoch =  784 Batch =  0 Loss =  13.864898446791411 Gradient_max =  0.03251698958367762 learning rate ratio =  0.0003002393742991731\n",
      "Epoch =  785 Batch =  0 Loss =  13.864897015099903 Gradient_max =  0.03255743220561403 learning rate ratio =  0.00030748734591790983\n",
      "Epoch =  786 Batch =  0 Loss =  13.864895585129151 Gradient_max =  0.032597925181190694 learning rate ratio =  0.0003150847200003095\n",
      "Epoch =  787 Batch =  0 Loss =  13.864894156882663 Gradient_max =  0.032638468573071115 learning rate ratio =  0.0003230573786731209\n",
      "Epoch =  788 Batch =  0 Loss =  13.864892730395827 Gradient_max =  0.032679062444034145 learning rate ratio =  0.00033143382508636046\n",
      "Epoch =  789 Batch =  0 Loss =  13.864891305640318 Gradient_max =  0.032719706856862166 learning rate ratio =  0.0003402455238168607\n",
      "Epoch =  790 Batch =  0 Loss =  13.86488988262282 Gradient_max =  0.032760401874455385 learning rate ratio =  0.000349527295728568\n",
      "Epoch =  791 Batch =  0 Loss =  13.86488846134375 Gradient_max =  0.032801147559787684 learning rate ratio =  0.00035931777774941224\n",
      "Epoch =  792 Batch =  0 Loss =  13.864887041806663 Gradient_max =  0.032841943975913696 learning rate ratio =  0.0003696599603558753\n",
      "Epoch =  793 Batch =  0 Loss =  13.86488562401515 Gradient_max =  0.032882791185966515 learning rate ratio =  0.0003806018185359885\n",
      "Epoch =  794 Batch =  0 Loss =  13.864884207972795 Gradient_max =  0.032923689253157876 learning rate ratio =  0.0003921970557629541\n",
      "Epoch =  795 Batch =  0 Loss =  13.864882793683208 Gradient_max =  0.03296463824077818 learning rate ratio =  0.000404505985320919\n",
      "Epoch =  796 Batch =  0 Loss =  13.864881381150004 Gradient_max =  0.03300563821219665 learning rate ratio =  0.0004175965795063212\n",
      "Epoch =  797 Batch =  0 Loss =  13.864879970376816 Gradient_max =  0.03304668923086144 learning rate ratio =  0.0004315457252345882\n",
      "Epoch =  798 Batch =  0 Loss =  13.864878561367277 Gradient_max =  0.033087791360299657 learning rate ratio =  0.0004464407350309612\n",
      "Epoch =  799 Batch =  0 Loss =  13.864877154125061 Gradient_max =  0.0331289446641175 learning rate ratio =  0.0004623811761330383\n",
      "Epoch =  800 Batch =  0 Loss =  13.864875748653827 Gradient_max =  0.033170149206000386 learning rate ratio =  0.0004794810986811311\n",
      "Epoch =  801 Batch =  0 Loss =  13.864874344957254 Gradient_max =  0.03321140504971305 learning rate ratio =  0.0004978717684188415\n",
      "Epoch =  802 Batch =  0 Loss =  13.864872943035532 Gradient_max =  0.03325271225909322 learning rate ratio =  0.000517705042401334\n",
      "Epoch =  803 Batch =  0 Loss =  13.864871542889082 Gradient_max =  0.03329407089806214 learning rate ratio =  0.0005391575714322694\n",
      "Epoch =  804 Batch =  0 Loss =  13.864870144592699 Gradient_max =  0.03333548103070116 learning rate ratio =  0.0005624360754912669\n",
      "Epoch =  805 Batch =  0 Loss =  13.864868748145685 Gradient_max =  0.03337694272108951 learning rate ratio =  0.0005877840260093139\n",
      "Epoch =  806 Batch =  0 Loss =  13.864867353527641 Gradient_max =  0.03341845603336105 learning rate ratio =  0.0006154901930116283\n",
      "Epoch =  807 Batch =  0 Loss =  13.864865960725032 Gradient_max =  0.03346002103174284 learning rate ratio =  0.0006458996939579161\n",
      "Epoch =  808 Batch =  0 Loss =  13.864864569768953 Gradient_max =  0.03350163778058419 learning rate ratio =  0.0006794284423533085\n",
      "Epoch =  809 Batch =  0 Loss =  13.86486318061754 Gradient_max =  0.03354330634423953 learning rate ratio =  0.0007165822826055823\n",
      "Epoch =  810 Batch =  0 Loss =  13.864861793274596 Gradient_max =  0.033585026787191524 learning rate ratio =  0.0007579826856571436\n",
      "Epoch =  811 Batch =  0 Loss =  13.864860407743937 Gradient_max =  0.03362679917400333 learning rate ratio =  0.0008044017889287584\n",
      "Epoch =  812 Batch =  0 Loss =  13.864859024029391 Gradient_max =  0.033668623569318414 learning rate ratio =  0.0008568110007637334\n",
      "Epoch =  813 Batch =  0 Loss =  13.864857642165639 Gradient_max =  0.03371050003789339 learning rate ratio =  0.0009164497180930092\n",
      "Epoch =  814 Batch =  0 Loss =  13.864856262125716 Gradient_max =  0.03375242864450012 learning rate ratio =  0.0009849245870018192\n",
      "Epoch =  815 Batch =  0 Loss =  13.864854883950425 Gradient_max =  0.03379440945406479 learning rate ratio =  0.0010643564111947734\n",
      "Epoch =  816 Batch =  0 Loss =  13.86485350762007 Gradient_max =  0.033836442531528135 learning rate ratio =  0.0011576037134057604\n",
      "Epoch =  817 Batch =  0 Loss =  13.86485213316436 Gradient_max =  0.03387852794196317 learning rate ratio =  0.0012686140552735203\n",
      "Epoch =  818 Batch =  0 Loss =  13.864850760548075 Gradient_max =  0.03392066575045611 learning rate ratio =  0.0014029972610403727\n",
      "Epoch =  819 Batch =  0 Loss =  13.86484938977515 Gradient_max =  0.03396285602221589 learning rate ratio =  0.0015690033054392132\n",
      "Epoch =  820 Batch =  0 Loss =  13.864848020849514 Gradient_max =  0.03400509882253271 learning rate ratio =  0.001779282590249341\n",
      "Epoch =  821 Batch =  0 Loss =  13.864846653775121 Gradient_max =  0.03404739421677801 learning rate ratio =  0.0020542712792195068\n",
      "Epoch =  822 Batch =  0 Loss =  13.864845288619037 Gradient_max =  0.03408974227047544 learning rate ratio =  0.002429270229830575\n",
      "Epoch =  823 Batch =  0 Loss =  13.864843925322159 Gradient_max =  0.03413214304908856 learning rate ratio =  0.002970964075922707\n",
      "Epoch =  824 Batch =  0 Loss =  13.86484256388849 Gradient_max =  0.034174596618233334 learning rate ratio =  0.0038222648136360072\n",
      "Epoch =  825 Batch =  0 Loss =  13.864841204322028 Gradient_max =  0.03421710304360737 learning rate ratio =  0.005354812326037566\n",
      "Epoch =  826 Batch =  0 Loss =  13.864839846626808 Gradient_max =  0.034259662390990084 learning rate ratio =  0.00893178249234478\n",
      "Epoch =  827 Batch =  0 Loss =  13.864838490844026 Gradient_max =  0.034302274726283714 learning rate ratio =  0.02683815877989592\n",
      "Epoch =  828 Batch =  0 Loss =  13.86483713694057 Gradient_max =  0.034344940115390694 learning rate ratio =  0.0008119603912109599\n",
      "Epoch =  829 Batch =  0 Loss =  13.864835784920507 Gradient_max =  0.03438765862433646 learning rate ratio =  0.0008564001197547318\n",
      "Epoch =  830 Batch =  0 Loss =  13.864834434787916 Gradient_max =  0.03443043031922867 learning rate ratio =  0.0009059225925623993\n",
      "Epoch =  831 Batch =  0 Loss =  13.864833086546891 Gradient_max =  0.03447325526625726 learning rate ratio =  0.0009614526518403445\n",
      "Epoch =  832 Batch =  0 Loss =  13.864831740201543 Gradient_max =  0.034516133531694596 learning rate ratio =  0.001024154003105667\n",
      "Epoch =  833 Batch =  0 Loss =  13.864830395755982 Gradient_max =  0.034559065181895526 learning rate ratio =  0.0010955116535481238\n",
      "Epoch =  834 Batch =  0 Loss =  13.864829053214358 Gradient_max =  0.034602050283297554 learning rate ratio =  0.0011774510239246684\n",
      "Epoch =  835 Batch =  0 Loss =  13.864827712623613 Gradient_max =  0.03464508890246801 learning rate ratio =  0.001272514291908516\n",
      "Epoch =  836 Batch =  0 Loss =  13.864826373987931 Gradient_max =  0.034688181106009935 learning rate ratio =  0.0013841288416688153\n",
      "Epoch =  837 Batch =  0 Loss =  13.86482503726869 Gradient_max =  0.034731326960562144 learning rate ratio =  0.0015170292976868127\n",
      "Epoch =  838 Batch =  0 Loss =  13.864823702465355 Gradient_max =  0.034774526532887144 learning rate ratio =  0.0016779464605780332\n",
      "Epoch =  839 Batch =  0 Loss =  13.864822369679489 Gradient_max =  0.03481777988994607 learning rate ratio =  0.0018767832731803645\n",
      "Epoch =  840 Batch =  0 Loss =  13.86482103882271 Gradient_max =  0.03486108709857173 learning rate ratio =  0.0021287331441231334\n",
      "Epoch =  841 Batch =  0 Loss =  13.86481970993878 Gradient_max =  0.034904448225825016 learning rate ratio =  0.00245835744839342\n",
      "Epoch =  842 Batch =  0 Loss =  13.864818382992425 Gradient_max =  0.03494786333876584 learning rate ratio =  0.00290812143363579\n",
      "Epoch =  843 Batch =  0 Loss =  13.864817058003226 Gradient_max =  0.03499133250459778 learning rate ratio =  0.0035583433217197175\n",
      "Epoch =  844 Batch =  0 Loss =  13.864815734960157 Gradient_max =  0.03503485579057226 learning rate ratio =  0.004581465545313982\n",
      "Epoch =  845 Batch =  0 Loss =  13.864814413867522 Gradient_max =  0.035078433264042336 learning rate ratio =  0.006427237362495682\n",
      "Epoch =  846 Batch =  0 Loss =  13.864813094729639 Gradient_max =  0.03512206499244495 learning rate ratio =  0.010754908690846667\n",
      "Epoch =  847 Batch =  0 Loss =  13.864811778073074 Gradient_max =  0.035165751043710856 learning rate ratio =  0.03284121014659713\n",
      "Epoch =  848 Batch =  0 Loss =  13.864810465366086 Gradient_max =  0.0352094914865743 learning rate ratio =  0.0003046821871400626\n",
      "Epoch =  849 Batch =  0 Loss =  13.864809154630185 Gradient_max =  0.03525328638718776 learning rate ratio =  0.00031173841035671317\n",
      "Epoch =  850 Batch =  0 Loss =  13.864807845952589 Gradient_max =  0.03529713581341434 learning rate ratio =  0.00031911967097427395\n",
      "Epoch =  851 Batch =  0 Loss =  13.864806539295582 Gradient_max =  0.0353410398330662 learning rate ratio =  0.0003268489523072455\n",
      "Epoch =  852 Batch =  0 Loss =  13.864805234622851 Gradient_max =  0.03538499851404157 learning rate ratio =  0.0003349514568307891\n",
      "Epoch =  853 Batch =  0 Loss =  13.864803931941937 Gradient_max =  0.03542901192436958 learning rate ratio =  0.0003434548806447848\n",
      "Epoch =  854 Batch =  0 Loss =  13.864802631254145 Gradient_max =  0.0354730801321595 learning rate ratio =  0.00035238972970759225\n",
      "Epoch =  855 Batch =  0 Loss =  13.86480133256393 Gradient_max =  0.03551720320560775 learning rate ratio =  0.00036178968544548325\n",
      "Epoch =  856 Batch =  0 Loss =  13.864800035875758 Gradient_max =  0.03556138121299553 learning rate ratio =  0.00037169202897378373\n",
      "Epoch =  857 Batch =  0 Loss =  13.864798741194104 Gradient_max =  0.03560561422268913 learning rate ratio =  0.0003821381351926365\n",
      "Epoch =  858 Batch =  0 Loss =  13.864797448523477 Gradient_max =  0.035649902303139996 learning rate ratio =  0.00039317405056388716\n",
      "Epoch =  859 Batch =  0 Loss =  13.864796157868373 Gradient_max =  0.0356942455228847 learning rate ratio =  0.00040485117158386606\n",
      "Epoch =  860 Batch =  0 Loss =  13.864794869274792 Gradient_max =  0.03573864395059382 learning rate ratio =  0.0004172270450379142\n",
      "Epoch =  861 Batch =  0 Loss =  13.864793582716286 Gradient_max =  0.035783097654936286 learning rate ratio =  0.00043036631633047316\n",
      "Epoch =  862 Batch =  0 Loss =  13.864792298186984 Gradient_max =  0.03582760670469495 learning rate ratio =  0.0004443418588635624\n",
      "Epoch =  863 Batch =  0 Loss =  13.864791015691464 Gradient_max =  0.035872171168748426 learning rate ratio =  0.0004592361261235856\n",
      "Epoch =  864 Batch =  0 Loss =  13.86478973523432 Gradient_max =  0.03591679111606103 learning rate ratio =  0.00047514277945061447\n",
      "Epoch =  865 Batch =  0 Loss =  13.864788456820149 Gradient_max =  0.03596146661568298 learning rate ratio =  0.0004921686593752312\n",
      "Epoch =  866 Batch =  0 Loss =  13.864787180533023 Gradient_max =  0.03600619773683624 learning rate ratio =  0.0005104361882070292\n",
      "Epoch =  867 Batch =  0 Loss =  13.864785906301329 Gradient_max =  0.036050984548659556 learning rate ratio =  0.0005300863181169216\n",
      "Epoch =  868 Batch =  0 Loss =  13.86478463412923 Gradient_max =  0.03609582712046099 learning rate ratio =  0.000551282174861619\n",
      "Epoch =  869 Batch =  0 Loss =  13.864783364021891 Gradient_max =  0.03614072552163567 learning rate ratio =  0.000574213596524092\n",
      "Epoch =  870 Batch =  0 Loss =  13.864782095980773 Gradient_max =  0.036185679821662396 learning rate ratio =  0.0005991028346880313\n",
      "Epoch =  871 Batch =  0 Loss =  13.864780830027183 Gradient_max =  0.0362306900901282 learning rate ratio =  0.0006262117808706452\n",
      "Epoch =  872 Batch =  0 Loss =  13.86477956615217 Gradient_max =  0.03627575639667004 learning rate ratio =  0.0006558512165040466\n",
      "Epoch =  873 Batch =  0 Loss =  13.864778304400524 Gradient_max =  0.036320878811073445 learning rate ratio =  0.0006883927798929219\n",
      "Epoch =  874 Batch =  0 Loss =  13.864777044775385 Gradient_max =  0.03636605740316426 learning rate ratio =  0.0007242846291493817\n",
      "Epoch =  875 Batch =  0 Loss =  13.864775787240154 Gradient_max =  0.0364112922428125 learning rate ratio =  0.0007640722051082507\n",
      "Epoch =  876 Batch =  0 Loss =  13.864774531846173 Gradient_max =  0.03645658340006918 learning rate ratio =  0.000808426142753926\n",
      "Epoch =  877 Batch =  0 Loss =  13.86477327858908 Gradient_max =  0.03650193094501445 learning rate ratio =  0.000858180377349866\n",
      "Epoch =  878 Batch =  0 Loss =  13.864772027436288 Gradient_max =  0.03654733494778409 learning rate ratio =  0.00091438507108292\n",
      "Epoch =  879 Batch =  0 Loss =  13.864770778392622 Gradient_max =  0.03659279547864168 learning rate ratio =  0.0009783815511431714\n",
      "Epoch =  880 Batch =  0 Loss =  13.864769531462922 Gradient_max =  0.03663831260793825 learning rate ratio =  0.0010519107346043017\n",
      "Epoch =  881 Batch =  0 Loss =  13.864768286654996 Gradient_max =  0.03668388640611456 learning rate ratio =  0.0011372739028039395\n",
      "Epoch =  882 Batch =  0 Loss =  13.864767043993785 Gradient_max =  0.03672951694372046 learning rate ratio =  0.0012375778935955618\n",
      "Epoch =  883 Batch =  0 Loss =  13.86476580346118 Gradient_max =  0.03677520429134445 learning rate ratio =  0.00135712138386032\n",
      "Epoch =  884 Batch =  0 Loss =  13.864764565062096 Gradient_max =  0.036820948519688625 learning rate ratio =  0.0015020270281242431\n",
      "Epoch =  885 Batch =  0 Loss =  13.864763328801466 Gradient_max =  0.03686674969954306 learning rate ratio =  0.0016813236760212938\n",
      "Epoch =  886 Batch =  0 Loss =  13.86476209468422 Gradient_max =  0.036912607901785965 learning rate ratio =  0.001908902868885494\n",
      "Epoch =  887 Batch =  0 Loss =  13.864760862753233 Gradient_max =  0.03695852319742477 learning rate ratio =  0.002207301835358641\n",
      "Epoch =  888 Batch =  0 Loss =  13.864759633062432 Gradient_max =  0.037004495657565944 learning rate ratio =  0.002615668873697549\n",
      "Epoch =  889 Batch =  0 Loss =  13.864758405529956 Gradient_max =  0.03705052535326021 learning rate ratio =  0.003208524279188811\n",
      "Epoch =  890 Batch =  0 Loss =  13.864757180177465 Gradient_max =  0.037096612355758875 learning rate ratio =  0.004147367580043683\n",
      "Epoch =  891 Batch =  0 Loss =  13.864755957080053 Gradient_max =  0.03714275673645696 learning rate ratio =  0.0058598655322709605\n",
      "Epoch =  892 Batch =  0 Loss =  13.864754736156076 Gradient_max =  0.03718895856666987 learning rate ratio =  0.009972438119943586\n",
      "Epoch =  893 Batch =  0 Loss =  13.864753517397846 Gradient_max =  0.03723521791787936 learning rate ratio =  0.033345915659839\n",
      "Epoch =  894 Batch =  0 Loss =  13.86475230082647 Gradient_max =  0.037281534861691584 learning rate ratio =  0.0008679622165871343\n",
      "Epoch =  895 Batch =  0 Loss =  13.86475108644379 Gradient_max =  0.03732790946978059 learning rate ratio =  0.00093403435345221\n",
      "Epoch =  896 Batch =  0 Loss =  13.86474987425491 Gradient_max =  0.037374341813911985 learning rate ratio =  0.0010108907072611058\n",
      "Epoch =  897 Batch =  0 Loss =  13.86474866426497 Gradient_max =  0.037420831965940766 learning rate ratio =  0.001101406114676589\n",
      "Epoch =  898 Batch =  0 Loss =  13.864747456479112 Gradient_max =  0.03746737999781138 learning rate ratio =  0.0012095768910991309\n",
      "Epoch =  899 Batch =  0 Loss =  13.864746250922204 Gradient_max =  0.037513985981580865 learning rate ratio =  0.0013411267906617633\n",
      "Epoch =  900 Batch =  0 Loss =  13.864745047582701 Gradient_max =  0.037560649989352046 learning rate ratio =  0.001504553464940247\n",
      "Epoch =  901 Batch =  0 Loss =  13.864743846462837 Gradient_max =  0.0376073720933362 learning rate ratio =  0.001713040510432086\n",
      "Epoch =  902 Batch =  0 Loss =  13.864742647567818 Gradient_max =  0.037654152365836795 learning rate ratio =  0.0019882039851151687\n",
      "Epoch =  903 Batch =  0 Loss =  13.86474145090567 Gradient_max =  0.0377009908792492 learning rate ratio =  0.0023681176136066398\n",
      "Epoch =  904 Batch =  0 Loss =  13.864740256478859 Gradient_max =  0.03774788770605498 learning rate ratio =  0.002926657743609284\n",
      "Epoch =  905 Batch =  0 Loss =  13.86473906429265 Gradient_max =  0.037794842918827905 learning rate ratio =  0.003828521063421843\n",
      "Epoch =  906 Batch =  0 Loss =  13.86473787439659 Gradient_max =  0.03784185659028013 learning rate ratio =  0.005530713041296783\n",
      "Epoch =  907 Batch =  0 Loss =  13.864736686754503 Gradient_max =  0.03788892879312022 learning rate ratio =  0.00994829906313725\n",
      "Epoch =  908 Batch =  0 Loss =  13.864735501387186 Gradient_max =  0.03793605960021279 learning rate ratio =  0.04918626040306718\n",
      "Epoch =  909 Batch =  0 Loss =  13.864734318281734 Gradient_max =  0.0379832490844724 learning rate ratio =  0.00036790354489294884\n",
      "Epoch =  910 Batch =  0 Loss =  13.8647331374435 Gradient_max =  0.038030497318925736 learning rate ratio =  0.00037953357077614584\n",
      "Epoch =  911 Batch =  0 Loss =  13.864731958877861 Gradient_max =  0.0380778043766904 learning rate ratio =  0.0003919077400967412\n",
      "Epoch =  912 Batch =  0 Loss =  13.864730782637206 Gradient_max =  0.038125170331025804 learning rate ratio =  0.00040509982205824284\n",
      "Epoch =  913 Batch =  0 Loss =  13.86472960867995 Gradient_max =  0.03817259525518099 learning rate ratio =  0.0004191936697062527\n",
      "Epoch =  914 Batch =  0 Loss =  13.864728437029394 Gradient_max =  0.03822007922256793 learning rate ratio =  0.00043428500389082383\n",
      "Epoch =  915 Batch =  0 Loss =  13.864727267723437 Gradient_max =  0.038267622306702526 learning rate ratio =  0.000450483589860202\n",
      "Epoch =  916 Batch =  0 Loss =  13.864726100717233 Gradient_max =  0.038315224581104305 learning rate ratio =  0.0004679159111326673\n",
      "Epoch =  917 Batch =  0 Loss =  13.864724936146388 Gradient_max =  0.03836288611957924 learning rate ratio =  0.0004867284784097744\n",
      "Epoch =  918 Batch =  0 Loss =  13.86472377389205 Gradient_max =  0.0384106069957482 learning rate ratio =  0.0005070919568170438\n",
      "Epoch =  919 Batch =  0 Loss =  13.86472261395396 Gradient_max =  0.03845838728345612 learning rate ratio =  0.0005292063576209428\n",
      "Epoch =  920 Batch =  0 Loss =  13.864721456340332 Gradient_max =  0.03850622705664581 learning rate ratio =  0.0005533076292154801\n",
      "Epoch =  921 Batch =  0 Loss =  13.864720301054037 Gradient_max =  0.03855412638934849 learning rate ratio =  0.0005796761080042206\n",
      "Epoch =  922 Batch =  0 Loss =  13.864719148209758 Gradient_max =  0.038602085355808904 learning rate ratio =  0.0006086474717118917\n",
      "Epoch =  923 Batch =  0 Loss =  13.864717997703996 Gradient_max =  0.03865010403012507 learning rate ratio =  0.0006406271045590086\n",
      "Epoch =  924 Batch =  0 Loss =  13.864716849542356 Gradient_max =  0.03869818248660689 learning rate ratio =  0.0006761091819876281\n",
      "Epoch =  925 Batch =  0 Loss =  13.864715703730472 Gradient_max =  0.038746320799656786 learning rate ratio =  0.0007157023888369753\n",
      "Epoch =  926 Batch =  0 Loss =  13.864714560273995 Gradient_max =  0.038794519043769835 learning rate ratio =  0.0007601651262378723\n",
      "Epoch =  927 Batch =  0 Loss =  13.86471341917859 Gradient_max =  0.03884277729353378 learning rate ratio =  0.0008104545591876\n",
      "Epoch =  928 Batch =  0 Loss =  13.864712280449947 Gradient_max =  0.038891095623629336 learning rate ratio =  0.0008677962978466526\n",
      "Epoch =  929 Batch =  0 Loss =  13.864711144093762 Gradient_max =  0.038939474108830176 learning rate ratio =  0.0009337856032372513\n",
      "Epoch =  930 Batch =  0 Loss =  13.864710010115767 Gradient_max =  0.03898791282400297 learning rate ratio =  0.0010105381131386552\n",
      "Epoch =  931 Batch =  0 Loss =  13.864708878563402 Gradient_max =  0.03903641184415269 learning rate ratio =  0.001100920866940177\n",
      "Epoch =  932 Batch =  0 Loss =  13.864707749400731 Gradient_max =  0.0390849712442876 learning rate ratio =  0.0012089184017814803\n",
      "Epoch =  933 Batch =  0 Loss =  13.864706622633527 Gradient_max =  0.039133591099554416 learning rate ratio =  0.0013402359996834883\n",
      "Epoch =  934 Batch =  0 Loss =  13.864705498267591 Gradient_max =  0.03918227148519344 learning rate ratio =  0.0015033410038972999\n",
      "Epoch =  935 Batch =  0 Loss =  13.864704376377517 Gradient_max =  0.03923101247661504 learning rate ratio =  0.0017113644482846398\n",
      "Epoch =  936 Batch =  0 Loss =  13.864703256900377 Gradient_max =  0.039279814149170506 learning rate ratio =  0.0019858246495600917\n",
      "Epoch =  937 Batch =  0 Loss =  13.864702139842024 Gradient_max =  0.03932867657838152 learning rate ratio =  0.002364596342918766\n",
      "Epoch =  938 Batch =  0 Loss =  13.864701025208337 Gradient_max =  0.03937759983986371 learning rate ratio =  0.002921097753190411\n",
      "Epoch =  939 Batch =  0 Loss =  13.864699912997434 Gradient_max =  0.03942658400931712 learning rate ratio =  0.003818765811095234\n",
      "Epoch =  940 Batch =  0 Loss =  13.864698803223023 Gradient_max =  0.03947562916255561 learning rate ratio =  0.00551000534435946\n",
      "Epoch =  941 Batch =  0 Loss =  13.864697695891039 Gradient_max =  0.03952473537547761 learning rate ratio =  0.00988074861168188\n",
      "Epoch =  942 Batch =  0 Loss =  13.864696591053052 Gradient_max =  0.03957390272412539 learning rate ratio =  0.04757091879713435\n",
      "Epoch =  943 Batch =  0 Loss =  13.864695488769934 Gradient_max =  0.0396231312846464 learning rate ratio =  0.0001489284301211084\n",
      "Epoch =  944 Batch =  0 Loss =  13.864694388967235 Gradient_max =  0.03967242113313724 learning rate ratio =  0.00015105829425197758\n",
      "Epoch =  945 Batch =  0 Loss =  13.864693291630896 Gradient_max =  0.03972177234585148 learning rate ratio =  0.00015324734870275067\n",
      "Epoch =  946 Batch =  0 Loss =  13.864692196766953 Gradient_max =  0.039771184999161234 learning rate ratio =  0.00015549809367017427\n",
      "Epoch =  947 Batch =  0 Loss =  13.864691104427232 Gradient_max =  0.039820659169587325 learning rate ratio =  0.00015781317217241007\n",
      "Epoch =  948 Batch =  0 Loss =  13.864690014572037 Gradient_max =  0.03987019493363847 learning rate ratio =  0.00016019538039724365\n",
      "Epoch =  949 Batch =  0 Loss =  13.864688927253077 Gradient_max =  0.03991979236802185 learning rate ratio =  0.00016264767895772888\n",
      "Epoch =  950 Batch =  0 Loss =  13.864687842430849 Gradient_max =  0.03996945154944116 learning rate ratio =  0.00016517320516188863\n",
      "Epoch =  951 Batch =  0 Loss =  13.864686760111473 Gradient_max =  0.040019172554745026 learning rate ratio =  0.00016777528639151887\n",
      "Epoch =  952 Batch =  0 Loss =  13.864685680303815 Gradient_max =  0.04006895546087973 learning rate ratio =  0.00017045745471901122\n",
      "Epoch =  953 Batch =  0 Loss =  13.864684603014984 Gradient_max =  0.04011880034488621 learning rate ratio =  0.0001732234628931677\n",
      "Epoch =  954 Batch =  0 Loss =  13.864683528242246 Gradient_max =  0.04016870728388975 learning rate ratio =  0.00017607730184690862\n",
      "Epoch =  955 Batch =  0 Loss =  13.86468245606934 Gradient_max =  0.0402186763552107 learning rate ratio =  0.00017902321989723592\n",
      "Epoch =  956 Batch =  0 Loss =  13.864681386481251 Gradient_max =  0.04026870763615239 learning rate ratio =  0.00018206574383767052\n",
      "Epoch =  957 Batch =  0 Loss =  13.864680319433226 Gradient_max =  0.040318801204084175 learning rate ratio =  0.00018520970213472483\n",
      "Epoch =  958 Batch =  0 Loss =  13.864679254931538 Gradient_max =  0.040368957136527 learning rate ratio =  0.00018846025048185907\n",
      "Epoch =  959 Batch =  0 Loss =  13.864678192982474 Gradient_max =  0.04041917551109828 learning rate ratio =  0.0001918228999987622\n",
      "Epoch =  960 Batch =  0 Loss =  13.864677133613332 Gradient_max =  0.04046945640553663 learning rate ratio =  0.00019530354839627072\n",
      "Epoch =  961 Batch =  0 Loss =  13.864676076809468 Gradient_max =  0.04051979989762838 learning rate ratio =  0.0001989085144807695\n",
      "Epoch =  962 Batch =  0 Loss =  13.864675022577238 Gradient_max =  0.04057020606528115 learning rate ratio =  0.00020264457641855626\n",
      "Epoch =  963 Batch =  0 Loss =  13.86467397097206 Gradient_max =  0.04062067498655279 learning rate ratio =  0.00020651901425060689\n",
      "Epoch =  964 Batch =  0 Loss =  13.864672921970033 Gradient_max =  0.04067120673951394 learning rate ratio =  0.00021053965722210019\n",
      "Epoch =  965 Batch =  0 Loss =  13.864671875558818 Gradient_max =  0.04072180140234169 learning rate ratio =  0.0002147149365651951\n",
      "Epoch =  966 Batch =  0 Loss =  13.8646708317368 Gradient_max =  0.04077245905332232 learning rate ratio =  0.00021905394449114535\n",
      "Epoch =  967 Batch =  0 Loss =  13.864669790518514 Gradient_max =  0.04082317977085986 learning rate ratio =  0.00022356650025866183\n",
      "Epoch =  968 Batch =  0 Loss =  13.86466875191042 Gradient_max =  0.04087396363344579 learning rate ratio =  0.00022826322432814816\n",
      "Epoch =  969 Batch =  0 Loss =  13.86466771591903 Gradient_max =  0.04092481071966926 learning rate ratio =  0.0002331556217808408\n",
      "Epoch =  970 Batch =  0 Loss =  13.864666682600022 Gradient_max =  0.04097572110827059 learning rate ratio =  0.00023825617638077072\n",
      "Epoch =  971 Batch =  0 Loss =  13.864665651910775 Gradient_max =  0.041026694877981565 learning rate ratio =  0.0002435784569048976\n",
      "Epoch =  972 Batch =  0 Loss =  13.86466462385784 Gradient_max =  0.04107773210768521 learning rate ratio =  0.00024913723763544894\n",
      "Epoch =  973 Batch =  0 Loss =  13.864663598501886 Gradient_max =  0.041128832876421446 learning rate ratio =  0.0002549486352784501\n",
      "Epoch =  974 Batch =  0 Loss =  13.864662575795425 Gradient_max =  0.041179997263211296 learning rate ratio =  0.0002610302649820158\n",
      "Epoch =  975 Batch =  0 Loss =  13.86466155579376 Gradient_max =  0.04123122534728545 learning rate ratio =  0.00026740141861995045\n",
      "Epoch =  976 Batch =  0 Loss =  13.864660538473935 Gradient_max =  0.041282517207890286 learning rate ratio =  0.00027408326916303203\n",
      "Epoch =  977 Batch =  0 Loss =  13.864659523823551 Gradient_max =  0.04133387292437887 learning rate ratio =  0.00028109910568017293\n",
      "Epoch =  978 Batch =  0 Loss =  13.864658511849292 Gradient_max =  0.04138529257622533 learning rate ratio =  0.0002884746044754442\n",
      "Epoch =  979 Batch =  0 Loss =  13.864657502577996 Gradient_max =  0.04143677624302638 learning rate ratio =  0.00029623814301075406\n",
      "Epoch =  980 Batch =  0 Loss =  13.86465649599628 Gradient_max =  0.04148832400443062 learning rate ratio =  0.00030442116470561333\n",
      "Epoch =  981 Batch =  0 Loss =  13.864655492110888 Gradient_max =  0.04153993594020946 learning rate ratio =  0.0003130586044949392\n",
      "Epoch =  982 Batch =  0 Loss =  13.864654490928606 Gradient_max =  0.04159161213023355 learning rate ratio =  0.00032218938729493545\n",
      "Epoch =  983 Batch =  0 Loss =  13.864653492456226 Gradient_max =  0.04164335265447303 learning rate ratio =  0.00033185701437432976\n",
      "Epoch =  984 Batch =  0 Loss =  13.86465249675458 Gradient_max =  0.04169515759305619 learning rate ratio =  0.0003421102562633784\n",
      "Epoch =  985 Batch =  0 Loss =  13.864651503779328 Gradient_max =  0.04174702702609578 learning rate ratio =  0.00035300397549970126\n",
      "Epoch =  986 Batch =  0 Loss =  13.864650513534492 Gradient_max =  0.041798961033858995 learning rate ratio =  0.000364600108486277\n",
      "Epoch =  987 Batch =  0 Loss =  13.864649526026955 Gradient_max =  0.04185095969671495 learning rate ratio =  0.0003769688435814244\n",
      "Epoch =  988 Batch =  0 Loss =  13.864648541257912 Gradient_max =  0.04190302309512525 learning rate ratio =  0.00039019004273915467\n",
      "Epoch =  989 Batch =  0 Loss =  13.864647559240026 Gradient_max =  0.04195515130966685 learning rate ratio =  0.00040435496754040337\n",
      "Epoch =  990 Batch =  0 Loss =  13.864646579980233 Gradient_max =  0.04200734442100939 learning rate ratio =  0.00041956838846515646\n",
      "Epoch =  991 Batch =  0 Loss =  13.864645603485513 Gradient_max =  0.04205960250992297 learning rate ratio =  0.000435951180495979\n",
      "Epoch =  992 Batch =  0 Loss =  13.864644629762862 Gradient_max =  0.04211192565727823 learning rate ratio =  0.00045364354110235356\n",
      "Epoch =  993 Batch =  0 Loss =  13.864643658821995 Gradient_max =  0.0421643139440484 learning rate ratio =  0.0004728090119623644\n",
      "Epoch =  994 Batch =  0 Loss =  13.864642690667255 Gradient_max =  0.04221676745130371 learning rate ratio =  0.0004936395487969009\n",
      "Epoch =  995 Batch =  0 Loss =  13.864641725378105 Gradient_max =  0.04226928626029576 learning rate ratio =  0.0005163619724585435\n",
      "Epoch =  996 Batch =  0 Loss =  13.86464076288927 Gradient_max =  0.04232187045222021 learning rate ratio =  0.0005412462612218512\n",
      "Epoch =  997 Batch =  0 Loss =  13.864639803210945 Gradient_max =  0.04237452010845466 learning rate ratio =  0.0005686163279163407\n",
      "Epoch =  998 Batch =  0 Loss =  13.864638846347162 Gradient_max =  0.042427235310473446 learning rate ratio =  0.0005988641963317399\n",
      "Epoch =  999 Batch =  0 Loss =  13.864637892350563 Gradient_max =  0.04248001613990378 learning rate ratio =  0.0006324688971428711\n"
     ]
    }
   ],
   "source": [
    "SGD = NewSolver(Model, x_train, y_train, lr = 1.9e-5, batch_size = 20, num_epochs = 1000, print_every = 1000)\n",
    "SGD.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(0,SGD.num_epochs)\n",
    "test_epochs  = range(0,SGD.loss_history.shape[0])\n",
    "plt.plot(test_epochs, SGD.loss_history, label = 'Loss Batch 0 ', color = 'red')\n",
    "plt.plot(test_epochs, SGD.loss_2_history, label = 'Loss Batch 35', color = 'blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epochs, SGD.vel_history, label = 'Velocity', color = 'blue')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2vUlEQVR4nO3deZyN5f/H8denMbIvaVEkQ5YkoSFLWbKvQ4MsZUZJKmX5WZKvCtU3aSFE9jVLY5fsIfsMTbYhS2IQgxhhMDPX74/r8J3Mcs4wZ5Yzn+fjMY9mzrmvc657Gu+55rqv+3OJMQallFKe65607oBSSin30qBXSikPp0GvlFIeToNeKaU8nAa9Ukp5OA16pZTycFlcOUhEGgIjAC9ggjHms9ueLw1MBioCA4wxX8R5Lh8wASgLGOBVY8yWpN7v/vvvN0WLFnX9LJRSKpPbsWPHWWPMAwk95zToRcQLGA3UA8KBYBFZbIzZF+ew88C7QIsEXmIEsNwY00pEsgI5nL1n0aJFCQkJcXaYUkopBxH5M7HnXJm6qQwcMsYcMcZcB2YDfnEPMMacMcYEAzdue+M8QA1gouO468aYC8nrvlJKqbvhStAXAo7H+Trc8ZgrigERwGQR+VVEJohIzmT2USml1F1wJeglgcdcrZuQBTtvP8YYUwG4DLyX4JuIdBGREBEJiYiIcPHllVJKOePKxdhw4NE4XxcGTrr4+uFAuDFmm+PrIBIJemPMOGAcgK+vb7xfJDdu3CA8PJyoqCgX31plJtmyZaNw4cJ4e3undVeUSndcCfpgoISI+AAngLZAe1de3Bjzl4gcF5FSxpgDQB1gn7N2CQkPDyd37twULVoUkYT+yFCZlTGGc+fOER4ejo+PT1p3R6l0x2nQG2OiRaQbsAK7vHKSMWaviHR1PD9WRAoCIUAeIFZEegBljDGRwDvATMeKmyNApzvpaFRUlIa8SpCIUKBAAXTKT6mEubSO3hizDFh222Nj43z+F3ZKJ6G2oYDvnXfxfzTkVWL0Z0OpxOmdsUoplS5sBr7C9bUurtOgT6YFCxYgIuzfv//WY+vWraNp06Z3/dqBgYEEBQUlecy6devYvHlzsl97+/bt1KpVixIlSlCxYkWaNGnC7t2777SrANSqVevWjW2NGzfmwoULd/Q6CxcuZN++O7p0o5SH2Ac0BcZiFyemLA36ZJo1axbPPfccs2fPTpP3v5OgP336NG3atOHTTz/l4MGD7Ny5k/79+3P48OF4x0ZHR99Rv5YtW0a+fPnuqK0GvcrcjgENgHuB5UCuFH8HDfpk+Oeff9i0aRMTJ06MF/SRkZG0bNmSMmXK0LVrV2JjY4mJiSEwMJCyZcvy1FNP8fXXXwMQGhpKlSpVKFeuHC1btuTvv/+O915Fixbl7NmzAISEhFCrVi2OHj3K2LFj+frrrylfvjy//PILERER+Pv7U6lSJSpVqsSmTZvivdaoUaMICAigWrVqtx577rnnaNGiBWD/kujVqxe1a9emX79+bN++nWrVqlGhQgWqVavGgQMHALh69Spt27alXLlyvPTSS1y9ejXB/s6YMYPKlStTvnx53njjDWJiYgDIlSsXAwYM4Omnn6ZKlSqcPn2azZs3s3jxYvr06UP58uUT/OWjlOc6iw35S9j1LsXc8i4uXYxNb3os70HoX6Ep+prlC5ZneMPhSR6zcOFCGjZsSMmSJbnvvvvYuXMnFStWBOzUyL59+3jsscdo2LAh8+fPx8fHhxMnTrBnzx6AW1MbHTt2ZOTIkdSsWZMPPviAQYMGMXx40u8NNky7du1Krly56N27NwDt27enZ8+ePPfccxw7dowGDRoQFhb2r3Z79+4lICAgydf+/fffWb16NV5eXkRGRrJhwwayZMnC6tWref/995k3bx5jxowhR44c7Nq1i127dt0697jCwsKYM2cOmzZtwtvbm7feeouZM2fSsWNHLl++TJUqVfjkk0/o27cv48eP5z//+Q/NmzenadOmtGrVyun3QCnP8Q/QBDgKrATKue2dMmTQp5VZs2bRo0cPANq2bcusWbNuhV3lypUpVsz+Nm7Xrh0bN26kTp06HDlyhHfeeYcmTZpQv359Ll68yIULF6hZsyYAAQEBtG7d+o77tHr16n9Ne0RGRnLp0iVy586daJtnn32WyMhI6tevz4gRIwBo3bo1Xl5eAFy8eJGAgAAOHjyIiHDjhi1htGHDBt59910AypUrR7ly8X8w16xZw44dO6hUqRJg/wp48MEHAciaNeutaxnPPPMMq1atuuPzVipjuw74AzuA+cDzbn23DBn0zkbe7nDu3DnWrl3Lnj17EBFiYmIQET7//HMg/vI+ESF//vz89ttvrFixgtGjRzN37txb0zfOZMmShdjYWIAk7waOjY1ly5YtZM+ePdFjnnzySXbu3Imfn61Ft23bNoKCgli6dOmtY3Lm/F8JooEDB1K7dm0WLFjA0aNHqVWr1r/OKynGGAICAvjvf/8b7zlvb+9b7b28vO74eoBSGVssEIAdxU8Cmrv9HXWO3kVBQUF07NiRP//8k6NHj3L8+HF8fHzYuHEjYKdu/vjjD2JjY5kzZw7PPfccZ8+eJTY2Fn9/f4YMGcLOnTvJmzcv+fPn55dffgFg+vTpt0b3cRUtWpQdO3YAMG/evFuP586dm0uXLt36un79+owaNerW16GhofFe6+2332bKlCn/uoh75cqVRM/14sWLFCpk69ZNmTLl1uM1atRg5syZAOzZs4ddu3bFa1unTh2CgoI4c+YMAOfPn+fPPxOtnprgOSnluQzQHVsEeCh3eP9osmnQu2jWrFm0bNnyX4/5+/vz/fffA1C1alXee+89ypYti4+PDy1btuTEiRPUqlWL8uXLExgYeGuUO3XqVPr06UO5cuUIDQ3lgw8+iPd+H374Id27d+f555+/NaUC0KxZMxYsWHDrYuw333xDSEgI5cqVo0yZMowdOzbeaxUsWJA5c+bQv39/Hn/8capVq0ZQUBDdunVL8Fz79u1L//79qV69+q0LqQBvvvkm//zzD+XKlePzzz+ncuXK8dqWKVOGjz/+mPr161OuXDnq1avHqVOnkvzetm3blmHDhlGhQgW9GKs83CfAKKAX0CfV3lWMSfnF+XfL19fX3L7xSFhYGE888UQa9UhlBPozotK374CuwCvAFFJ6nC0iO4wxCVYh0BG9Ukq5XRDwJtAYuw9T6kavBr1SSrnVcmzB36rAD0Dql9LWoFdKKbfZALQEngR+xIUts91Cg14ppdwiGFu/pih2KWW+NOuJBr1SSqW43djSBvcDq4EHnLaIio7i2MVjbumNBr1SSqWo34F6QHZgDVDIaYsbMTdo80Mbqk2sxqVrKX9PSYa8M1YppdKnY0BdIAb4GXC+tWVMbAyvLHiFJb8vYXTj0eS+N/HyJXdKR/TJpPXo/yc169H36dOH0qVL36r4eafvpZT7/IXdFjsSOyfv/J6OWBNL5yWdmbN3DsPqDeOtSm+5pWca9Mmk9egT5u569PXq1btVdqFkyZIJ1tJRKu2cw07XnMLuulrBaQtjDO8se4cpoVP4qOZH9K7W2229y6BB3wOolcIfPZy+q9ajT7t69PXr1ydLFjvTWKVKFcLDw53+/1IqdUQCDYGDwCKgWtKHY0O+3+p+fBvyLX2q9eGDmvHLoKSkDBr0aSOhevQ3bd++nS+//JLdu3dz+PBh5s+fT2ho6K169Lt376ZTJ1vAqGPHjgwdOpRdu3bx1FNPMWjQIJfe/2Y9+p49exIaGsrzzz9P9+7d6dmzJ8HBwcybN4/OnTvHa7d3794Ea8fHdbMe/Zdffknp0qXZsGEDv/76K4MHD+b9998H+Fc9+gEDBtwquhZX3Hr0oaGheHl53SqEdrMe/W+//UaNGjUYP3481apVo3nz5gwbNozQ0FCKFy/u9PswadIkGjVq5Mq3TCk3u1lTPhR7M1Qdl1oNXj+YYZuH8Xaltxlad6jbN7fPoBdjh6fJu2o9+rSvR//JJ5+QJUsWOnTokOy2SqWsy9h18puBWUAzl1p9vulzPlr/EYHlA/mm0TduD3nIsEGf+rQe/f/OKynurEc/depUli5dypo1a1LlH4dSibuCrSP/CzAdaONSqy83f0m/1f1oV7YdE5pN4B5JnUkVnbpxkdajT9t69MuXL2fo0KEsXryYHDnS5jZypawooAV2+eQUbB0b54ZvHU7vVb1p82QbprWchtc9Xs4bpRANehdpPfq0rUffrVs3Ll26RL169Shfvjxdu3ZN8jWVco9rwIvAKmwVyldcajVq+yh6ruiJ/xP+zGg5gyz3pO5kitajVx5Df0aUe10HWgFLgHHA6y61GhM8hreWvUXL0i2Z02oO3l7uqV551/XoRaShiBwQkUMi8l4Cz5cWkS0ick1E4i0GFREvEflVRJbe/pxSSqV/N4C22JD/FldDftyOcby17C2al2rO7Faz3Rbyzjj9+0FEvIDR2LsBwoFgEVlsjIl7h8t54F3sxFVCugNhQJ676q3yeG+//Xa8ewG6d+9+a2mqUqkvGugALAC+wW4g4tzEnRN5Y+kbNCnRhLmt5pLVK6sb+5g0VyaKKgOHjDFHAERkNuAH3Ap6Y8wZ4IyINLm9sYgUxi40/QS7UeIdM8boagsPN3r06Dtqlx6nIJUniMbOw/8AfAm841KrKaFTeH3J6zR8vCFBbYK4N8u9buyjc65M3RQCjsf5OhxXyrH9z3CgLxCbjDbxZMuWjXPnzuk/aBWPMYZz586RLVu2tO6K8ijRQEdgNjAUV8ep03+bzquLXqVusboseGkB2bKk/c+lKyP6hIbQLqWtiDQFzhhjdohILSfHdgG6ABQpUiTe84ULFyY8PJyIiAhX3lplMtmyZaNw4cJp3Q3lMW5gl00GAZ9hx6rOTf9tOoGLAqntU5tFbReli5AH14I+HHg0zteFgZMuvn51oLmINAayAXlEZIYx5uXbDzTGjMNeysbX1zfeLxJvb298fJyX/FRKqbtzHXgJWAh8BfR0qdWkXyfReXFnXvB5gcXtFpPdO/GbGFObK1M3wUAJEfERkazYS8+LXXlxY0x/Y0xhY0xRR7u1CYW8UkqlD1HYdfILgZG4GvLfhXzHa4tfo37x+ixpt4Qc3unrpj6nI3pjTLSIdANWAF7AJGPMXhHp6nh+rIgUBEKwq2piRaQHUMYYE+m+riulVEq6it3IewUwFnjDpVajto/inZ/eoUmJJgS1CUo30zVxZZgbppRSyn1u1q5ZC0wAXnWp1ddbvqbXyl60KN2COa3mpOkSyru+YUoppTzXP0BjbO2aqbga8kM3DqXXyl60LtM6zdfJO6NBr5TKxG5uGrIRmIGrtWuGrB/Ce2veo13Zdnzv/32a3fHqKi1TrJTKpC4AjbCXF2cBzveFMMbw4boPGbJhCK+Ue4XJfpNTtQrlndKgV0plQmexI/ld2LteWzhtYYzh/TXv89mmz3itwmt81/S7DBHyoEGvlMp0TgF1gcPY+jXxKrfEE2ti6f5Td0YFj6LrM10Z3WR0qm0akhI06JVSmchRbMifBn4CajttER0bTefFnZn621T+r+r/MazesAxXc0uDXimVSezHhvwVYDXwrNMW16Kv0X5+e+aHzWdwrcH8p8Z/MlzIgwa9UipT+BVogF1ouA6Iv7H97S5fv8yLc19k5eGVDG8wnO5Vuru3i26kQa+U8nCbsevk82JH8iWctrgYdZEm3zdhS/gWJjafyKsVXFtbn15p0CulPNhq7PYZhRyfx6+Me7uIyxE0nNmQ3ad3M9t/Nq2fdL7sMr3ToFdKeahFQBugNLASeMhpixORJ6g7vS5HLxxlUdtFNCrRyM19TB0a9EopDzQTCAB8satr8jttceTvI9SdVpezV86y4uUV1Hishpv7mHoyzkJQpZRyyXDgZaAGsApXQn7X6V08N+k5Iq9FsjZgrUeFPGjQK6U8hgH6Y2vI+wPLgNxOW234cwM1JtfgHrmH9YHr8X0kwQKQGZoGvVLKA0QDnbHb/r0BzMFuape0hfsXUn96fR7O/TCbX9vMkw8+6d5uphENeqVUBncVO4KfBHwAjMHukZS0CTsn4D/Xn/IFy7Ox00aK5HW+Iiej0qBXSmVgF7A3Qi0BRgGDgKTvXDXG8Okvn/L6ktepX7w+azquoUCOAm7vaVrSVTdKqQzqJLYC5X5gNnYpZdJiTSw9lvdg5PaRdHiqA5P9Jqf7WvIpQYNeKZUBHQTqY8sNL8PWsEna9ZjrBCwMYPae2fSs0pMv6n+RoSpQ3g0NeqVUBrMDu2EI2O3/nK+SuXTtEv5z/Vl1ZBWf1fmMvtX7ZsjiZHdKg14plYEsw+4E9QD2bteSTlv89c9fNP2+KaF/hTKp+SQ6Vejk5j6mPxr0SqkMYhzwFvA08CNQ0GmL/Wf302hmI85cPsPCtgtpWrKpm/uYPmnQK6XSOQP8B/gUO2UzF8jltNUvf/6C32w/vL28WRewjkqFKrm3m+lY5rgSoZTKoK4DHbEh3xlYjCshP2fPHOpOr8uDOR9k62tbM3XIgwa9Uirduogdwc8APsZO3SQ9CWGM4YvNX9B2XlsqF6rMplc34ZPfx/1dTed06kYplQ4dx24Wsh+YBrzitEVMbAzdl3dndPBo2jzZhqktppIti/MyCJmBSyN6EWkoIgdE5JCIvJfA86VFZIuIXBOR3nEef1REfhaRMBHZKyIZdy8upVQq+Q2oChwDluNKyF+5cQX/uf6MDh5N76q9meU/S0M+DqcjehHxAkYD9YBwIFhEFhtj9sU57DzwLtDitubRwP8ZY3aKSG5gh4isuq2tUko5rMLWrckDbASectrizOUzNJvVjOATwYxsNJJulbu5uY8Zjysj+srAIWPMEWPMdey9xn5xDzDGnDHGBAM3bnv8lDFmp+PzS0AYdk8vpZS6zVjsnHxRYCuuhHxYRBhVJ1Zl9+ndLHhpgYZ8IlwJ+kLYCbObwrmDsBaRokAFYFty2yqlPFkMtob8m9gCZRuBwk5brTq8iqoTq3L5+mV+DvgZv9J+TttkVq4EfUL3CZvkvImI5ALmAT2MMZGJHNNFREJEJCQiIiI5L6+UyrAuYScIhgPdsfu85nHaakzwGBrNbESRvEXY1nkbzxZ+1q29zOhcCfpw4NE4XxfGlo1ziYh4Y0N+pjFmfmLHGWPGGWN8jTG+DzzwgKsvr5TKsI4Bz2EvuH6LDfukLxvGxMbQY3kP3lr2Fg0fb8imVzfxWL7H3N7TjM6V5ZXBQAkR8QFOAG2B9q68uNiqQROBMGPMV3fcS6WUh9kONMduGvIjdsomaZHXImk3rx3LDi6jZ5WeDKs3DK97nG8wolwIemNMtIh0A1Zgt22ZZIzZKyJdHc+PFZGCQAj2b65YEekBlAHKYddG7RaRUMdLvm+MWZbiZ6KUyiDmAgHAw8BabFQk7c8Lf9J0VlPCIsIY02QMXX27urmPnsWlG6YcwbzstsfGxvn8LxK+erIRZ9u9KKUyCQN8AgwEqgMLsFUok7Y1fCt+s/24Fn2N5S8vp24x57Xn1b9pCQSlVCq4iq1ZMxB4GViNKyE/e89sak2pRa6sudjaeauG/B3SoFdKudkJoCa2Zs0QbEmDpO9ajTWxDFgzgHbz2lG5UGW2dd5G6ftLu7+rHkpr3Sil3Ggr0BL4B1jIbfdaJijyWiQvz3+ZJb8v4bUKrzG68WjuzXKve7vp4TTolVJuMgV4A3v5bhVQ1mmLg+cO4jfbj9/P/c6oRqN4q9JbmWrLP3fRoFdKpbBooDcwAqgDzAEKOG214tAK2s5ri5d4sbrjamoVreXWXmYmOkevlEpB57H1akYAPbA3QyUd8sYYvtz8JY2/b0yRvEUI6RKiIZ/CdESvlEohe7Fz8MeBSYDzTbiv3rhKl6VdmLFrBv5P+DOlxRRyZXW+g5RKHg16pVQKWIRdNpkLWIetJ5+08MhwWs5pScjJEIbUHsKA5wfofLybaNArpe5CDPARdqu/StiboJwXt914bCOt5rbi8o3LLHxpoVaedDOdo1dK3aHzQBNsyL8KbMBZyBtjGLF1BLWn1ibPvXnY+tpWDflUoCN6pdQd2IndCeokdtPuzjirdnL5+mVeX/I6s/bMwq+UH1NbTCVvtrzu76rSoFdKJdcU7CYh9wO/YDehS9rBcwd5ce6L7IvYx6cvfEq/5/pxj+iEQmrRoFdKuegadsnkWOAF7K6izuvVLNq/iI4LO+J9jzfLOyynXvF6bu2lik9/pSqlXBCOrVczFuiLrVqedMjHxMYwYM0AWsxpQckCJdnRZYeGfBrREb1SyomfgZewFSiDsHPzSTt75Szt5rVj9ZHVdK7QmZGNR5ItS9KFzJT7aNArpRIRCwwDBgAlgPnAE05bBZ8IptUPrTj9z2nGNxtP54qd3dtN5ZRO3SilEnAOu9Xfe8CL2K3/kg55Ywwjt42k+qTqAGx8daOGfDqhI3ql1G22YKdqTgOjgLdwtnTyYtRFXlv8GvPC5tG0ZFOmtpjKfdnvc39XlUs06JVSDgb4GugHPApsAnydttp5aidtfmjD0QtH+bzu5/xftf/TpZPpjAa9Ugr4G1uEbBHQApgM5EuyhTGGMSFj6LmiJw/keID1geupXqS6uzuq7oAGvVKZXjDQBruE8mugO86maiKvRdJlSRfm7J1Dw8cbMr3ldO7Pcb/7u6ruiAa9UpmWwc7B/x/wMLAReNZpq9/++o3WP7Tm8N+H9S7XDEKDXqlM6W/gdWAe0BSYCiR98dQYw/id43n3p3e5L/t9/BzwMzUeq+H+rqq7pkGvVKazCWiPLUj2OXZEn/SI/ELUBbos6cIP+36gXrF6zHhxBg/mfND9XVUpQoNeqUwjBvgEGAT4YAPfeUGyTcc20X5+e05eOslndT6jT/U+OlWTwWjQK5UpHMfuALXB8d/RQJ4kW8TExvDpL5/y0fqPeCzvY2zstJFnCzufw1fpj0u/lkWkoYgcEJFDIvJeAs+XFpEtInJNRHonp61Syt0WAE9ja8hPA6bjLOSPXzzOC9Ne4IN1H9C2bFtCu4ZqyGdgTkf0IuKF/fVfD7v+KlhEFhtj9sU57DzwLnYBbnLbKqXc4irQC1tx0heYBTzutNWCsAW8tvg1bsTeYGqLqbxS7hXdyzWDc2VEXxk4ZIw5Yoy5ji1C/a+9v4wxZ4wxwcCN5LZVSrnDHuwerjfLCm/CWchfvXGVN5e+yYtzX6RY/mLs7LKTjk931JD3AK7M0RfCTvDdFI4ri23vvq1SKtkM9o/oPkBebN34+k5b7T69m3bz2rE3Yi99qvXh4xc+JqtXVvd2VaUaV4I+oV/nxsXXd7mtiHQBugAUKVLExZdXSv3PSewm3SuARtgt/5JeAhlrYhmxdQT91/QnX7Z8rHh5BfWLO//FoDIWV4I+HFvh6KbC2J8oV7jc1hgzDrvLML6+vq7+IlFKAXZDkDew8/LfAl1xVsbg2MVjBC4M5OejP9O8VHPGNxuva+M9lCtBHwyUEBEf4ATQFnu3hSvupq1SyqmLwDvYlTSVgBlAySRbGGP4fvf3vL3sbWJMDBObT6RT+U46F+/BnAa9MSZaRLph/x70AiYZY/aKSFfH82NFpCAQgl2zFSsiPYAyxpjIhNq66VyUymTWAx2xY6gPsTtBeSfZ4vzV87z545vM3TuX6o9WZ1rLaRTLX8z9XVVpSoxJf7Mkvr6+JiQkJK27oVQ6dQ0YCHwBFMeO4p2vcVh5eCWdFnXizOUzDK41mL7V++J1j5d7u6pSjYjsMMYkuIGA3hmrVIayG3tn6y7sPPwXQM4kW1y5cYV+q/oxKngUT9z/BEvaLaHiwxXd31WVbmjQK5UhRANfYUfy+YGlQBOnrUJOhvDKglfYf3Y/3Z/tzn/r/Jfs3tnd21WV7mjQK5Xu7QcCgW3YjbrHAg8k2eJa9DUGrx/M0E1DKZirIKteWUXdYnXd3lOVPmnQK5VuxWB3fPoPdnpmFnbT7qRXx4ScDCFwYSB7I/YSWD6Qrxt8Tb5s+dzdWZWOadArlS79jh3Fb8FWDRkLFEyyRdxR/EO5HuLH9j/SuERjt/dUpX8a9EqlKzHAN8D7QHbsipr2uDKK77SoE3vO7NFRvIpHg16pdOMg0AlbgKwZ8B12L9fE3T6KX9puKU1KOr9IqzIXDXql0lwMdpPu/sC92P1bX8HZKH7HyR0ELgpkz5k9BDwdwNcNviZ/9vxu763KeDTolUpTe4HOwFagMbbcU6EkW0RFRzFk/RAdxSuXadArlSauAf8FPsVWDpkOdMDZKH790fW8vuR1Dp4/qKN45TINeqVS3RbsKH4f9kLrcJyti78QdYG+q/oyfud4fPL5sPLlldQrXs/tPVWeQYNeqVRzCVt4bBS2Yrdrd7fOD5vP28ve5szlM/Sp1oePan1EDu8c7u2q8iga9Eqlip+wtWmOA29jp2xyJ9niROQJuv3UjYX7F1KhYAWWtlvKM4884/6uKo+jQa+UW0UAPYGZwBPARqBaki1iTSzjdoyj3+p+XI+5zud1P6dn1Z5kuUf/uao7oz85SrmFAaYBvbGbg3yAvQnq3iRb7T+7n9eXvM7GYxt5wecFvmv6HY/fl/Sm3ko5o0GvVIoLA97EbgxSFbtksmySLaKio/jvL//ls02fkdM7J5P9JhPwdIDu+qRShAa9UinmCvAJMAzIBYzHbtZ9T5KtVh5eydvL3ubQ+UO0K9uOrxt8zUO5HnJ7b1XmoUGvVIpYBnQD/sBu7zcMSHqj7ZOXTtJzRU/m7p1LyQIlWf3KauoUq+P+rqpMR4NeqbtyAugBBAGlgZ+BWkm2iI6NZvT20Qz8eSDXY67f2tbv3ixJz98rdac06JW6I9HAaGyt+GjgY6APkDXJVtvCt/Hmj2/y61+/0vDxhoxqNIri9xV3e29V5qZBr1SybcGuhf8VaIi9ASrpsP776t/0X9OfcTvG8XDuh/mh9Q/4P+GvF1tVqtCgV8plfwHvYatLPgLMBVqRVH2aWBPL1NCp9Fvdj3NXz9GjSg8G1RpE7nuTvllKqZSkQa+UUzewo/YPgSigH3bKJleSrYJPBPPOT++w7cQ2qhauysomKylfsLy7O6tUPBr0SiVpDfAutgBZQ2AEUDLJFmcun6H/6v5MCp1EwVwFmdZiGh3KdeAeSXqZpVLuokGvVIKOAf+HXU3jAyzC7vqU+DTNjZgbfBv8LR+u+5DLNy7Tu2pvBtYcSJ5786RKj5VKjAa9Uv8SBXyBLTpmgMHYMgbZk2y19o+1vPvTu+yN2EuD4g0Y3nA4pe8v7fbeKuUKl/6WFJGGInJARA6JyHsJPC8i8o3j+V0iUjHOcz1FZK+I7BGRWSKSLSVPQKmUYbCj9ieBgdjdnvY7Pk885I9dPEbrH1pTZ1odrty4wsKXFvJTh5805FW64jToRcQLu2C4EVAGaCciZW47rBFQwvHRBRjjaFsIO8Hpa4wpC3gBbVOs90qliFCgDtACW3RsFXbK5rFEW1y+fplB6wZRelRpfvz9R4bUHsK+t/fhV9pPl0yqdMeVqZvKwCFjzBEAEZkN+GGvTt3kB0wzxhhgq4jkE5Gb29dnAbKLyA0gB3AyxXqv1F35C7t6ZhKQHxgJvAF4J9oi1sQyY9cM3l/zPicunaB1mdZ8Uf8LiuQtkio9VupOuBL0hbC7JdwUDjzrwjGFjDEhIvIF9srWVWClMWblXfRXqRQQBXyNnYePwpYwGIgN+8Rt+HMDvVb0YsepHVR6pBJzWs2hepHq7u6sUnfNlTn6hP4ONa4cIyL5saN9H+wdJjlF5OUE30Ski4iEiEhIRESEC91SKrkMMAdbk+Z97HTNPuArkgr5w+cP4z/Xn5pTanL68mlmtJzB1s5bNeRVhuFK0IcDj8b5ujDxp18SO6Yu8IcxJsIYcwOYTyLb6xhjxhljfI0xvg88kPRGyUolXzDwPPYSUT7s+viF2MtKCbsQdYHeK3vzxOgnWHFoBUNqD+FAtwO6Jl5lOK5M3QQDJUTEB1uqry126/q4FgPdHPP3zwIXjTGnROQYUEVEcmCnbuoAISnWe6Wc+hM7Dz8DeAhbI74Tdl1AwqJjo/ku5Ds+XPch56+eJ7B8IB+/8DGP5H4kVXqsVEpzGvTGmGgR6QaswP7rmGSM2SsiXR3Pj8UW424MHMLuvtDJ8dw2EQkCdmJL/P2K3W5HKTc7j52DH4n9w7W/4yPxGjPGGBYfWEz/Nf0JOxtGraK1+Kr+V1R4uEKq9FgpdxG7UCZ98fX1NSEhOvBXd+Iq8A3wX+ASEIC96alwkq02HdtEv9X92HR8E6UKlGJo3aE0L9Vcl0qqDENEdhhjfBN6Tu+MVR4iBrsZ9wfYS0ZNgM9wtlfrvoh9vL/mfRYdWMTDuR7mu6bf8WqFV8lyj/7TUJ5Df5pVBmewM4fvAXuAStj5+JpJtjoReYIP133I5NDJ5PTOyce1P6ZHlR7kzJrT7T1WKrVp0KsMbDvQF1gPPI4r9eEvRF1g6MahDN82nJjYGN6t/C4Dagzg/hz3p0qPlUoLGvQqA9qLrQ0/D3gAe8G1C0lt4xcVHcW3wd/yyS+fcP7qeTo81YEhtYfgk98nVXqsVFrSoFcZyEHgI2AWdtOPD7ClhBMvA3wj5gZTQqcwZMMQjkcep37x+nxW5zNdSaMyFQ16lQEcBYZgt/DLip2u6QMUSLRFTGwMs/bM4qN1H3H478M8W+hZJvlNom6xuqnRYaXSFQ16lY6dAD4BJmDXwnfDXnQtmGiLWBPL/LD5fPDzB4SdDePph55mSbslNCnRRJdKqkxLg16lQ6exSyPHYJdNdgYGkNRaeGMMyw4uY+DPA/n1r18pfX9p5raai38Zfy1XoDI9DXqVjpzD7u70DbaqZAC2qmTSF0zX/rGW/6z9D1vCt1AsfzGmtZhG+6fa43VP4mUOlMpMNOhVOvA3dtPtr4B/gHbYVTVJb8K96dgmBv48kJ+P/kzhPIUZ13QcgeUD8fZKvJ68UpmRBr1KQ+eA4dgRfCTQEluuIOm7WdcfXc/gDYNZ+8daHsr5ECMajqDLM13IlkV3qVQqIRr0Kg2cxY7eR2JH8P7YCpPlE21hjGHd0XUMWj+I9X+up2CugnxV/yve8H2DHN45UqPTSmVYGvQqFZ3BzsF/iy1y2gYb8ImP4I0xrD6ymsEbBrPx2EYeyf0IIxqO4PWKr5PdO/FNu5VS/6NBr1LBX8Aw7Cqaa9g5+AHAE4m2MMaw4vAKBq8fzJbwLRTOU5hRjUbxWsXXdIpGqWTSoFdudBIYit2C4AbQAbuFX6lEW9xcJjl4w2C2n9hOkbxFGNNkDJ3Kd+LeLPemSq+V8jQa9MoNDgGfY+9kjcEuk+yPLTyWsJjYGIL2BTF001B+/etXiuYryrim4wgoH0BWr8Rr2CilnNOgVynoV+wI/gfAG3gVW6qgWKItoqKjmBo6lWGbh3H478OUKlCKic0n8kq5V3SZpFIpRINe3SUDbMDeybocu1VfH6A78HCirS5GXWRMyBiGbx3O6cunqVyoMsPqDcOvtJ/eyapUCtOgV3coFvgRu2XfFmy54E+BN4F8ibY6dekUI7aNYEzIGCKvRdKgeAP6Ve9HraK1tBaNUm6iQa+S6QYwBztFswd4DBiFnaZJfLnjofOHGLZpGFN+m0J0bDSty7SmX/V+Wi5YqVSgQa9cdAmYDHyNLRv8JDAdeAk7H5+wHSd3MHTTUOaFzcP7Hm86le9E72q9efy+xC/MKqVSlga9ciIcW6JgHHARqO74ugm2dHB8sSaW5YeW8+WWL1n7x1ry3JuHvtX60r1KdwrmSrzEsFLKPTToVSJ2YMsUzMVecG0F9ASeTbTF1RtXmbFrBl9t/Yr9Z/dTKHchhtYdyhvPvEHebHlTpddKqfg06FUcscBS4EvsSprcwLuOj8cSbXXm8hm+Df6Wb4O/JeJKBBUKVmBGyxm0ebKNLpFUKh3QoFfAZezNTcOx+7IWwYZ9Z5Laj3VfxD6+2vIVM3bN4FrMNZqVbEavqr2o+VhNXUGjVDqiQZ+pncKumBkLnAcqY1fUvEhiPxrGGNb8sYavtnzFT4d+IluWbHQq34keVXpQ6v7ESxsopdKOS0EvIg2xO0N4AROMMZ/d9rw4nm+MLUsYaIzZ6XguH3bTz7LYyd5XjTFbUuoEVHIZ7Lr3kUAQtkRBS6AXUA1IeCR+6dolZu6eybfB37L7zG4eyvkQQ2oPoatvV+7PcX8q9V0pdSecBr2IeAGjgXrYJRjBIrLYGLMvzmGNgBKOj2exZQpvXrUbASw3xrQSkayAFg9PE1HY0fpI7IXWvMA7wNtA8URb7Tq9izHBY5ixewb/XP+H8gXLM7H5RNo/1V6rSCqVQbgyoq8MHDLGHAEQkdmAHxA36P2AacYYA2wVkXwi8jB28rcGEAhgjLkOXE+57ivnwrG/d8dhN/wo4/j6ZSBXgi2ioqMI2hfEmJAxbD6+mWxZsvHSky/xpu+bVC5UWefflcpgXAn6QsDxOF+HE3+NXULHFAKigQhgsog8jR1KdjfGXL7jHisXGOAX7Oh9AXY1TXPsCP4FEpueOXz+MGNDxjI5dDLnrp6jxH0l+LL+lwSWD+S+7PelUt+VUinNlaBPKBWMi8dkASoC7xhjtonICOA9YGC8NxHpAnQBKFKkiAvdUvFdAWZhA/43ID927ftbgE+CLaJjo1n6+1LGhIxh5eGVeIkXLUq3oKtvV17weUELjCnlAVwJ+nDg0ThfF8buKOHKMQYIN8ZsczwehA36eIwx47DzC/j6+t7+i0QlaR/wHXaJ5EXgKey3sgOJXRI5eekkE3ZOYPzO8YRHhlModyEG1RpE54qdeST3I6nVcaVUKnAl6IOBEiLiA5wA2gLtbztmMdDNMX//LHDRGHMKQESOi0gpY8wBoA7/nttXd+waMB+7NHIDtt5MK+AN7GWR+H9kxZpY1v6xljEhY1i0fxExJoYGxRswstFImpZsSpZ7dLWtUp7I6b9sY0y0iHQDVmCXV04yxuwVka6O58cCy7BLKw9h5w86xXmJd4CZjhU3R257TiXbYexofRL24moxbCXJQODBBFucv3qeKaFTGBsyloPnD1IgewF6Ve3FG8+8QfH7El9xo5TyDGIXyqQvvr6+JiQkJK27kY5EA0uwo/eV2N+3zYGuQF0SKi4Wa2JZd3QdE3ZOYH7YfK7FXKPao9V40/dNWpVppUsjlfIwIrLDGOOb0HP6t3q6dhRbGngC9pJHYWAw8BqQ8Dz6qUunmPrbVCbsnMDhvw+TL1s+ujzThc4VO1PuoXKp1G+lVHqiQZ/uXMXOvU8G1mDn2hthR/ONSOh/WUxsDCsOr2D8zvEsObCEGBNDzcdqMqjWIF584kWyeye+IYhSyvNp0KcLBnvNezJ2eeRF7HLIwUAAtshYfH9e+JNJv05iUugkwiPDeSDHA/Sq2ovOFTtTskDJVOq7Uiq906BPU2eAGdgLq3uxW/G1wl6vrklCc+/Xoq+x5PclTNg5gZWHVwJQv3h9hjcYTrNSzcjqlTW1Oq+UyiA06FPdDeAn7Oh9KfZCaxXsOviXsDVo/s0YQ8jJEKaETmHWnln8HfU3hfMUZmCNgXSq0Imi+YqmXveVUhmOBn2qCcOG+zTgNPAQ0AM7ei+TYIuTl04yY9cMpoROIexsGNmyZKNl6ZYEPB1A3WJ18brHK5X6rpTKyDTo3eosMBu7ifZ27Le7CfAq9sJq/N2XoqKjWLR/EVN+m8LKwyuJNbFUe7Qa45qOo82TbXRLPqVUsmnQp7go7Jr36dgpmmjgaeALbMXIh+K1MMaw7cQ2poROYfae2Vy8dpFH8zxK/+f60/HpjnphVSl1VzToU0QssBEb7j9gV808gp2aeQVIeP364fOH+X7398zcPZMD5w6QPUt2/Mv4E/h0ILV9amtBMaVUitCgvysHsOE+A/gTyIndhu8VbDng+HPoZy6fYc6eOXy/53u2hm8FoMZjNehbvS+tyrQiz72J79GqlFJ3QoM+2U4Dc7EBH4xdAlkX+Bi7JV/OeC0uXbvEwv0L+X7P96w6vIoYE0O5h8oxtO5Q2pVtx6N5H43XRimlUooGvUvOYzfwmAX8jJ2quTnv3h54OF6L6zHXWXl4JTN3z2TR/kVcjb7KY3kfo2/1vrR/qj1lHyybiv1XSmVmGvSJuoStvjwbW7jzBvA48D62UvOT8VrExMaw6fgmZu2exQ/7fuDc1XMUyF6AwPKBdHiqA1Ufrarz7kqpVKdB/y9XsRWXZ2NvZorC7qfSHRvuFbm9znt0bDTrj64naF8QC/Yv4PTl02TPkh2/0n50eKoD9YvX17tVlVJpSoOe68BqbLgvxI7kHwQ6Y8O9KreXIrgec521f6wlaF8QC/cv5NzVc+TwzkGTEk3wf8KfJiWbkCtrwhtvK6VUasukQX8NWAXMAxYBfwP5gDbYcK/F7d+aqOgoVh1eRVBYEIsPLOZC1AVyZ81Ns1LN8H/Cn4aPNySHd8Lb9imlVFrKREF/BTvXHoSdlonE1pVpDrQGGgD/nmK5cuMKyw8tZ17YPJYcWMKl65fIly0ffqX8aFWmFXWL1dUNPJRS6Z6HB/0/wI/YkfuP2LAvgA32Vti17v8O93+u/8OPv//IvLB5/HjwR67cuEKB7AV46cmX8C/jzws+L+icu1IqQ/HAoL+ILUEQhB3BR2HLDgQA/tjyv/8+7YjLEfx48EcW7l/IisMriIqO4qGcDxHwdAD+T/hTs2hN3ThbKZVheVB6XcbOsa/CLoUsBHTBjtyrcftdqgfOHmDxgcUsOrCIzcc3YzAUzlOYLhW70KpMK6o9Wk2rQyqlPIIHBf3NO1K7Y0fulYm7WiYmNoYt4Vtuhfvv534HoELBCnxQ8wP8SvlRvmB5ROT2F1ZKqQzNg4Ie7Dz8/1y+fplVR1ax6MAilv6+lLNXzuJ9jze1itbi3crv0qxUM4rkTXibPqWU8hQeFvRw6tIplv6+lEUHFrH6yGquxVwjX7Z8NC7RGL9SfjQo3kBruiulMhWPCforN67wwtQX2HZiGwBF8xWlq29XmpdqzvNFnsfbK/4mH0oplRl4TNDn8M5BiQIlaFayGc1LNafsg2V1vl0ppfCgoAeY3nJ6WndBKaXSHZdKKYpIQxE5ICKHROS9BJ4XEfnG8fwuEal42/NeIvKriCxNqY4rpZRyjdOgFxEvYDR2N+syQDsRKXPbYY2AEo6PLsCY257vDoTddW+VUkolmysj+srAIWPMEWPMdWyZR7/bjvEDphlrK5BPRB4GEJHCQBNgQgr2WymllItcCfpCwPE4X4c7HnP1mOFAX+y2TEoppVKZK0Gf0NIV48oxItIUOGOM2eH0TUS6iEiIiIRERES40C2llFKucCXow7HbLN1UGDjp4jHVgeYichQ75fOCiMxI6E2MMeOMMb7GGN8HHnjAxe4rpZRyxpWgDwZKiIiPiGTF7syx+LZjFgMdHatvqgAXjTGnjDH9jTGFjTFFHe3WGmNeTskTUEoplTSn6+iNMdEi0g1b89cLmGSM2SsiXR3Pj8VutNoYOIQt+t7JfV1WSimVHGLM7dPtaU9EIoA/77D5/cDZFOxORqDnnDnoOXu+uznfx4wxCc57p8ugvxsiEmKM8U3rfqQmPefMQc/Z87nrfF26M1YppVTGpUGvlFIezhODflxadyAN6DlnDnrOns8t5+txc/RKKaX+zRNH9EoppeLwmKB3Vko5oxKRR0XkZxEJE5G9ItLd8fh9IrJKRA46/ps/Tpv+ju/DARFpkHa9vzu3l7f29HMWkXwiEiQi+x3/v6tmgnPu6fi53iMis0Qkm6eds4hMEpEzIrInzmPJPkcReUZEdjue+0aSs7OSMSbDf2Bv5DoMFAOyAr8BZdK6Xyl0bg8DFR2f5wZ+x5aL/hx4z/H4e8BQx+dlHOd/L+Dj+L54pfV53OG59wK+B5Y6vvbocwamAp0dn2cF8nnyOWMLH/4BZHd8PRcI9LRzBmoAFYE9cR5L9jkC24Gq2NpiPwGNXO2Dp4zoXSmlnCEZW0pip+PzS9i6/oWw5zfVcdhUoIXjcz9gtjHmmjHmD+zdypVTtdMpIJHy1h57ziKSBxsIEwGMMdeNMRfw4HN2yAJkF5EsQA5sjSyPOmdjzAbg/G0PJ+scHWXf8xhjthib+tPitHHKU4LelVLKGZ6IFAUqANuAh4wxp8D+MgAedBzmKd+L4cQvb+3J51wMiAAmO6arJohITjz4nI0xJ4AvgGPAKWyNrJV48DnHkdxzLOT4/PbHXeIpQe9KKeUMTURyAfOAHsaYyKQOTeCxDPW9SE5565tNEngsQ50zdmRbERhjjKkAXMb+SZ+YDH/OjnlpP+wUxSNAThFJquhhhj9nFyR2jnd17p4S9K6UUs6wRMQbG/IzjTHzHQ+fjrOL18PAGcfjnvC9SKy8tSefczgQbozZ5vg6CBv8nnzOdYE/jDERxpgbwHygGp59zjcl9xzDHZ/f/rhLPCXoXSmlnCE5rqxPBMKMMV/FeWoxEOD4PABYFOfxtiJyr4j4YPfx3Z5a/U0JJvHy1p58zn8Bx0WklOOhOsA+PPicsVM2VUQkh+PnvA72GpQnn/NNyTpHx/TOJRGp4vhedYzTxrm0viKdgle2G2NXpBwGBqR1f1LwvJ7D/om2Cwh1fDQGCgBrgIOO/94Xp80Ax/fhAMm4Mp8eP4Ba/G/VjUefM1AeCHH8v14I5M8E5zwI2A/sAaZjV5t41DkDs7DXIG5gR+av3ck5Ar6O79NhYBSOG15d+dA7Y5VSysN5ytSNUkqpRGjQK6WUh9OgV0opD6dBr5RSHk6DXimlPJwGvVJKeTgNeqWU8nAa9Eop5eH+Hw9g6gUtSVwPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_epochs, SGD.grad_history, label = 'Absolute Gradient', color = 'green')\n",
    "plt.plot(test_epochs, SGD.grad_2_history, label = 'Absolute Gradient_2', color = 'yellow')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 2)\n",
      "1276\n",
      "(1279, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5324472243940579"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def training_accuracy(model, x_train,y_train):\n",
    "    y_pred = model.predict(x_train)\n",
    "    y_new = y_train.reshape(y_pred.shape)\n",
    "    return np.sum(y_new == y_pred) / y_new.shape[0]\n",
    "\n",
    "print(np.sum(Model.predict(x_train)))\n",
    "training_accuracy(Model, x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54375"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_accuracy(model, x_test, y_test, mu, sigma):\n",
    "    y_pred = model.predict((x_test - mu) / sigma)\n",
    "    y_new = y_test.reshape(y_pred.shape)\n",
    "    return np.sum(y_new == y_pred) / y_new.shape[0]\n",
    "\n",
    "x_test_new = x_test\n",
    "x_test_new = x_test_new \n",
    "test_accuracy(Model, x_test_new, y_test, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 2)\n",
      "training accuracy =  46.98983580922596 %\n"
     ]
    }
   ],
   "source": [
    "print(\"training accuracy = \", training_accuracy(NN, x_train, y_train)* 100 ,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45625"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"test accuracy = \", test_accuracy(NN, x_test, y_test, mu, sigma),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8201e9342a4a492ddbd4e81efec90b2ccf0d205cda2cc39ac893f0c43374b5e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
