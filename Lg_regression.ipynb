{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Models.Layers import *\n",
    "from Solver import *\n",
    "from Models.Classifiers.Logistic_Classifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n",
      "320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>percentage_free_sulphur</th>\n",
       "      <th>n_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>30.909091</td>\n",
       "      <td>0.6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>0.8290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.7440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "      <td>35.294118</td>\n",
       "      <td>0.7195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>30.909091</td>\n",
       "      <td>0.6080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.6610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1</td>\n",
       "      <td>13.076923</td>\n",
       "      <td>0.7110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.793103</td>\n",
       "      <td>0.7540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.6615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>1.2075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  percentage_free_sulphur  n_value  \n",
       "0         9.4        0                30.909091   0.6080  \n",
       "1         9.8        0                26.800000   0.8290  \n",
       "2         9.8        0                36.000000   0.7440  \n",
       "3         9.8        1                35.294118   0.7195  \n",
       "4         9.4        0                30.909091   0.6080  \n",
       "...       ...      ...                      ...      ...  \n",
       "1594     10.5        0                13.750000   0.6610  \n",
       "1595     11.2        1                13.076923   0.7110  \n",
       "1596     11.0        1                13.793103   0.7540  \n",
       "1597     10.2        0                13.750000   0.6615  \n",
       "1598     11.0        1                23.333333   1.2075  \n",
       "\n",
       "[1599 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first step would be to import the dataset\n",
    "X_full = pd.read_csv('./Datasets/red_wine_dataset.csv')\n",
    "percentage = 0.8\n",
    "X_full.pop('k_value')\n",
    "X_full.pop('l_value')\n",
    "X_full.pop('m_value')\n",
    "X_train = X_full.sample(frac=percentage, random_state=0)\n",
    "y_train = X_train.pop('quality')\n",
    "X_test = X_full.drop(X_train.index)\n",
    "y_test = X_test.pop('quality')\n",
    "print(len(X_train.index))\n",
    "print(len(X_test.index))\n",
    "X_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 13)\n",
      "(320, 13)\n",
      "float64\n",
      "(1279,)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train.to_numpy()\n",
    "x_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_train.dtype)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #classic way to plot the data\n",
    "# for i in range(x_train.shape[0]):\n",
    "#     if(y_train[i] == 1):\n",
    "#         plt.scatter(x_train[i,0], x_train[i,1], color='red')\n",
    "#     else:\n",
    "#         plt.scatter(x_train[i,0], x_train[i,1], color='blue')\n",
    "# plt.xlabel('fixed acidity')\n",
    "# plt.ylabel('volatile acidity')\n",
    "# plt.title('Red Wine Dataset')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#quicker way to plot the data\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=50, cmap='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1279, 13)\n",
      "(1279,)\n",
      "0.12433391471702653\n",
      "0.10003662221022867\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Normalizing the data (mean=0, std=1)\n",
    "x_max = np.max(x_train, axis = 0)\n",
    "x_min = np.min(x_train, axis = 0)\n",
    "x_train = (x_train - x_min) / (x_max - x_min)\n",
    "mu = x_min\n",
    "sigma = (x_max - x_min)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=50, cmap='spring')\n",
    "print(x_train[:,3].mean())\n",
    "print(x_train[:,3].std())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old loss =  [5.61635667]\n",
      "[[0.99199222]\n",
      " [1.32586469]\n",
      " [1.65617951]\n",
      " [0.4061653 ]\n",
      " [1.37783006]]\n",
      "new loss =  [4.90119085]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "w = np.random.randn(x_train.shape[1], 1)\n",
    "b = np.zeros((1, 1))\n",
    "op, cache_sigmoid = sigmoid_forward(x_train[15:20], w, b )\n",
    "# print(op)\n",
    "loss,cache_logistic = cross_entropy_loss(op, y_train[15:20])\n",
    "print(\"old loss = \", loss)\n",
    "grads = cross_entropy_loss_backward(cache_logistic)\n",
    "# print(grads.shape,'\\n', grads)\n",
    "dw, db = sigmoid_backward(grads, cache_sigmoid)\n",
    "# print(dw.shape)\n",
    "# print(db.shape)\n",
    "b = b - 0.1 * db\n",
    "w = w - 0.1 * dw\n",
    "op_new,cache_sigmoid = sigmoid_forward(x_train[15:20], w, b)\n",
    "print(np.dot(x_train[15:20],w) + b)\n",
    "# print(op_new)\n",
    "loss, cache_logistic = cross_entropy_loss(op_new, y_train[15:20].reshape(-1,1))\n",
    "print(\"new loss = \",loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplelogistic = Logisitic_Classifier(input_dim=x_train.shape[1], reg = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  0 Batch =  0 Loss =  [10.40273634] Gradient_max =  0.8116065866873277 learning rate ratio =  0.9968882763653846\n",
      "Epoch =  1 Batch =  0 Loss =  [9.33805528] Gradient_max =  0.9306079108079708 learning rate ratio =  0.10031516066398921\n",
      "Epoch =  2 Batch =  0 Loss =  [8.99050005] Gradient_max =  0.6734765766433983 learning rate ratio =  1.333555603683306\n",
      "Epoch =  3 Batch =  0 Loss =  [8.85277777] Gradient_max =  0.5644032299870778 learning rate ratio =  0.06484107768738158\n",
      "Epoch =  4 Batch =  0 Loss =  [8.77025278] Gradient_max =  0.5254509298750879 learning rate ratio =  0.010888361367658493\n",
      "Epoch =  5 Batch =  0 Loss =  [8.71559886] Gradient_max =  0.5089820325079217 learning rate ratio =  0.2750082077326142\n",
      "Epoch =  6 Batch =  0 Loss =  [8.67649461] Gradient_max =  0.4974067132706609 learning rate ratio =  0.022000405585996186\n",
      "Epoch =  7 Batch =  0 Loss =  [8.64663803] Gradient_max =  0.4890986706301206 learning rate ratio =  0.011881029644118135\n",
      "Epoch =  8 Batch =  0 Loss =  [8.6226553] Gradient_max =  0.4830806108941598 learning rate ratio =  0.008460091388344207\n",
      "Epoch =  9 Batch =  0 Loss =  [8.60266116] Gradient_max =  0.47871416991045584 learning rate ratio =  0.006795520630340448\n",
      "Epoch =  10 Batch =  0 Loss =  [8.58554569] Gradient_max =  0.4755610181326643 learning rate ratio =  0.006488332233578784\n",
      "Epoch =  11 Batch =  0 Loss =  [8.57061532] Gradient_max =  0.47330981681017725 learning rate ratio =  0.007657550463405841\n",
      "Epoch =  12 Batch =  0 Loss =  [8.5574092] Gradient_max =  0.47173408482303136 learning rate ratio =  0.00926562487113749\n",
      "Epoch =  13 Batch =  0 Loss =  [8.54560311] Gradient_max =  0.4706662593140622 learning rate ratio =  0.01160975711312662\n",
      "Epoch =  14 Batch =  0 Loss =  [8.53495751] Gradient_max =  0.4699808814908023 learning rate ratio =  0.015335541659096476\n",
      "Epoch =  15 Batch =  0 Loss =  [8.52528848] Gradient_max =  0.46958323294855253 learning rate ratio =  0.022158401084740917\n",
      "Epoch =  16 Batch =  0 Loss =  [8.51645069] Gradient_max =  0.469401395833804 learning rate ratio =  0.03864537550892596\n",
      "Epoch =  17 Batch =  0 Loss =  [8.50832695] Gradient_max =  0.4693805596755638 learning rate ratio =  0.13474999146686156\n",
      "Epoch =  18 Batch =  0 Loss =  [8.50082132] Gradient_max =  0.469478857678725 learning rate ratio =  0.0039835344570320836\n",
      "Epoch =  19 Batch =  0 Loss =  [8.49385453] Gradient_max =  0.4696642759680505 learning rate ratio =  0.003979004774633832\n",
      "Epoch =  20 Batch =  0 Loss =  [8.48736054] Gradient_max =  0.46991233368721386 learning rate ratio =  0.003984911085302904\n",
      "Epoch =  21 Batch =  0 Loss =  [8.48128406] Gradient_max =  0.47020432726560113 learning rate ratio =  0.003991575627265687\n",
      "Epoch =  22 Batch =  0 Loss =  [8.4755786] Gradient_max =  0.47052599347858054 learning rate ratio =  0.003998734777633427\n",
      "Epoch =  23 Batch =  0 Loss =  [8.47020487] Gradient_max =  0.4708664867474777 learning rate ratio =  0.004006190045806239\n",
      "Epoch =  24 Batch =  0 Loss =  [8.46512957] Gradient_max =  0.4712175941503736 learning rate ratio =  0.00401379240873924\n",
      "Epoch =  25 Batch =  0 Loss =  [8.46032438] Gradient_max =  0.4715731313581876 learning rate ratio =  0.0040214305333842815\n",
      "Epoch =  26 Batch =  0 Loss =  [8.45576505] Gradient_max =  0.47192847691616535 learning rate ratio =  0.0040290218637235925\n",
      "Epoch =  27 Batch =  0 Loss =  [8.45143081] Gradient_max =  0.4722802126859353 learning rate ratio =  0.004036505841005983\n",
      "Epoch =  28 Batch =  0 Loss =  [8.44730368] Gradient_max =  0.472625845971349 learning rate ratio =  0.004043838726028992\n",
      "Epoch =  29 Batch =  0 Loss =  [8.44336812] Gradient_max =  0.4729635946260881 learning rate ratio =  0.004050989632773885\n",
      "Epoch =  30 Batch =  0 Loss =  [8.43961053] Gradient_max =  0.4732922208018857 learning rate ratio =  0.004057937482976143\n",
      "Epoch =  31 Batch =  0 Loss =  [8.436019] Gradient_max =  0.4736109023095091 learning rate ratio =  0.004064668663886135\n",
      "Epoch =  32 Batch =  0 Loss =  [8.43258299] Gradient_max =  0.4739191330939738 learning rate ratio =  0.004071175224817584\n",
      "Epoch =  33 Batch =  0 Loss =  [8.42929316] Gradient_max =  0.4742166462632439 learning rate ratio =  0.004077453487653637\n",
      "Epoch =  34 Batch =  0 Loss =  [8.42614112] Gradient_max =  0.47450335459860093 learning rate ratio =  0.004083502976095741\n",
      "Epoch =  35 Batch =  0 Loss =  [8.42311935] Gradient_max =  0.4747793046213578 learning rate ratio =  0.004089325590762498\n",
      "Epoch =  36 Batch =  0 Loss =  [8.42022101] Gradient_max =  0.4750446411751502 learning rate ratio =  0.004094924974169716\n",
      "Epoch =  37 Batch =  0 Loss =  [8.41743989] Gradient_max =  0.4752995801662323 learning rate ratio =  0.004100306022512483\n",
      "Epoch =  38 Batch =  0 Loss =  [8.41477027] Gradient_max =  0.47554438763287277 learning rate ratio =  0.004105474511027677\n",
      "Epoch =  39 Batch =  0 Loss =  [8.41220691] Gradient_max =  0.4757793637240988 learning rate ratio =  0.004110436807274341\n",
      "Epoch =  40 Batch =  0 Loss =  [8.40974493] Gradient_max =  0.47600483048520925 learning rate ratio =  0.00411519965248268\n",
      "Epoch =  41 Batch =  0 Loss =  [8.4073798] Gradient_max =  0.47622112259347193 learning rate ratio =  0.004119769995601763\n",
      "Epoch =  42 Batch =  0 Loss =  [8.40510727] Gradient_max =  0.476428580378268 learning rate ratio =  0.0041241548681332256\n",
      "Epoch =  43 Batch =  0 Loss =  [8.40292339] Gradient_max =  0.47662754460822016 learning rate ratio =  0.004128361290511186\n",
      "Epoch =  44 Batch =  0 Loss =  [8.4008244] Gradient_max =  0.4768183526430285 learning rate ratio =  0.004132396202857467\n",
      "Epoch =  45 Batch =  0 Loss =  [8.39880678] Gradient_max =  0.4770013356372528 learning rate ratio =  0.004136266414543898\n",
      "Epoch =  46 Batch =  0 Loss =  [8.39686719] Gradient_max =  0.4771768165529775 learning rate ratio =  0.004139978568237291\n",
      "Epoch =  47 Batch =  0 Loss =  [8.39500244] Gradient_max =  0.47734510879242614 learning rate ratio =  0.004143539115067401\n",
      "Epoch =  48 Batch =  0 Loss =  [8.39320953] Gradient_max =  0.4775065153038055 learning rate ratio =  0.004146954298308269\n",
      "Epoch =  49 Batch =  0 Loss =  [8.39148559] Gradient_max =  0.4776613280465016 learning rate ratio =  0.0041502301435463705\n",
      "Epoch =  50 Batch =  0 Loss =  [8.38982789] Gradient_max =  0.4778098277272823 learning rate ratio =  0.004153372453762099\n",
      "Epoch =  51 Batch =  0 Loss =  [8.38823379] Gradient_max =  0.47795228373913456 learning rate ratio =  0.004156386808104439\n",
      "Epoch =  52 Batch =  0 Loss =  [8.38670082] Gradient_max =  0.47808895424981634 learning rate ratio =  0.004159278563413069\n",
      "Epoch =  53 Batch =  0 Loss =  [8.38522656] Gradient_max =  0.4782200863993317 learning rate ratio =  0.004162052857756395\n",
      "Epoch =  54 Batch =  0 Loss =  [8.38380875] Gradient_max =  0.4783459165749256 learning rate ratio =  0.00416471461542072\n",
      "Epoch =  55 Batch =  0 Loss =  [8.38244518] Gradient_max =  0.4784666707394993 learning rate ratio =  0.004167268552915365\n",
      "Epoch =  56 Batch =  0 Loss =  [8.38113375] Gradient_max =  0.478582564795083 learning rate ratio =  0.004169719185660164\n",
      "Epoch =  57 Batch =  0 Loss =  [8.37987244] Gradient_max =  0.4786938049673797 learning rate ratio =  0.004172070835100043\n",
      "Epoch =  58 Batch =  0 Loss =  [8.37865931] Gradient_max =  0.47880058820087795 learning rate ratio =  0.004174327636053203\n",
      "Epoch =  59 Batch =  0 Loss =  [8.37749251] Gradient_max =  0.4789031025566607 learning rate ratio =  0.004176493544146701\n",
      "Epoch =  60 Batch =  0 Loss =  [8.37637025] Gradient_max =  0.4790015276071076 learning rate ratio =  0.004178572343230443\n",
      "Epoch =  61 Batch =  0 Loss =  [8.37529082] Gradient_max =  0.4790960348232998 learning rate ratio =  0.0041805676526894325\n",
      "Epoch =  62 Batch =  0 Loss =  [8.37425258] Gradient_max =  0.4791867879521302 learning rate ratio =  0.0041824829345960615\n",
      "Epoch =  63 Batch =  0 Loss =  [8.37325393] Gradient_max =  0.4792739433810783 learning rate ratio =  0.004184321500661624\n",
      "Epoch =  64 Batch =  0 Loss =  [8.37229336] Gradient_max =  0.4793576504893361 learning rate ratio =  0.00418608651895938\n",
      "Epoch =  65 Batch =  0 Loss =  [8.37136941] Gradient_max =  0.4794380519844759 learning rate ratio =  0.004187781020401369\n",
      "Epoch =  66 Batch =  0 Loss =  [8.37048067] Gradient_max =  0.47951528422432 learning rate ratio =  0.004189407904959282\n",
      "Epoch =  67 Batch =  0 Loss =  [8.3696258] Gradient_max =  0.47958947752390485 learning rate ratio =  0.004190969947624956\n",
      "Epoch =  68 Batch =  0 Loss =  [8.3688035] Gradient_max =  0.4796607564477263 learning rate ratio =  0.004192469804110915\n",
      "Epoch =  69 Batch =  0 Loss =  [8.36801251] Gradient_max =  0.4797292400875627 learning rate ratio =  0.004193910016294201\n",
      "Epoch =  70 Batch =  0 Loss =  [8.36725164] Gradient_max =  0.4797950423263035 learning rate ratio =  0.004195293017409165\n",
      "Epoch =  71 Batch =  0 Loss =  [8.36651975] Gradient_max =  0.47985827208830234 learning rate ratio =  0.004196621136996631\n",
      "Epoch =  72 Batch =  0 Loss =  [8.36581571] Gradient_max =  0.47991903357678306 learning rate ratio =  0.004197896605617647\n",
      "Epoch =  73 Batch =  0 Loss =  [8.36513846] Gradient_max =  0.47997742649891373 learning rate ratio =  0.004199121559341253\n",
      "Epoch =  74 Batch =  0 Loss =  [8.36448699] Gradient_max =  0.48003354627910255 learning rate ratio =  0.0042002980440154075\n",
      "Epoch =  75 Batch =  0 Loss =  [8.36386029] Gradient_max =  0.4800874842611366 learning rate ratio =  0.004201428019331017\n",
      "Epoch =  76 Batch =  0 Loss =  [8.36325744] Gradient_max =  0.48013932789972813 learning rate ratio =  0.00420251336268858\n",
      "Epoch =  77 Batch =  0 Loss =  [8.36267751] Gradient_max =  0.4801891609420448 learning rate ratio =  0.004203555872877052\n",
      "Epoch =  78 Batch =  0 Loss =  [8.36211962] Gradient_max =  0.48023706359977136 learning rate ratio =  0.004204557273574193\n",
      "Epoch =  79 Batch =  0 Loss =  [8.36158295] Gradient_max =  0.4802831127122198 learning rate ratio =  0.004205519216677367\n",
      "Epoch =  80 Batch =  0 Loss =  [8.36106667] Gradient_max =  0.48032738190100266 learning rate ratio =  0.004206443285473551\n",
      "Epoch =  81 Batch =  0 Loss =  [8.36057001] Gradient_max =  0.4803699417167401 learning rate ratio =  0.0042073309976568\n",
      "Epoch =  82 Batch =  0 Loss =  [8.36009222] Gradient_max =  0.4804108597782556 learning rate ratio =  0.004208183808201117\n",
      "Epoch =  83 Batch =  0 Loss =  [8.35963258] Gradient_max =  0.4804502009046773 learning rate ratio =  0.004209003112096245\n",
      "Epoch =  84 Batch =  0 Loss =  [8.3591904] Gradient_max =  0.48048802724087075 learning rate ratio =  0.004209790246953606\n",
      "Epoch =  85 Batch =  0 Loss =  [8.35876501] Gradient_max =  0.4805243983765654 learning rate ratio =  0.0042105464954891895\n",
      "Epoch =  86 Batch =  0 Loss =  [8.35835577] Gradient_max =  0.48055937145953476 learning rate ratio =  0.004211273087889742\n",
      "Epoch =  87 Batch =  0 Loss =  [8.35796206] Gradient_max =  0.480593001303175 learning rate ratio =  0.004211971204068417\n",
      "Epoch =  88 Batch =  0 Loss =  [8.3575833] Gradient_max =  0.4806253404887839 learning rate ratio =  0.0042126419758155495\n",
      "Epoch =  89 Batch =  0 Loss =  [8.35721891] Gradient_max =  0.4806564394628551 learning rate ratio =  0.004213286488850129\n",
      "Epoch =  90 Batch =  0 Loss =  [8.35686834] Gradient_max =  0.4806863466296479 learning rate ratio =  0.004213905784776893\n",
      "Epoch =  91 Batch =  0 Loss =  [8.35653108] Gradient_max =  0.4807151084393034 learning rate ratio =  0.004214500862954006\n",
      "Epoch =  92 Batch =  0 Loss =  [8.3562066] Gradient_max =  0.4807427694717471 learning rate ratio =  0.0042150726822757865\n",
      "Epoch =  93 Batch =  0 Loss =  [8.35589443] Gradient_max =  0.48076937251661744 learning rate ratio =  0.004215622162874896\n",
      "Epoch =  94 Batch =  0 Loss =  [8.3555941] Gradient_max =  0.48079495864941907 learning rate ratio =  0.004216150187747899\n",
      "Epoch =  95 Batch =  0 Loss =  [8.35530516] Gradient_max =  0.48081956730411907 learning rate ratio =  0.004216657604308092\n",
      "Epoch =  96 Batch =  0 Loss =  [8.35502716] Gradient_max =  0.48084323634237003 learning rate ratio =  0.0042171452258692455\n",
      "Epoch =  97 Batch =  0 Loss =  [8.35475971] Gradient_max =  0.48086600211953034 learning rate ratio =  0.004217613833063517\n",
      "Epoch =  98 Batch =  0 Loss =  [8.35450239] Gradient_max =  0.4808878995476699 learning rate ratio =  0.004218064175196962\n",
      "Epoch =  99 Batch =  0 Loss =  [8.35425481] Gradient_max =  0.4809089621556908 learning rate ratio =  0.004218496971545429\n",
      "Epoch =  100 Batch =  0 Loss =  [8.35401662] Gradient_max =  0.48092922214673894 learning rate ratio =  0.004218912912593947\n",
      "Epoch =  101 Batch =  0 Loss =  [8.35378745] Gradient_max =  0.4809487104530261 learning rate ratio =  0.004219312661222158\n",
      "Epoch =  102 Batch =  0 Loss =  [8.35356696] Gradient_max =  0.4809674567882006 learning rate ratio =  0.00421969685383837\n",
      "Epoch =  103 Batch =  0 Loss =  [8.35335482] Gradient_max =  0.4809854896973924 learning rate ratio =  0.004220066101464757\n",
      "Epoch =  104 Batch =  0 Loss =  [8.35315071] Gradient_max =  0.48100283660504456 learning rate ratio =  0.004220420990775892\n",
      "Epoch =  105 Batch =  0 Loss =  [8.35295433] Gradient_max =  0.48101952386064206 learning rate ratio =  0.004220762085092827\n",
      "Epoch =  106 Batch =  0 Loss =  [8.35276539] Gradient_max =  0.48103557678244135 learning rate ratio =  0.004221089925334793\n",
      "Epoch =  107 Batch =  0 Loss =  [8.35258359] Gradient_max =  0.4810510196992976 learning rate ratio =  0.004221405030930425\n",
      "Epoch =  108 Batch =  0 Loss =  [8.35240868] Gradient_max =  0.4810658759906888 learning rate ratio =  0.004221707900690497\n",
      "Epoch =  109 Batch =  0 Loss =  [8.35224039] Gradient_max =  0.481080168125003 learning rate ratio =  0.004221999013643645\n",
      "Epoch =  110 Batch =  0 Loss =  [8.35207846] Gradient_max =  0.4810939176961981 learning rate ratio =  0.004222278829837066\n",
      "Epoch =  111 Batch =  0 Loss =  [8.35192266] Gradient_max =  0.4811071454588926 learning rate ratio =  0.004222547791103586\n",
      "Epoch =  112 Batch =  0 Loss =  [8.35177275] Gradient_max =  0.481119871361969 learning rate ratio =  0.004222806321796622\n",
      "Epoch =  113 Batch =  0 Loss =  [8.35162851] Gradient_max =  0.48113211458074956 learning rate ratio =  0.004223054829494468\n",
      "Epoch =  114 Batch =  0 Loss =  [8.35148973] Gradient_max =  0.48114389354782916 learning rate ratio =  0.004223293705675347\n",
      "Epoch =  115 Batch =  0 Loss =  [8.3513562] Gradient_max =  0.4811552259826069 learning rate ratio =  0.004223523326364378\n",
      "Epoch =  116 Batch =  0 Loss =  [8.35122772] Gradient_max =  0.48116612891958827 learning rate ratio =  0.004223744052753814\n",
      "Epoch =  117 Batch =  0 Loss =  [8.35110409] Gradient_max =  0.4811766187355109 learning rate ratio =  0.004223956231797654\n",
      "Epoch =  118 Batch =  0 Loss =  [8.35098514] Gradient_max =  0.4811867111753364 learning rate ratio =  0.00422416019678167\n",
      "Epoch =  119 Batch =  0 Loss =  [8.35087068] Gradient_max =  0.4811964213771892 learning rate ratio =  0.004224356267870186\n",
      "Epoch =  120 Batch =  0 Loss =  [8.35076055] Gradient_max =  0.48120576389624775 learning rate ratio =  0.004224544752630232\n",
      "Epoch =  121 Batch =  0 Loss =  [8.35065459] Gradient_max =  0.4812147527276639 learning rate ratio =  0.0042247259465343035\n",
      "Epoch =  122 Batch =  0 Loss =  [8.35055262] Gradient_max =  0.4812234013285543 learning rate ratio =  0.00422490013344266\n",
      "Epoch =  123 Batch =  0 Loss =  [8.35045452] Gradient_max =  0.481231722639077 learning rate ratio =  0.004225067586065783\n",
      "Epoch =  124 Batch =  0 Loss =  [8.35036011] Gradient_max =  0.4812397291026677 learning rate ratio =  0.004225228566408143\n",
      "Epoch =  125 Batch =  0 Loss =  [8.35026928] Gradient_max =  0.4812474326854448 learning rate ratio =  0.004225383326193844\n",
      "Epoch =  126 Batch =  0 Loss =  [8.35018188] Gradient_max =  0.48125484489483866 learning rate ratio =  0.004225532107275047\n",
      "Epoch =  127 Batch =  0 Loss =  [8.35009777] Gradient_max =  0.4812619767974642 learning rate ratio =  0.004225675142023787\n",
      "Epoch =  128 Batch =  0 Loss =  [8.35001685] Gradient_max =  0.48126883903627554 learning rate ratio =  0.00422581265370798\n",
      "Epoch =  129 Batch =  0 Loss =  [8.34993898] Gradient_max =  0.4812754418470332 learning rate ratio =  0.004225944856852198\n",
      "Epoch =  130 Batch =  0 Loss =  [8.34986405] Gradient_max =  0.48128179507411895 learning rate ratio =  0.004226071957583959\n",
      "Epoch =  131 Batch =  0 Loss =  [8.34979196] Gradient_max =  0.48128790818570494 learning rate ratio =  0.004226194153965958\n",
      "Epoch =  132 Batch =  0 Loss =  [8.34972258] Gradient_max =  0.4812937902883297 learning rate ratio =  0.004226311636314987\n",
      "Epoch =  133 Batch =  0 Loss =  [8.34965583] Gradient_max =  0.4812994501408957 learning rate ratio =  0.004226424587508075\n",
      "Epoch =  134 Batch =  0 Loss =  [8.34959159] Gradient_max =  0.48130489616810157 learning rate ratio =  0.004226533183276233\n",
      "Epoch =  135 Batch =  0 Loss =  [8.34952978] Gradient_max =  0.4813101364733522 learning rate ratio =  0.00422663759248651\n",
      "Epoch =  136 Batch =  0 Loss =  [8.34947031] Gradient_max =  0.48131517885115727 learning rate ratio =  0.004226737977412735\n",
      "Epoch =  137 Batch =  0 Loss =  [8.34941308] Gradient_max =  0.481320030799031 learning rate ratio =  0.0042268344939953365\n",
      "Epoch =  138 Batch =  0 Loss =  [8.34935801] Gradient_max =  0.4813246995289382 learning rate ratio =  0.004226927292090909\n",
      "Epoch =  139 Batch =  0 Loss =  [8.34930502] Gradient_max =  0.4813291919782797 learning rate ratio =  0.004227016515711727\n",
      "Epoch =  140 Batch =  0 Loss =  [8.34925402] Gradient_max =  0.4813335148204479 learning rate ratio =  0.004227102303255696\n",
      "Epoch =  141 Batch =  0 Loss =  [8.34920496] Gradient_max =  0.4813376744749758 learning rate ratio =  0.004227184787727209\n",
      "Epoch =  142 Batch =  0 Loss =  [8.34915774] Gradient_max =  0.4813416771172775 learning rate ratio =  0.004227264096949113\n",
      "Epoch =  143 Batch =  0 Loss =  [8.34911231] Gradient_max =  0.48134552868801894 learning rate ratio =  0.004227340353766322\n",
      "Epoch =  144 Batch =  0 Loss =  [8.34906859] Gradient_max =  0.4813492349021153 learning rate ratio =  0.004227413676241265\n",
      "Epoch =  145 Batch =  0 Loss =  [8.34902652] Gradient_max =  0.481352801257382 learning rate ratio =  0.004227484177841631\n",
      "Epoch =  146 Batch =  0 Loss =  [8.34898604] Gradient_max =  0.4813562330428449 learning rate ratio =  0.004227551967620585\n",
      "Epoch =  147 Batch =  0 Loss =  [8.34894708] Gradient_max =  0.48135953534673614 learning rate ratio =  0.004227617150389943\n",
      "Epoch =  148 Batch =  0 Loss =  [8.3489096] Gradient_max =  0.4813627130641679 learning rate ratio =  0.004227679826886366\n",
      "Epoch =  149 Batch =  0 Loss =  [8.34887353] Gradient_max =  0.48136577090451466 learning rate ratio =  0.004227740093931017\n",
      "Epoch =  150 Batch =  0 Loss =  [8.34883882] Gradient_max =  0.48136871339851095 learning rate ratio =  0.004227798044582931\n",
      "Epoch =  151 Batch =  0 Loss =  [8.34880542] Gradient_max =  0.48137154490507383 learning rate ratio =  0.00422785376828628\n",
      "Epoch =  152 Batch =  0 Loss =  [8.34877328] Gradient_max =  0.48137426961785523 learning rate ratio =  0.004227907351011796\n",
      "Epoch =  153 Batch =  0 Loss =  [8.34874235] Gradient_max =  0.48137689157155217 learning rate ratio =  0.004227958875392676\n",
      "Epoch =  154 Batch =  0 Loss =  [8.34871259] Gradient_max =  0.48137941464795775 learning rate ratio =  0.0042280084208549925\n",
      "Epoch =  155 Batch =  0 Loss =  [8.34868395] Gradient_max =  0.4813818425818034 learning rate ratio =  0.004228056063743156\n",
      "Epoch =  156 Batch =  0 Loss =  [8.34865639] Gradient_max =  0.48138417896634556 learning rate ratio =  0.004228101877440192\n",
      "Epoch =  157 Batch =  0 Loss =  [8.34862987] Gradient_max =  0.481386427258764 learning rate ratio =  0.004228145932483487\n",
      "Epoch =  158 Batch =  0 Loss =  [8.34860435] Gradient_max =  0.4813885907853377 learning rate ratio =  0.0042281882966758375\n",
      "Epoch =  159 Batch =  0 Loss =  [8.34857979] Gradient_max =  0.4813906727464207 learning rate ratio =  0.00422822903519214\n",
      "Epoch =  160 Batch =  0 Loss =  [8.34855616] Gradient_max =  0.48139267622124216 learning rate ratio =  0.004228268210681997\n",
      "Epoch =  161 Batch =  0 Loss =  [8.34853342] Gradient_max =  0.4813946041725038 learning rate ratio =  0.0042283058833681286\n",
      "Epoch =  162 Batch =  0 Loss =  [8.34851153] Gradient_max =  0.48139645945080944 learning rate ratio =  0.0042283421111409866\n",
      "Epoch =  163 Batch =  0 Loss =  [8.34849048] Gradient_max =  0.48139824479892945 learning rate ratio =  0.0042283769496497025\n",
      "Epoch =  164 Batch =  0 Loss =  [8.34847021] Gradient_max =  0.48139996285589015 learning rate ratio =  0.004228410452389339\n",
      "Epoch =  165 Batch =  0 Loss =  [8.34845071] Gradient_max =  0.4814016161609172 learning rate ratio =  0.004228442670784825\n",
      "Epoch =  166 Batch =  0 Loss =  [8.34843195] Gradient_max =  0.48140320715722695 learning rate ratio =  0.004228473654271575\n",
      "Epoch =  167 Batch =  0 Loss =  [8.34841389] Gradient_max =  0.48140473819566204 learning rate ratio =  0.004228503450372855\n",
      "Epoch =  168 Batch =  0 Loss =  [8.34839651] Gradient_max =  0.4814062115382053 learning rate ratio =  0.004228532104774265\n",
      "Epoch =  169 Batch =  0 Loss =  [8.34837979] Gradient_max =  0.4814076293613431 learning rate ratio =  0.004228559661395161\n",
      "Epoch =  170 Batch =  0 Loss =  [8.3483637] Gradient_max =  0.48140899375931184 learning rate ratio =  0.004228586162457368\n",
      "Epoch =  171 Batch =  0 Loss =  [8.34834822] Gradient_max =  0.4814103067472124 learning rate ratio =  0.00422861164855115\n",
      "Epoch =  172 Batch =  0 Loss =  [8.34833332] Gradient_max =  0.4814115702640149 learning rate ratio =  0.0042286361586986515\n",
      "Epoch =  173 Batch =  0 Loss =  [8.34831898] Gradient_max =  0.48141278617543976 learning rate ratio =  0.004228659730414797\n",
      "Epoch =  174 Batch =  0 Loss =  [8.34830518] Gradient_max =  0.4814139562767319 learning rate ratio =  0.004228682399765823\n",
      "Epoch =  175 Batch =  0 Loss =  [8.3482919] Gradient_max =  0.48141508229533464 learning rate ratio =  0.00422870420142557\n",
      "Epoch =  176 Batch =  0 Loss =  [8.34827912] Gradient_max =  0.4814161658934555 learning rate ratio =  0.004228725168729512\n",
      "Epoch =  177 Batch =  0 Loss =  [8.34826682] Gradient_max =  0.481417208670533 learning rate ratio =  0.004228745333726678\n",
      "Epoch =  178 Batch =  0 Loss =  [8.34825499] Gradient_max =  0.4814182121656182 learning rate ratio =  0.004228764727229606\n",
      "Epoch =  179 Batch =  0 Loss =  [8.3482436] Gradient_max =  0.4814191778596538 learning rate ratio =  0.004228783378862253\n",
      "Epoch =  180 Batch =  0 Loss =  [8.34823264] Gradient_max =  0.4814201071776829 learning rate ratio =  0.0042288013171061555\n",
      "Epoch =  181 Batch =  0 Loss =  [8.3482221] Gradient_max =  0.4814210014909544 learning rate ratio =  0.00422881856934465\n",
      "Epoch =  182 Batch =  0 Loss =  [8.34821195] Gradient_max =  0.4814218621189612 learning rate ratio =  0.004228835161905463\n",
      "Epoch =  183 Batch =  0 Loss =  [8.34820218] Gradient_max =  0.4814226903314016 learning rate ratio =  0.004228851120101634\n",
      "Epoch =  184 Batch =  0 Loss =  [8.34819279] Gradient_max =  0.4814234873500569 learning rate ratio =  0.00422886646827079\n",
      "Epoch =  185 Batch =  0 Loss =  [8.34818374] Gradient_max =  0.4814242543506069 learning rate ratio =  0.004228881229812943\n",
      "Epoch =  186 Batch =  0 Loss =  [8.34817504] Gradient_max =  0.48142499246436965 learning rate ratio =  0.004228895427226768\n",
      "Epoch =  187 Batch =  0 Loss =  [8.34816667] Gradient_max =  0.48142570277998464 learning rate ratio =  0.004228909082144544\n",
      "Epoch =  188 Batch =  0 Loss =  [8.34815861] Gradient_max =  0.4814263863450181 learning rate ratio =  0.004228922215365617\n",
      "Epoch =  189 Batch =  0 Loss =  [8.34815085] Gradient_max =  0.48142704416751925 learning rate ratio =  0.004228934846888673\n",
      "Epoch =  190 Batch =  0 Loss =  [8.34814339] Gradient_max =  0.4814276772175155 learning rate ratio =  0.004228946995942707\n",
      "Epoch =  191 Batch =  0 Loss =  [8.3481362] Gradient_max =  0.48142828642844404 learning rate ratio =  0.004228958681016752\n",
      "Epoch =  192 Batch =  0 Loss =  [8.34812929] Gradient_max =  0.48142887269853984 learning rate ratio =  0.004228969919888542\n",
      "Epoch =  193 Batch =  0 Loss =  [8.34812264] Gradient_max =  0.4814294368921601 learning rate ratio =  0.004228980729651957\n",
      "Epoch =  194 Batch =  0 Loss =  [8.34811624] Gradient_max =  0.48142997984106717 learning rate ratio =  0.004228991126743484\n",
      "Epoch =  195 Batch =  0 Loss =  [8.34811008] Gradient_max =  0.48143050234565826 learning rate ratio =  0.004229001126967596\n",
      "Epoch =  196 Batch =  0 Loss =  [8.34810415] Gradient_max =  0.4814310051761517 learning rate ratio =  0.0042290107455211935\n",
      "Epoch =  197 Batch =  0 Loss =  [8.34809845] Gradient_max =  0.4814314890737211 learning rate ratio =  0.004229019997017015\n",
      "Epoch =  198 Batch =  0 Loss =  [8.34809296] Gradient_max =  0.4814319547516018 learning rate ratio =  0.004229028895506279\n",
      "Epoch =  199 Batch =  0 Loss =  [8.34808768] Gradient_max =  0.48143240289613404 learning rate ratio =  0.004229037454500256\n",
      "Epoch =  200 Batch =  0 Loss =  [8.34808259] Gradient_max =  0.48143283416778815 learning rate ratio =  0.004229045686991201\n",
      "Epoch =  201 Batch =  0 Loss =  [8.3480777] Gradient_max =  0.4814332492021315 learning rate ratio =  0.004229053605472296\n",
      "Epoch =  202 Batch =  0 Loss =  [8.34807299] Gradient_max =  0.4814336486107813 learning rate ratio =  0.004229061221957015\n",
      "Epoch =  203 Batch =  0 Loss =  [8.34806846] Gradient_max =  0.4814340329822927 learning rate ratio =  0.004229068547997479\n",
      "Epoch =  204 Batch =  0 Loss =  [8.3480641] Gradient_max =  0.4814344028830412 learning rate ratio =  0.004229075594702368\n",
      "Epoch =  205 Batch =  0 Loss =  [8.34805991] Gradient_max =  0.4814347588580563 learning rate ratio =  0.004229082372753983\n",
      "Epoch =  206 Batch =  0 Loss =  [8.34805587] Gradient_max =  0.4814351014318228 learning rate ratio =  0.004229088892424644\n",
      "Epoch =  207 Batch =  0 Loss =  [8.34805198] Gradient_max =  0.48143543110906295 learning rate ratio =  0.004229095163592549\n",
      "Epoch =  208 Batch =  0 Loss =  [8.34804824] Gradient_max =  0.4814357483754741 learning rate ratio =  0.004229101195756895\n",
      "Epoch =  209 Batch =  0 Loss =  [8.34804465] Gradient_max =  0.4814360536984553 learning rate ratio =  0.004229106998052557\n",
      "Epoch =  210 Batch =  0 Loss =  [8.34804118] Gradient_max =  0.4814363475277879 learning rate ratio =  0.004229112579264034\n",
      "Epoch =  211 Batch =  0 Loss =  [8.34803785] Gradient_max =  0.48143663029630934 learning rate ratio =  0.004229117947839017\n",
      "Epoch =  212 Batch =  0 Loss =  [8.34803464] Gradient_max =  0.4814369024205434 learning rate ratio =  0.004229123111901286\n",
      "Epoch =  213 Batch =  0 Loss =  [8.34803156] Gradient_max =  0.48143716430132255 learning rate ratio =  0.004229128079263246\n",
      "Epoch =  214 Batch =  0 Loss =  [8.34802859] Gradient_max =  0.48143741632437714 learning rate ratio =  0.004229132857437872\n",
      "Epoch =  215 Batch =  0 Loss =  [8.34802573] Gradient_max =  0.4814376588609023 learning rate ratio =  0.004229137453650221\n",
      "Epoch =  216 Batch =  0 Loss =  [8.34802298] Gradient_max =  0.48143789226811157 learning rate ratio =  0.004229141874848545\n",
      "Epoch =  217 Batch =  0 Loss =  [8.34802033] Gradient_max =  0.4814381168897581 learning rate ratio =  0.004229146127714903\n",
      "Epoch =  218 Batch =  0 Loss =  [8.34801778] Gradient_max =  0.4814383330566485 learning rate ratio =  0.004229150218675439\n",
      "Epoch =  219 Batch =  0 Loss =  [8.34801533] Gradient_max =  0.48143854108712125 learning rate ratio =  0.0042291541539101404\n",
      "Epoch =  220 Batch =  0 Loss =  [8.34801297] Gradient_max =  0.48143874128752806 learning rate ratio =  0.004229157939362403\n",
      "Epoch =  221 Batch =  0 Loss =  [8.3480107] Gradient_max =  0.4814389339526806 learning rate ratio =  0.004229161580748077\n",
      "Epoch =  222 Batch =  0 Loss =  [8.34800852] Gradient_max =  0.4814391193662828 learning rate ratio =  0.0042291650835641886\n",
      "Epoch =  223 Batch =  0 Loss =  [8.34800642] Gradient_max =  0.48143929780135386 learning rate ratio =  0.0042291684530973775\n",
      "Epoch =  224 Batch =  0 Loss =  [8.34800439] Gradient_max =  0.4814394695206283 learning rate ratio =  0.004229171694431972\n",
      "Epoch =  225 Batch =  0 Loss =  [8.34800245] Gradient_max =  0.48143963477694696 learning rate ratio =  0.004229174812457782\n",
      "Epoch =  226 Batch =  0 Loss =  [8.34800057] Gradient_max =  0.4814397938136251 learning rate ratio =  0.004229177811877509\n",
      "Epoch =  227 Batch =  0 Loss =  [8.34799877] Gradient_max =  0.48143994686481384 learning rate ratio =  0.004229180697213983\n",
      "Epoch =  228 Batch =  0 Loss =  [8.34799703] Gradient_max =  0.481440094155847 learning rate ratio =  0.004229183472817054\n",
      "Epoch =  229 Batch =  0 Loss =  [8.34799536] Gradient_max =  0.48144023590357066 learning rate ratio =  0.004229186142870221\n",
      "Epoch =  230 Batch =  0 Loss =  [8.34799376] Gradient_max =  0.48144037231666875 learning rate ratio =  0.0042291887113970585\n",
      "Epoch =  231 Batch =  0 Loss =  [8.34799221] Gradient_max =  0.481440503595959 learning rate ratio =  0.004229191182267265\n",
      "Epoch =  232 Batch =  0 Loss =  [8.34799072] Gradient_max =  0.4814406299347022 learning rate ratio =  0.00422919355920266\n",
      "Epoch =  233 Batch =  0 Loss =  [8.34798929] Gradient_max =  0.4814407515188775 learning rate ratio =  0.004229195845782786\n",
      "Epoch =  234 Batch =  0 Loss =  [8.34798791] Gradient_max =  0.48144086852746315 learning rate ratio =  0.004229198045450419\n",
      "Epoch =  235 Batch =  0 Loss =  [8.34798658] Gradient_max =  0.481440981132696 learning rate ratio =  0.004229200161516764\n",
      "Epoch =  236 Batch =  0 Loss =  [8.34798531] Gradient_max =  0.4814410895003284 learning rate ratio =  0.004229202197166543\n",
      "Epoch =  237 Batch =  0 Loss =  [8.34798408] Gradient_max =  0.4814411937898712 learning rate ratio =  0.004229204155462821\n",
      "Epoch =  238 Batch =  0 Loss =  [8.3479829] Gradient_max =  0.48144129415482856 learning rate ratio =  0.004229206039351662\n",
      "Epoch =  239 Batch =  0 Loss =  [8.34798176] Gradient_max =  0.4814413907429234 learning rate ratio =  0.004229207851666615\n",
      "Epoch =  240 Batch =  0 Loss =  [8.34798066] Gradient_max =  0.48144148369631823 learning rate ratio =  0.00422920959513305\n",
      "Epoch =  241 Batch =  0 Loss =  [8.34797961] Gradient_max =  0.48144157315182295 learning rate ratio =  0.00422921127237228\n",
      "Epoch =  242 Batch =  0 Loss =  [8.34797859] Gradient_max =  0.4814416592410945 learning rate ratio =  0.004229212885905528\n",
      "Epoch =  243 Batch =  0 Loss =  [8.34797762] Gradient_max =  0.48144174209083435 learning rate ratio =  0.004229214438157808\n",
      "Epoch =  244 Batch =  0 Loss =  [8.34797668] Gradient_max =  0.48144182182297346 learning rate ratio =  0.004229215931461572\n",
      "Epoch =  245 Batch =  0 Loss =  [8.34797578] Gradient_max =  0.48144189855485015 learning rate ratio =  0.004229217368060253\n",
      "Epoch =  246 Batch =  0 Loss =  [8.34797491] Gradient_max =  0.4814419723993889 learning rate ratio =  0.0042292187501117165\n",
      "Epoch =  247 Batch =  0 Loss =  [8.34797407] Gradient_max =  0.4814420434652595 learning rate ratio =  0.004229220079691466\n",
      "Epoch =  248 Batch =  0 Loss =  [8.34797326] Gradient_max =  0.4814421118570419 learning rate ratio =  0.0042292213587958516\n",
      "Epoch =  249 Batch =  0 Loss =  [8.34797249] Gradient_max =  0.4814421776753789 learning rate ratio =  0.004229222589345085\n",
      "Epoch =  250 Batch =  0 Loss =  [8.34797174] Gradient_max =  0.4814422410171235 learning rate ratio =  0.004229223773186128\n",
      "Epoch =  251 Batch =  0 Loss =  [8.34797102] Gradient_max =  0.48144230197548277 learning rate ratio =  0.0042292249120955165\n",
      "Epoch =  252 Batch =  0 Loss =  [8.34797033] Gradient_max =  0.4814423606401573 learning rate ratio =  0.0042292260077820675\n",
      "Epoch =  253 Batch =  0 Loss =  [8.34796967] Gradient_max =  0.48144241709746627 learning rate ratio =  0.0042292270618894175\n",
      "Epoch =  254 Batch =  0 Loss =  [8.34796903] Gradient_max =  0.4814424714304848 learning rate ratio =  0.0042292280759985935\n",
      "Epoch =  255 Batch =  0 Loss =  [8.34796841] Gradient_max =  0.48144252371915536 learning rate ratio =  0.004229229051630312\n",
      "Epoch =  256 Batch =  0 Loss =  [8.34796782] Gradient_max =  0.4814425740404167 learning rate ratio =  0.004229229990247384\n",
      "Epoch =  257 Batch =  0 Loss =  [8.34796725] Gradient_max =  0.48144262246830594 learning rate ratio =  0.004229230893256829\n",
      "Epoch =  258 Batch =  0 Loss =  [8.3479667] Gradient_max =  0.481442669074078 learning rate ratio =  0.0042292317620121\n",
      "Epoch =  259 Batch =  0 Loss =  [8.34796617] Gradient_max =  0.481442713926302 learning rate ratio =  0.004229232597815052\n",
      "Epoch =  260 Batch =  0 Loss =  [8.34796566] Gradient_max =  0.48144275709096723 learning rate ratio =  0.004229233401917959\n",
      "Epoch =  261 Batch =  0 Loss =  [8.34796517] Gradient_max =  0.481442798631581 learning rate ratio =  0.004229234175525415\n",
      "Epoch =  262 Batch =  0 Loss =  [8.3479647] Gradient_max =  0.4814428386092566 learning rate ratio =  0.004229234919796103\n",
      "Epoch =  263 Batch =  0 Loss =  [8.34796425] Gradient_max =  0.48144287708280903 learning rate ratio =  0.004229235635844603\n",
      "Epoch =  264 Batch =  0 Loss =  [8.34796381] Gradient_max =  0.4814429141088406 learning rate ratio =  0.004229236324743054\n",
      "Epoch =  265 Batch =  0 Loss =  [8.3479634] Gradient_max =  0.4814429497418208 learning rate ratio =  0.004229236987522767\n",
      "Epoch =  266 Batch =  0 Loss =  [8.34796299] Gradient_max =  0.4814429840341703 learning rate ratio =  0.004229237625175793\n",
      "Epoch =  267 Batch =  0 Loss =  [8.3479626] Gradient_max =  0.48144301703633724 learning rate ratio =  0.004229238238656434\n",
      "Epoch =  268 Batch =  0 Loss =  [8.34796223] Gradient_max =  0.48144304879687005 learning rate ratio =  0.004229238828882657\n",
      "Epoch =  269 Batch =  0 Loss =  [8.34796187] Gradient_max =  0.48144307936249 learning rate ratio =  0.004229239396737493\n",
      "Epoch =  270 Batch =  0 Loss =  [8.34796152] Gradient_max =  0.4814431087781593 learning rate ratio =  0.004229239943070366\n",
      "Epoch =  271 Batch =  0 Loss =  [8.34796119] Gradient_max =  0.4814431370871498 learning rate ratio =  0.004229240468698398\n",
      "Epoch =  272 Batch =  0 Loss =  [8.34796087] Gradient_max =  0.4814431643311036 learning rate ratio =  0.0042292409744076115\n",
      "Epoch =  273 Batch =  0 Loss =  [8.34796056] Gradient_max =  0.4814431905500951 learning rate ratio =  0.004229241460954122\n",
      "Epoch =  274 Batch =  0 Loss =  [8.34796026] Gradient_max =  0.4814432157826919 learning rate ratio =  0.004229241929065296\n",
      "Epoch =  275 Batch =  0 Loss =  [8.34795998] Gradient_max =  0.4814432400660075 learning rate ratio =  0.0042292423794408115\n",
      "Epoch =  276 Batch =  0 Loss =  [8.3479597] Gradient_max =  0.48144326343576277 learning rate ratio =  0.004229242812753774\n",
      "Epoch =  277 Batch =  0 Loss =  [8.34795944] Gradient_max =  0.4814432859263309 learning rate ratio =  0.004229243229651665\n",
      "Epoch =  278 Batch =  0 Loss =  [8.34795918] Gradient_max =  0.4814433075707911 learning rate ratio =  0.004229243630757342\n",
      "Epoch =  279 Batch =  0 Loss =  [8.34795894] Gradient_max =  0.4814433284009802 learning rate ratio =  0.004229244016670004\n",
      "Epoch =  280 Batch =  0 Loss =  [8.3479587] Gradient_max =  0.48144334844753645 learning rate ratio =  0.00422924438796606\n",
      "Epoch =  281 Batch =  0 Loss =  [8.34795847] Gradient_max =  0.48144336773994423 learning rate ratio =  0.004229244745200012\n",
      "Epoch =  282 Batch =  0 Loss =  [8.34795825] Gradient_max =  0.4814433863065782 learning rate ratio =  0.004229245088905273\n",
      "Epoch =  283 Batch =  0 Loss =  [8.34795804] Gradient_max =  0.4814434041747449 learning rate ratio =  0.0042292454195949885\n",
      "Epoch =  284 Batch =  0 Loss =  [8.34795784] Gradient_max =  0.4814434213707245 learning rate ratio =  0.004229245737762807\n",
      "Epoch =  285 Batch =  0 Loss =  [8.34795765] Gradient_max =  0.4814434379198078 learning rate ratio =  0.004229246043883622\n",
      "Epoch =  286 Batch =  0 Loss =  [8.34795746] Gradient_max =  0.48144345384633325 learning rate ratio =  0.004229246338414273\n",
      "Epoch =  287 Batch =  0 Loss =  [8.34795728] Gradient_max =  0.4814434691737252 learning rate ratio =  0.004229246621794259\n",
      "Epoch =  288 Batch =  0 Loss =  [8.34795711] Gradient_max =  0.4814434839245243 learning rate ratio =  0.004229246894446367\n",
      "Epoch =  289 Batch =  0 Loss =  [8.34795694] Gradient_max =  0.4814434981204238 learning rate ratio =  0.004229247156777327\n",
      "Epoch =  290 Batch =  0 Loss =  [8.34795678] Gradient_max =  0.48144351178230155 learning rate ratio =  0.004229247409178432\n",
      "Epoch =  291 Batch =  0 Loss =  [8.34795662] Gradient_max =  0.4814435249302473 learning rate ratio =  0.00422924765202609\n",
      "Epoch =  292 Batch =  0 Loss =  [8.34795647] Gradient_max =  0.4814435375835978 learning rate ratio =  0.004229247885682441\n",
      "Epoch =  293 Batch =  0 Loss =  [8.34795633] Gradient_max =  0.48144354976096226 learning rate ratio =  0.004229248110495871\n",
      "Epoch =  294 Batch =  0 Loss =  [8.34795619] Gradient_max =  0.4814435614802459 learning rate ratio =  0.004229248326801514\n",
      "Epoch =  295 Batch =  0 Loss =  [8.34795606] Gradient_max =  0.4814435727586841 learning rate ratio =  0.0042292485349218045\n",
      "Epoch =  296 Batch =  0 Loss =  [8.34795593] Gradient_max =  0.48144358361286177 learning rate ratio =  0.004229248735166925\n",
      "Epoch =  297 Batch =  0 Loss =  [8.34795581] Gradient_max =  0.48144359405874 learning rate ratio =  0.004229248927835286\n",
      "Epoch =  298 Batch =  0 Loss =  [8.34795569] Gradient_max =  0.4814436041116802 learning rate ratio =  0.0042292491132139755\n",
      "Epoch =  299 Batch =  0 Loss =  [8.34795558] Gradient_max =  0.4814436137864647 learning rate ratio =  0.0042292492915791775\n",
      "Epoch =  300 Batch =  0 Loss =  [8.34795547] Gradient_max =  0.4814436230973206 learning rate ratio =  0.004229249463196603\n",
      "Epoch =  301 Batch =  0 Loss =  [8.34795536] Gradient_max =  0.48144363205793844 learning rate ratio =  0.004229249628321872\n",
      "Epoch =  302 Batch =  0 Loss =  [8.34795526] Gradient_max =  0.4814436406814936 learning rate ratio =  0.004229249787200898\n",
      "Epoch =  303 Batch =  0 Loss =  [8.34795516] Gradient_max =  0.48144364898066894 learning rate ratio =  0.004229249940070299\n",
      "Epoch =  304 Batch =  0 Loss =  [8.34795507] Gradient_max =  0.4814436569676661 learning rate ratio =  0.004229250087157678\n",
      "Epoch =  305 Batch =  0 Loss =  [8.34795498] Gradient_max =  0.48144366465422783 learning rate ratio =  0.0042292502286820095\n",
      "Epoch =  306 Batch =  0 Loss =  [8.34795489] Gradient_max =  0.4814436720516586 learning rate ratio =  0.00422925036485398\n",
      "Epoch =  307 Batch =  0 Loss =  [8.34795481] Gradient_max =  0.481443679170834 learning rate ratio =  0.00422925049587627\n",
      "Epoch =  308 Batch =  0 Loss =  [8.34795473] Gradient_max =  0.4814436860222229 learning rate ratio =  0.004229250621943881\n",
      "Epoch =  309 Batch =  0 Loss =  [8.34795465] Gradient_max =  0.48144369261589914 learning rate ratio =  0.004229250743244418\n",
      "Epoch =  310 Batch =  0 Loss =  [8.34795457] Gradient_max =  0.48144369896155564 learning rate ratio =  0.004229250859958351\n",
      "Epoch =  311 Batch =  0 Loss =  [8.3479545] Gradient_max =  0.4814437050685235 learning rate ratio =  0.004229250972259333\n",
      "Epoch =  312 Batch =  0 Loss =  [8.34795443] Gradient_max =  0.48144371094578314 learning rate ratio =  0.004229251080314436\n",
      "Epoch =  313 Batch =  0 Loss =  [8.34795437] Gradient_max =  0.48144371660197394 learning rate ratio =  0.004229251184284368\n",
      "Epoch =  314 Batch =  0 Loss =  [8.3479543] Gradient_max =  0.48144372204541364 learning rate ratio =  0.004229251284323779\n",
      "Epoch =  315 Batch =  0 Loss =  [8.34795424] Gradient_max =  0.481443727284105 learning rate ratio =  0.004229251380581433\n",
      "Epoch =  316 Batch =  0 Loss =  [8.34795418] Gradient_max =  0.4814437323257497 learning rate ratio =  0.00422925147320046\n",
      "Epoch =  317 Batch =  0 Loss =  [8.34795412] Gradient_max =  0.4814437371777604 learning rate ratio =  0.004229251562318566\n",
      "Epoch =  318 Batch =  0 Loss =  [8.34795407] Gradient_max =  0.48144374184726924 learning rate ratio =  0.004229251648068231\n",
      "Epoch =  319 Batch =  0 Loss =  [8.34795402] Gradient_max =  0.48144374634114395 learning rate ratio =  0.004229251730576939\n",
      "Epoch =  320 Batch =  0 Loss =  [8.34795397] Gradient_max =  0.4814437506659897 learning rate ratio =  0.004229251809967314\n",
      "Epoch =  321 Batch =  0 Loss =  [8.34795392] Gradient_max =  0.4814437548281661 learning rate ratio =  0.004229251886357357\n",
      "Epoch =  322 Batch =  0 Loss =  [8.34795387] Gradient_max =  0.48144375883379065 learning rate ratio =  0.0042292519598605745\n",
      "Epoch =  323 Batch =  0 Loss =  [8.34795382] Gradient_max =  0.48144376268875194 learning rate ratio =  0.004229252030586179\n",
      "Epoch =  324 Batch =  0 Loss =  [8.34795378] Gradient_max =  0.48144376639872055 learning rate ratio =  0.004229252098639277\n",
      "Epoch =  325 Batch =  0 Loss =  [8.34795374] Gradient_max =  0.4814437699691464 learning rate ratio =  0.004229252164120934\n",
      "Epoch =  326 Batch =  0 Loss =  [8.3479537] Gradient_max =  0.48144377340528166 learning rate ratio =  0.004229252227128449\n",
      "Epoch =  327 Batch =  0 Loss =  [8.34795366] Gradient_max =  0.48144377671217514 learning rate ratio =  0.004229252287755383\n",
      "Epoch =  328 Batch =  0 Loss =  [8.34795362] Gradient_max =  0.48144377989469134 learning rate ratio =  0.004229252346091801\n",
      "Epoch =  329 Batch =  0 Loss =  [8.34795359] Gradient_max =  0.48144378295750623 learning rate ratio =  0.004229252402224312\n",
      "Epoch =  330 Batch =  0 Loss =  [8.34795355] Gradient_max =  0.48144378590512327 learning rate ratio =  0.004229252456236276\n",
      "Epoch =  331 Batch =  0 Loss =  [8.34795352] Gradient_max =  0.4814437887418766 learning rate ratio =  0.004229252508207895\n",
      "Epoch =  332 Batch =  0 Loss =  [8.34795348] Gradient_max =  0.4814437914719344 learning rate ratio =  0.004229252558216308\n",
      "Epoch =  333 Batch =  0 Loss =  [8.34795345] Gradient_max =  0.481443794099312 learning rate ratio =  0.0042292526063357665\n",
      "Epoch =  334 Batch =  0 Loss =  [8.34795342] Gradient_max =  0.4814437966278699 learning rate ratio =  0.004229252652637683\n",
      "Epoch =  335 Batch =  0 Loss =  [8.3479534] Gradient_max =  0.4814437990613256 learning rate ratio =  0.004229252697190778\n",
      "Epoch =  336 Batch =  0 Loss =  [8.34795337] Gradient_max =  0.48144380140325793 learning rate ratio =  0.0042292527400611905\n",
      "Epoch =  337 Batch =  0 Loss =  [8.34795334] Gradient_max =  0.4814438036571074 learning rate ratio =  0.004229252781312512\n",
      "Epoch =  338 Batch =  0 Loss =  [8.34795332] Gradient_max =  0.48144380582618806 learning rate ratio =  0.004229252821005965\n",
      "Epoch =  339 Batch =  0 Loss =  [8.34795329] Gradient_max =  0.48144380791368846 learning rate ratio =  0.00422925285920044\n",
      "Epoch =  340 Batch =  0 Loss =  [8.34795327] Gradient_max =  0.48144380992267644 learning rate ratio =  0.004229252895952591\n",
      "Epoch =  341 Batch =  0 Loss =  [8.34795324] Gradient_max =  0.48144381185610624 learning rate ratio =  0.00422925293131695\n",
      "Epoch =  342 Batch =  0 Loss =  [8.34795322] Gradient_max =  0.4814438137168187 learning rate ratio =  0.00422925296534596\n",
      "Epoch =  343 Batch =  0 Loss =  [8.3479532] Gradient_max =  0.4814438155075502 learning rate ratio =  0.004229252998090104\n",
      "Epoch =  344 Batch =  0 Loss =  [8.34795318] Gradient_max =  0.4814438172309317 learning rate ratio =  0.004229253029597926\n",
      "Epoch =  345 Batch =  0 Loss =  [8.34795316] Gradient_max =  0.48144381888949683 learning rate ratio =  0.00422925305991616\n",
      "Epoch =  346 Batch =  0 Loss =  [8.34795314] Gradient_max =  0.4814438204856839 learning rate ratio =  0.004229253089089758\n",
      "Epoch =  347 Batch =  0 Loss =  [8.34795313] Gradient_max =  0.4814438220218388 learning rate ratio =  0.004229253117161976\n",
      "Epoch =  348 Batch =  0 Loss =  [8.34795311] Gradient_max =  0.48144382350022 learning rate ratio =  0.004229253144174434\n",
      "Epoch =  349 Batch =  0 Loss =  [8.34795309] Gradient_max =  0.4814438249229993 learning rate ratio =  0.004229253170167167\n",
      "Epoch =  350 Batch =  0 Loss =  [8.34795308] Gradient_max =  0.48144382629226945 learning rate ratio =  0.004229253195178714\n",
      "Epoch =  351 Batch =  0 Loss =  [8.34795306] Gradient_max =  0.48144382761004123 learning rate ratio =  0.004229253219246134\n",
      "Epoch =  352 Batch =  0 Loss =  [8.34795304] Gradient_max =  0.4814438288782538 learning rate ratio =  0.004229253242405114\n",
      "Epoch =  353 Batch =  0 Loss =  [8.34795303] Gradient_max =  0.48144383009877045 learning rate ratio =  0.004229253264689969\n",
      "Epoch =  354 Batch =  0 Loss =  [8.34795302] Gradient_max =  0.4814438312733839 learning rate ratio =  0.004229253286133711\n",
      "Epoch =  355 Batch =  0 Loss =  [8.347953] Gradient_max =  0.48144383240382177 learning rate ratio =  0.004229253306768119\n",
      "Epoch =  356 Batch =  0 Loss =  [8.34795299] Gradient_max =  0.4814438334917451 learning rate ratio =  0.004229253326623767\n",
      "Epoch =  357 Batch =  0 Loss =  [8.34795298] Gradient_max =  0.48144383453875317 learning rate ratio =  0.004229253345730068\n",
      "Epoch =  358 Batch =  0 Loss =  [8.34795297] Gradient_max =  0.48144383554638565 learning rate ratio =  0.004229253364115336\n",
      "Epoch =  359 Batch =  0 Loss =  [8.34795295] Gradient_max =  0.4814438365161215 learning rate ratio =  0.004229253381806785\n",
      "Epoch =  360 Batch =  0 Loss =  [8.34795294] Gradient_max =  0.48144383744938857 learning rate ratio =  0.0042292533988306435\n",
      "Epoch =  361 Batch =  0 Loss =  [8.34795293] Gradient_max =  0.4814438383475561 learning rate ratio =  0.004229253415212102\n",
      "Epoch =  362 Batch =  0 Loss =  [8.34795292] Gradient_max =  0.48144383921194556 learning rate ratio =  0.004229253430975435\n",
      "Epoch =  363 Batch =  0 Loss =  [8.34795291] Gradient_max =  0.4814438400438266 learning rate ratio =  0.0042292534461439765\n",
      "Epoch =  364 Batch =  0 Loss =  [8.3479529] Gradient_max =  0.4814438408444225 learning rate ratio =  0.004229253460740191\n",
      "Epoch =  365 Batch =  0 Loss =  [8.34795289] Gradient_max =  0.48144384161490955 learning rate ratio =  0.0042292534747856905\n",
      "Epoch =  366 Batch =  0 Loss =  [8.34795289] Gradient_max =  0.48144384235642035 learning rate ratio =  0.00422925348830127\n",
      "Epoch =  367 Batch =  0 Loss =  [8.34795288] Gradient_max =  0.4814438430700444 learning rate ratio =  0.004229253501306932\n",
      "Epoch =  368 Batch =  0 Loss =  [8.34795287] Gradient_max =  0.4814438437568314 learning rate ratio =  0.004229253513821946\n",
      "Epoch =  369 Batch =  0 Loss =  [8.34795286] Gradient_max =  0.48144384441779003 learning rate ratio =  0.004229253525864823\n",
      "Epoch =  370 Batch =  0 Loss =  [8.34795285] Gradient_max =  0.4814438450538918 learning rate ratio =  0.0042292535374533905\n",
      "Epoch =  371 Batch =  0 Loss =  [8.34795285] Gradient_max =  0.48144384566607074 learning rate ratio =  0.004229253548604794\n",
      "Epoch =  372 Batch =  0 Loss =  [8.34795284] Gradient_max =  0.48144384625522774 learning rate ratio =  0.0042292535593355446\n",
      "Epoch =  373 Batch =  0 Loss =  [8.34795283] Gradient_max =  0.48144384682222796 learning rate ratio =  0.00422925356966152\n",
      "Epoch =  374 Batch =  0 Loss =  [8.34795283] Gradient_max =  0.4814438473679059 learning rate ratio =  0.004229253579598009\n",
      "Epoch =  375 Batch =  0 Loss =  [8.34795282] Gradient_max =  0.4814438478930617 learning rate ratio =  0.004229253589159697\n",
      "Epoch =  376 Batch =  0 Loss =  [8.34795281] Gradient_max =  0.48144384839846766 learning rate ratio =  0.004229253598360736\n",
      "Epoch =  377 Batch =  0 Loss =  [8.34795281] Gradient_max =  0.4814438488848687 learning rate ratio =  0.0042292536072147556\n",
      "Epoch =  378 Batch =  0 Loss =  [8.3479528] Gradient_max =  0.4814438493529769 learning rate ratio =  0.004229253615734835\n",
      "Epoch =  379 Batch =  0 Loss =  [8.3479528] Gradient_max =  0.48144384980348115 learning rate ratio =  0.004229253623933582\n",
      "Epoch =  380 Batch =  0 Loss =  [8.34795279] Gradient_max =  0.48144385023704317 learning rate ratio =  0.0042292536318231195\n",
      "Epoch =  381 Batch =  0 Loss =  [8.34795279] Gradient_max =  0.4814438506543004 learning rate ratio =  0.004229253639415126\n",
      "Epoch =  382 Batch =  0 Loss =  [8.34795278] Gradient_max =  0.4814438510558665 learning rate ratio =  0.004229253646720827\n",
      "Epoch =  383 Batch =  0 Loss =  [8.34795278] Gradient_max =  0.48144385144233137 learning rate ratio =  0.004229253653751027\n",
      "Epoch =  384 Batch =  0 Loss =  [8.34795277] Gradient_max =  0.4814438518142619 learning rate ratio =  0.0042292536605161146\n",
      "Epoch =  385 Batch =  0 Loss =  [8.34795277] Gradient_max =  0.48144385217220564 learning rate ratio =  0.004229253667026101\n",
      "Epoch =  386 Batch =  0 Loss =  [8.34795277] Gradient_max =  0.4814438525166892 learning rate ratio =  0.0042292536732906165\n",
      "Epoch =  387 Batch =  0 Loss =  [8.34795276] Gradient_max =  0.48144385284821817 learning rate ratio =  0.004229253679318915\n",
      "Epoch =  388 Batch =  0 Loss =  [8.34795276] Gradient_max =  0.48144385316727933 learning rate ratio =  0.00422925368511991\n",
      "Epoch =  389 Batch =  0 Loss =  [8.34795276] Gradient_max =  0.48144385347434143 learning rate ratio =  0.0042292536907021765\n",
      "Epoch =  390 Batch =  0 Loss =  [8.34795275] Gradient_max =  0.48144385376985743 learning rate ratio =  0.004229253696073981\n",
      "Epoch =  391 Batch =  0 Loss =  [8.34795275] Gradient_max =  0.48144385405425966 learning rate ratio =  0.004229253701243242\n",
      "Epoch =  392 Batch =  0 Loss =  [8.34795275] Gradient_max =  0.48144385432796666 learning rate ratio =  0.004229253706217614\n",
      "Epoch =  393 Batch =  0 Loss =  [8.34795274] Gradient_max =  0.4814438545913814 learning rate ratio =  0.004229253711004455\n",
      "Epoch =  394 Batch =  0 Loss =  [8.34795274] Gradient_max =  0.4814438548448898 learning rate ratio =  0.004229253715610824\n",
      "Epoch =  395 Batch =  0 Loss =  [8.34795274] Gradient_max =  0.48144385508886467 learning rate ratio =  0.004229253720043537\n",
      "Epoch =  396 Batch =  0 Loss =  [8.34795273] Gradient_max =  0.48144385532366546 learning rate ratio =  0.004229253724309147\n",
      "Epoch =  397 Batch =  0 Loss =  [8.34795273] Gradient_max =  0.4814438555496353 learning rate ratio =  0.004229253728413948\n",
      "Epoch =  398 Batch =  0 Loss =  [8.34795273] Gradient_max =  0.48144385576710874 learning rate ratio =  0.004229253732364018\n",
      "Epoch =  399 Batch =  0 Loss =  [8.34795273] Gradient_max =  0.48144385597640393 learning rate ratio =  0.004229253736165188\n",
      "Epoch =  400 Batch =  0 Loss =  [8.34795272] Gradient_max =  0.4814438561778288 learning rate ratio =  0.004229253739823077\n",
      "Epoch =  401 Batch =  0 Loss =  [8.34795272] Gradient_max =  0.4814438563716776 learning rate ratio =  0.00422925374334307\n",
      "Epoch =  402 Batch =  0 Loss =  [8.34795272] Gradient_max =  0.48144385655823807 learning rate ratio =  0.004229253746730396\n",
      "Epoch =  403 Batch =  0 Loss =  [8.34795272] Gradient_max =  0.4814438567377826 learning rate ratio =  0.004229253749990044\n",
      "Epoch =  404 Batch =  0 Loss =  [8.34795271] Gradient_max =  0.48144385691057506 learning rate ratio =  0.0042292537531268245\n",
      "Epoch =  405 Batch =  0 Loss =  [8.34795271] Gradient_max =  0.48144385707687043 learning rate ratio =  0.0042292537561453855\n",
      "Epoch =  406 Batch =  0 Loss =  [8.34795271] Gradient_max =  0.4814438572369128 learning rate ratio =  0.00422925375905018\n",
      "Epoch =  407 Batch =  0 Loss =  [8.34795271] Gradient_max =  0.4814438573909356 learning rate ratio =  0.004229253761845487\n",
      "Epoch =  408 Batch =  0 Loss =  [8.34795271] Gradient_max =  0.48144385753916696 learning rate ratio =  0.004229253764535449\n",
      "Epoch =  409 Batch =  0 Loss =  [8.34795271] Gradient_max =  0.48144385768182413 learning rate ratio =  0.0042292537671240315\n",
      "Epoch =  410 Batch =  0 Loss =  [8.3479527] Gradient_max =  0.48144385781911725 learning rate ratio =  0.004229253769615063\n",
      "Epoch =  411 Batch =  0 Loss =  [8.3479527] Gradient_max =  0.4814438579512473 learning rate ratio =  0.004229253772012214\n",
      "Epoch =  412 Batch =  0 Loss =  [8.3479527] Gradient_max =  0.48144385807840806 learning rate ratio =  0.0042292537743190235\n",
      "Epoch =  413 Batch =  0 Loss =  [8.3479527] Gradient_max =  0.4814438582007872 learning rate ratio =  0.004229253776538906\n",
      "Epoch =  414 Batch =  0 Loss =  [8.3479527] Gradient_max =  0.4814438583185646 learning rate ratio =  0.004229253778675138\n",
      "Epoch =  415 Batch =  0 Loss =  [8.3479527] Gradient_max =  0.4814438584319137 learning rate ratio =  0.004229253780730874\n",
      "Epoch =  416 Batch =  0 Loss =  [8.3479527] Gradient_max =  0.48144385854099936 learning rate ratio =  0.004229253782709139\n",
      "Epoch =  417 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.4814438586459839 learning rate ratio =  0.0042292537846128665\n",
      "Epoch =  418 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.4814438587470192 learning rate ratio =  0.004229253786444849\n",
      "Epoch =  419 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.4814438588442567 learning rate ratio =  0.004229253788207813\n",
      "Epoch =  420 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.4814438589378375 learning rate ratio =  0.004229253789904349\n",
      "Epoch =  421 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.48144385902789927 learning rate ratio =  0.004229253791536963\n",
      "Epoch =  422 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.48144385911457377 learning rate ratio =  0.0042292537931080615\n",
      "Epoch =  423 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.48144385919798943 learning rate ratio =  0.004229253794619966\n",
      "Epoch =  424 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.4814438592782677 learning rate ratio =  0.004229253796074904\n",
      "Epoch =  425 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.481443859355528 learning rate ratio =  0.004229253797475031\n",
      "Epoch =  426 Batch =  0 Loss =  [8.34795269] Gradient_max =  0.48144385942988266 learning rate ratio =  0.004229253798822403\n",
      "Epoch =  427 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.48144385950144186 learning rate ratio =  0.0042292538001190245\n",
      "Epoch =  428 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.48144385957030994 learning rate ratio =  0.004229253801366791\n",
      "Epoch =  429 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.481443859636588 learning rate ratio =  0.004229253802567549\n",
      "Epoch =  430 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.4814438597003745 learning rate ratio =  0.0042292538037230765\n",
      "Epoch =  431 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.48144385976176185 learning rate ratio =  0.00422925380483507\n",
      "Epoch =  432 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.4814438598208409 learning rate ratio =  0.004229253805905173\n",
      "Epoch =  433 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.4814438598776988 learning rate ratio =  0.004229253806934966\n",
      "Epoch =  434 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.48144385993241784 learning rate ratio =  0.004229253807925962\n",
      "Epoch =  435 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.4814438599850802 learning rate ratio =  0.004229253808879637\n",
      "Epoch =  436 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.4814438600357618 learning rate ratio =  0.0042292538097973825\n",
      "Epoch =  437 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.4814438600845378 learning rate ratio =  0.004229253810680562\n",
      "Epoch =  438 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.4814438601314791 learning rate ratio =  0.004229253811530471\n",
      "Epoch =  439 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.48144386017665586 learning rate ratio =  0.00422925381234837\n",
      "Epoch =  440 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.4814438602201335 learning rate ratio =  0.00422925381313546\n",
      "Epoch =  441 Batch =  0 Loss =  [8.34795268] Gradient_max =  0.48144386026197644 learning rate ratio =  0.004229253813892903\n",
      "Epoch =  442 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386030224573 learning rate ratio =  0.0042292538146218184\n",
      "Epoch =  443 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438603410013 learning rate ratio =  0.004229253815323283\n",
      "Epoch =  444 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438603782996 learning rate ratio =  0.0042292538159983285\n",
      "Epoch =  445 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438604141954 learning rate ratio =  0.004229253816647954\n",
      "Epoch =  446 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386044874127 learning rate ratio =  0.004229253817273108\n",
      "Epoch =  447 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438604819883 learning rate ratio =  0.00422925381787472\n",
      "Epoch =  448 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386051398486 learning rate ratio =  0.004229253818453672\n",
      "Epoch =  449 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438605447782 learning rate ratio =  0.004229253819010815\n",
      "Epoch =  450 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438605744138 learning rate ratio =  0.004229253819546981\n",
      "Epoch =  451 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438606029352 learning rate ratio =  0.0042292538200629564\n",
      "Epoch =  452 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386063038364 learning rate ratio =  0.004229253820559494\n",
      "Epoch =  453 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386065679984 learning rate ratio =  0.004229253821037334\n",
      "Epoch =  454 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386068222345 learning rate ratio =  0.004229253821497188\n",
      "Epoch =  455 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386070669115 learning rate ratio =  0.00422925382193972\n",
      "Epoch =  456 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438607302392 learning rate ratio =  0.0042292538223655946\n",
      "Epoch =  457 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438607529008 learning rate ratio =  0.004229253822775422\n",
      "Epoch =  458 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438607747104 learning rate ratio =  0.004229253823169821\n",
      "Epoch =  459 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386079570023 learning rate ratio =  0.00422925382354937\n",
      "Epoch =  460 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438608159008 learning rate ratio =  0.004229253823914628\n",
      "Epoch =  461 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386083534124 learning rate ratio =  0.004229253824266127\n",
      "Epoch =  462 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.481443860854051 learning rate ratio =  0.0042292538246043974\n",
      "Epoch =  463 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438608720576 learning rate ratio =  0.00422925382492993\n",
      "Epoch =  464 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386088938707 learning rate ratio =  0.0042292538252432016\n",
      "Epoch =  465 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386090606445 learning rate ratio =  0.004229253825544679\n",
      "Epoch =  466 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438609221154 learning rate ratio =  0.004229253825834811\n",
      "Epoch =  467 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386093756264 learning rate ratio =  0.0042292538261140145\n",
      "Epoch =  468 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386095242847 learning rate ratio =  0.004229253826382702\n",
      "Epoch =  469 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438609667354 learning rate ratio =  0.004229253826641274\n",
      "Epoch =  470 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438609805044 learning rate ratio =  0.004229253826890113\n",
      "Epoch =  471 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386099375613 learning rate ratio =  0.004229253827129588\n",
      "Epoch =  472 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386100650993 learning rate ratio =  0.004229253827360047\n",
      "Epoch =  473 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386101878245 learning rate ratio =  0.004229253827581815\n",
      "Epoch =  474 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438610305949 learning rate ratio =  0.004229253827795246\n",
      "Epoch =  475 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386104196285 learning rate ratio =  0.004229253828000642\n",
      "Epoch =  476 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438610529036 learning rate ratio =  0.004229253828198304\n",
      "Epoch =  477 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438610634326 learning rate ratio =  0.004229253828388523\n",
      "Epoch =  478 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438610735658 learning rate ratio =  0.004229253828571581\n",
      "Epoch =  479 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438610833187 learning rate ratio =  0.004229253828747755\n",
      "Epoch =  480 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438610927036 learning rate ratio =  0.004229253828917285\n",
      "Epoch =  481 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386110173676 learning rate ratio =  0.004229253829080442\n",
      "Epoch =  482 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386111042903 learning rate ratio =  0.004229253829237449\n",
      "Epoch =  483 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386111879584 learning rate ratio =  0.004229253829388555\n",
      "Epoch =  484 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.48144386112684723 learning rate ratio =  0.0042292538295339655\n",
      "Epoch =  485 Batch =  0 Loss =  [8.34795267] Gradient_max =  0.4814438611345957 learning rate ratio =  0.004229253829673902\n",
      "Epoch =  486 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386114205423 learning rate ratio =  0.0042292538298085835\n",
      "Epoch =  487 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438611492311 learning rate ratio =  0.0042292538299381855\n",
      "Epoch =  488 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438611561386 learning rate ratio =  0.004229253830062911\n",
      "Epoch =  489 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386116278515 learning rate ratio =  0.004229253830182934\n",
      "Epoch =  490 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438611691835 learning rate ratio =  0.004229253830298452\n",
      "Epoch =  491 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438611753406 learning rate ratio =  0.004229253830409616\n",
      "Epoch =  492 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386118126514 learning rate ratio =  0.004229253830516587\n",
      "Epoch =  493 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386118696747 learning rate ratio =  0.004229253830619537\n",
      "Epoch =  494 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438611924566 learning rate ratio =  0.004229253830718622\n",
      "Epoch =  495 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438611977379 learning rate ratio =  0.004229253830813966\n",
      "Epoch =  496 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386120282173 learning rate ratio =  0.004229253830905729\n",
      "Epoch =  497 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612077138 learning rate ratio =  0.004229253830994034\n",
      "Epoch =  498 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386121242216 learning rate ratio =  0.004229253831079017\n",
      "Epoch =  499 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386121695326 learning rate ratio =  0.004229253831160799\n",
      "Epoch =  500 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861221314 learning rate ratio =  0.004229253831239506\n",
      "Epoch =  501 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386122551147 learning rate ratio =  0.004229253831315252\n",
      "Epoch =  502 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386122954963 learning rate ratio =  0.004229253831388137\n",
      "Epoch =  503 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386123343685 learning rate ratio =  0.004229253831458287\n",
      "Epoch =  504 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612371776 learning rate ratio =  0.004229253831525792\n",
      "Epoch =  505 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386124077887 learning rate ratio =  0.0042292538315907655\n",
      "Epoch =  506 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612442436 learning rate ratio =  0.004229253831653286\n",
      "Epoch =  507 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386124757754 learning rate ratio =  0.0042292538317134495\n",
      "Epoch =  508 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612507873 learning rate ratio =  0.004229253831771359\n",
      "Epoch =  509 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386125387595 learning rate ratio =  0.004229253831827085\n",
      "Epoch =  510 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612568484 learning rate ratio =  0.004229253831880712\n",
      "Epoch =  511 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386125970867 learning rate ratio =  0.004229253831932319\n",
      "Epoch =  512 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612624621 learning rate ratio =  0.004229253831981988\n",
      "Epoch =  513 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612651121 learning rate ratio =  0.004229253832029791\n",
      "Epoch =  514 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612676618 learning rate ratio =  0.004229253832075788\n",
      "Epoch =  515 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386127011574 learning rate ratio =  0.0042292538321200565\n",
      "Epoch =  516 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612724774 learning rate ratio =  0.004229253832162658\n",
      "Epoch =  517 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612747506 learning rate ratio =  0.004229253832203658\n",
      "Epoch =  518 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386127693833 learning rate ratio =  0.004229253832243116\n",
      "Epoch =  519 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861279044 learning rate ratio =  0.004229253832281093\n",
      "Epoch =  520 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386128106975 learning rate ratio =  0.004229253832317635\n",
      "Epoch =  521 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386128301997 learning rate ratio =  0.004229253832352802\n",
      "Epoch =  522 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612848961 learning rate ratio =  0.0042292538323866445\n",
      "Epoch =  523 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386128670275 learning rate ratio =  0.00422925383241922\n",
      "Epoch =  524 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612884414 learning rate ratio =  0.0042292538324505665\n",
      "Epoch =  525 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386129011385 learning rate ratio =  0.00422925383248073\n",
      "Epoch =  526 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612917239 learning rate ratio =  0.004229253832509761\n",
      "Epoch =  527 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386129327343 learning rate ratio =  0.004229253832537699\n",
      "Epoch =  528 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612947638 learning rate ratio =  0.00422925383256458\n",
      "Epoch =  529 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612961997 learning rate ratio =  0.004229253832590464\n",
      "Epoch =  530 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386129758 learning rate ratio =  0.004229253832615361\n",
      "Epoch =  531 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438612989102 learning rate ratio =  0.004229253832639332\n",
      "Epoch =  532 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613001893 learning rate ratio =  0.004229253832662394\n",
      "Epoch =  533 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613014206 learning rate ratio =  0.004229253832684592\n",
      "Epoch =  534 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613026053 learning rate ratio =  0.0042292538327059525\n",
      "Epoch =  535 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386130374534 learning rate ratio =  0.004229253832726507\n",
      "Epoch =  536 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386130484196 learning rate ratio =  0.004229253832746283\n",
      "Epoch =  537 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613058982 learning rate ratio =  0.004229253832765322\n",
      "Epoch =  538 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613069148 learning rate ratio =  0.004229253832783645\n",
      "Epoch =  539 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613078933 learning rate ratio =  0.00422925383280128\n",
      "Epoch =  540 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613088347 learning rate ratio =  0.004229253832818251\n",
      "Epoch =  541 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613097408 learning rate ratio =  0.004229253832834583\n",
      "Epoch =  542 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613106129 learning rate ratio =  0.004229253832850301\n",
      "Epoch =  543 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613114523 learning rate ratio =  0.004229253832865425\n",
      "Epoch =  544 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386131225986 learning rate ratio =  0.004229253832879982\n",
      "Epoch =  545 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386131303607 learning rate ratio =  0.0042292538328939835\n",
      "Epoch =  546 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386131378475 learning rate ratio =  0.004229253832907469\n",
      "Epoch =  547 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613145047 learning rate ratio =  0.004229253832920442\n",
      "Epoch =  548 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613151975 learning rate ratio =  0.004229253832932929\n",
      "Epoch =  549 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613158645 learning rate ratio =  0.004229253832944947\n",
      "Epoch =  550 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613165056 learning rate ratio =  0.004229253832956505\n",
      "Epoch =  551 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613171244 learning rate ratio =  0.004229253832967646\n",
      "Epoch =  552 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386131771844 learning rate ratio =  0.004229253832978352\n",
      "Epoch =  553 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386131829087 learning rate ratio =  0.004229253832988661\n",
      "Epoch =  554 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613188411 learning rate ratio =  0.004229253832998578\n",
      "Epoch =  555 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613193701 learning rate ratio =  0.004229253833008115\n",
      "Epoch =  556 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386131988054 learning rate ratio =  0.0042292538330173084\n",
      "Epoch =  557 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132037154 learning rate ratio =  0.004229253833026154\n",
      "Epoch =  558 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613208436 learning rate ratio =  0.00422925383303466\n",
      "Epoch =  559 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861321298 learning rate ratio =  0.004229253833042849\n",
      "Epoch =  560 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132173583 learning rate ratio =  0.004229253833050732\n",
      "Epoch =  561 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613221558 learning rate ratio =  0.004229253833058308\n",
      "Epoch =  562 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132256123 learning rate ratio =  0.004229253833065611\n",
      "Epoch =  563 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613229516 learning rate ratio =  0.004229253833072638\n",
      "Epoch =  564 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132332684 learning rate ratio =  0.004229253833079397\n",
      "Epoch =  565 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132368794 learning rate ratio =  0.004229253833085905\n",
      "Epoch =  566 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613240352 learning rate ratio =  0.004229253833092163\n",
      "Epoch =  567 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132437 learning rate ratio =  0.004229253833098191\n",
      "Epoch =  568 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132469136 learning rate ratio =  0.004229253833103984\n",
      "Epoch =  569 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613250011 learning rate ratio =  0.004229253833109562\n",
      "Epoch =  570 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613252996 learning rate ratio =  0.004229253833114937\n",
      "Epoch =  571 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613255868 learning rate ratio =  0.004229253833120108\n",
      "Epoch =  572 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613258636 learning rate ratio =  0.004229253833125086\n",
      "Epoch =  573 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613261288 learning rate ratio =  0.0042292538331298694\n",
      "Epoch =  574 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613263843 learning rate ratio =  0.004229253833134475\n",
      "Epoch =  575 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613266302 learning rate ratio =  0.004229253833138907\n",
      "Epoch =  576 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132686734 learning rate ratio =  0.0042292538331431765\n",
      "Epoch =  577 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132709627 learning rate ratio =  0.004229253833147291\n",
      "Epoch =  578 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132731576 learning rate ratio =  0.004229253833151243\n",
      "Epoch =  579 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132752653 learning rate ratio =  0.004229253833155043\n",
      "Epoch =  580 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613277301 learning rate ratio =  0.004229253833158707\n",
      "Epoch =  581 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132792594 learning rate ratio =  0.0042292538331622324\n",
      "Epoch =  582 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613281134 learning rate ratio =  0.004229253833165614\n",
      "Epoch =  583 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132829475 learning rate ratio =  0.004229253833168876\n",
      "Epoch =  584 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613284689 learning rate ratio =  0.004229253833172015\n",
      "Epoch =  585 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613286366 learning rate ratio =  0.004229253833175036\n",
      "Epoch =  586 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132879846 learning rate ratio =  0.004229253833177945\n",
      "Epoch =  587 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132895334 learning rate ratio =  0.0042292538331807385\n",
      "Epoch =  588 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132910344 learning rate ratio =  0.004229253833183435\n",
      "Epoch =  589 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613292471 learning rate ratio =  0.004229253833186023\n",
      "Epoch =  590 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613293853 learning rate ratio =  0.004229253833188516\n",
      "Epoch =  591 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613295182 learning rate ratio =  0.004229253833190914\n",
      "Epoch =  592 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386132964745 learning rate ratio =  0.004229253833193231\n",
      "Epoch =  593 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613297706 learning rate ratio =  0.004229253833195451\n",
      "Epoch =  594 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613298889 learning rate ratio =  0.004229253833197586\n",
      "Epoch =  595 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613300042 learning rate ratio =  0.004229253833199653\n",
      "Epoch =  596 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613301141 learning rate ratio =  0.0042292538332016315\n",
      "Epoch =  597 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613302201 learning rate ratio =  0.00422925383320354\n",
      "Epoch =  598 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133032224 learning rate ratio =  0.004229253833205376\n",
      "Epoch =  599 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133041955 learning rate ratio =  0.004229253833207137\n",
      "Epoch =  600 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613305147 learning rate ratio =  0.004229253833208844\n",
      "Epoch =  601 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133060507 learning rate ratio =  0.004229253833210475\n",
      "Epoch =  602 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613306931 learning rate ratio =  0.004229253833212056\n",
      "Epoch =  603 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133077677 learning rate ratio =  0.004229253833213567\n",
      "Epoch =  604 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613308581 learning rate ratio =  0.004229253833215029\n",
      "Epoch =  605 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133093553 learning rate ratio =  0.0042292538332164295\n",
      "Epoch =  606 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613310112 learning rate ratio =  0.004229253833217786\n",
      "Epoch =  607 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133108374 learning rate ratio =  0.004229253833219091\n",
      "Epoch =  608 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133115324 learning rate ratio =  0.004229253833220341\n",
      "Epoch =  609 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133122 learning rate ratio =  0.0042292538332215435\n",
      "Epoch =  610 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133128425 learning rate ratio =  0.0042292538332227006\n",
      "Epoch =  611 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861331346 learning rate ratio =  0.0042292538332238125\n",
      "Epoch =  612 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133140626 learning rate ratio =  0.004229253833224892\n",
      "Epoch =  613 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613314637 learning rate ratio =  0.004229253833225928\n",
      "Epoch =  614 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133151834 learning rate ratio =  0.004229253833226914\n",
      "Epoch =  615 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613315719 learning rate ratio =  0.004229253833227874\n",
      "Epoch =  616 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133162337 learning rate ratio =  0.004229253833228795\n",
      "Epoch =  617 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613316722 learning rate ratio =  0.004229253833229678\n",
      "Epoch =  618 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613317195 learning rate ratio =  0.004229253833230531\n",
      "Epoch =  619 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133176553 learning rate ratio =  0.004229253833231357\n",
      "Epoch =  620 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861331809 learning rate ratio =  0.0042292538332321435\n",
      "Epoch =  621 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133185124 learning rate ratio =  0.0042292538332329025\n",
      "Epoch =  622 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613318919 learning rate ratio =  0.004229253833233632\n",
      "Epoch =  623 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613319306 learning rate ratio =  0.004229253833234334\n",
      "Epoch =  624 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861331969 learning rate ratio =  0.004229253833235016\n",
      "Epoch =  625 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133200523 learning rate ratio =  0.004229253833235667\n",
      "Epoch =  626 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133203976 learning rate ratio =  0.004229253833236293\n",
      "Epoch =  627 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133207273 learning rate ratio =  0.004229253833236892\n",
      "Epoch =  628 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133210537 learning rate ratio =  0.004229253833237476\n",
      "Epoch =  629 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133213685 learning rate ratio =  0.004229253833238039\n",
      "Epoch =  630 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613321662 learning rate ratio =  0.004229253833238573\n",
      "Epoch =  631 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613321954 learning rate ratio =  0.004229253833239096\n",
      "Epoch =  632 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133222333 learning rate ratio =  0.004229253833239596\n",
      "Epoch =  633 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133225026 learning rate ratio =  0.004229253833240079\n",
      "Epoch =  634 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613322751 learning rate ratio =  0.004229253833240535\n",
      "Epoch =  635 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133229994 learning rate ratio =  0.00422925383324098\n",
      "Epoch =  636 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613323237 learning rate ratio =  0.004229253833241408\n",
      "Epoch =  637 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613323471 learning rate ratio =  0.004229253833241824\n",
      "Epoch =  638 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133236877 learning rate ratio =  0.004229253833242218\n",
      "Epoch =  639 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613323898 learning rate ratio =  0.0042292538332426\n",
      "Epoch =  640 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133240974 learning rate ratio =  0.004229253833242963\n",
      "Epoch =  641 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613324297 learning rate ratio =  0.004229253833243318\n",
      "Epoch =  642 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613324485 learning rate ratio =  0.004229253833243657\n",
      "Epoch =  643 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332467 learning rate ratio =  0.004229253833243988\n",
      "Epoch =  644 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613324843 learning rate ratio =  0.004229253833244302\n",
      "Epoch =  645 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133250106 learning rate ratio =  0.004229253833244604\n",
      "Epoch =  646 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613325175 learning rate ratio =  0.004229253833244899\n",
      "Epoch =  647 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613325335 learning rate ratio =  0.004229253833245183\n",
      "Epoch =  648 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332548 learning rate ratio =  0.004229253833245449\n",
      "Epoch =  649 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133256245 learning rate ratio =  0.00422925383324571\n",
      "Epoch =  650 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133257594 learning rate ratio =  0.004229253833245954\n",
      "Epoch =  651 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133258954 learning rate ratio =  0.004229253833246197\n",
      "Epoch =  652 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133260286 learning rate ratio =  0.0042292538332464324\n",
      "Epoch =  653 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133261513 learning rate ratio =  0.004229253833246652\n",
      "Epoch =  654 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613326264 learning rate ratio =  0.004229253833246862\n",
      "Epoch =  655 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613326382 learning rate ratio =  0.004229253833247072\n",
      "Epoch =  656 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133264927 learning rate ratio =  0.00422925383324727\n",
      "Epoch =  657 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613326597 learning rate ratio =  0.004229253833247459\n",
      "Epoch =  658 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133267 learning rate ratio =  0.004229253833247644\n",
      "Epoch =  659 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133267936 learning rate ratio =  0.004229253833247818\n",
      "Epoch =  660 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613326889 learning rate ratio =  0.004229253833247988\n",
      "Epoch =  661 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133269834 learning rate ratio =  0.004229253833248154\n",
      "Epoch =  662 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613327072 learning rate ratio =  0.004229253833248314\n",
      "Epoch =  663 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613327155 learning rate ratio =  0.004229253833248464\n",
      "Epoch =  664 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133272354 learning rate ratio =  0.004229253833248609\n",
      "Epoch =  665 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133273154 learning rate ratio =  0.004229253833248748\n",
      "Epoch =  666 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613327384 learning rate ratio =  0.004229253833248878\n",
      "Epoch =  667 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613327456 learning rate ratio =  0.004229253833249009\n",
      "Epoch =  668 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133275285 learning rate ratio =  0.004229253833249135\n",
      "Epoch =  669 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613327594 learning rate ratio =  0.004229253833249255\n",
      "Epoch =  670 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133276584 learning rate ratio =  0.004229253833249371\n",
      "Epoch =  671 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613327726 learning rate ratio =  0.004229253833249488\n",
      "Epoch =  672 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133277844 learning rate ratio =  0.004229253833249594\n",
      "Epoch =  673 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613327848 learning rate ratio =  0.0042292538332497015\n",
      "Epoch =  674 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613327897 learning rate ratio =  0.004229253833249795\n",
      "Epoch =  675 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133279554 learning rate ratio =  0.004229253833249893\n",
      "Epoch =  676 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133280026 learning rate ratio =  0.004229253833249984\n",
      "Epoch =  677 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328057 learning rate ratio =  0.004229253833250077\n",
      "Epoch =  678 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328105 learning rate ratio =  0.004229253833250162\n",
      "Epoch =  679 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328149 learning rate ratio =  0.004229253833250241\n",
      "Epoch =  680 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133281936 learning rate ratio =  0.00422925383325032\n",
      "Epoch =  681 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133282285 learning rate ratio =  0.004229253833250389\n",
      "Epoch =  682 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133282735 learning rate ratio =  0.004229253833250466\n",
      "Epoch =  683 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328309 learning rate ratio =  0.004229253833250533\n",
      "Epoch =  684 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328349 learning rate ratio =  0.0042292538332506\n",
      "Epoch =  685 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328383 learning rate ratio =  0.004229253833250664\n",
      "Epoch =  686 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328421 learning rate ratio =  0.004229253833250729\n",
      "Epoch =  687 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133284534 learning rate ratio =  0.004229253833250788\n",
      "Epoch =  688 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133284856 learning rate ratio =  0.004229253833250846\n",
      "Epoch =  689 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133285155 learning rate ratio =  0.004229253833250901\n",
      "Epoch =  690 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133285483 learning rate ratio =  0.0042292538332509575\n",
      "Epoch =  691 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328579 learning rate ratio =  0.004229253833251009\n",
      "Epoch =  692 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328608 learning rate ratio =  0.00422925383325106\n",
      "Epoch =  693 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133286365 learning rate ratio =  0.004229253833251108\n",
      "Epoch =  694 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133286604 learning rate ratio =  0.004229253833251154\n",
      "Epoch =  695 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133286843 learning rate ratio =  0.004229253833251199\n",
      "Epoch =  696 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133287037 learning rate ratio =  0.0042292538332512385\n",
      "Epoch =  697 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133287276 learning rate ratio =  0.004229253833251282\n",
      "Epoch =  698 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133287503 learning rate ratio =  0.004229253833251323\n",
      "Epoch =  699 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332877 learning rate ratio =  0.004229253833251358\n",
      "Epoch =  700 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133287925 learning rate ratio =  0.004229253833251395\n",
      "Epoch =  701 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328813 learning rate ratio =  0.004229253833251431\n",
      "Epoch =  702 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133288364 learning rate ratio =  0.004229253833251468\n",
      "Epoch =  703 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328852 learning rate ratio =  0.004229253833251499\n",
      "Epoch =  704 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133288697 learning rate ratio =  0.004229253833251528\n",
      "Epoch =  705 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328886 learning rate ratio =  0.004229253833251557\n",
      "Epoch =  706 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328903 learning rate ratio =  0.0042292538332515854\n",
      "Epoch =  707 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328917 learning rate ratio =  0.004229253833251611\n",
      "Epoch =  708 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133289374 learning rate ratio =  0.004229253833251642\n",
      "Epoch =  709 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328952 learning rate ratio =  0.004229253833251668\n",
      "Epoch =  710 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133289613 learning rate ratio =  0.004229253833251688\n",
      "Epoch =  711 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328972 learning rate ratio =  0.0042292538332517095\n",
      "Epoch =  712 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613328986 learning rate ratio =  0.004229253833251735\n",
      "Epoch =  713 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133289974 learning rate ratio =  0.0042292538332517554\n",
      "Epoch =  714 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329012 learning rate ratio =  0.004229253833251781\n",
      "Epoch =  715 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329022 learning rate ratio =  0.004229253833251799\n",
      "Epoch =  716 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133290373 learning rate ratio =  0.004229253833251822\n",
      "Epoch =  717 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133290407 learning rate ratio =  0.004229253833251835\n",
      "Epoch =  718 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329055 learning rate ratio =  0.004229253833251858\n",
      "Epoch =  719 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133290673 learning rate ratio =  0.004229253833251879\n",
      "Epoch =  720 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329078 learning rate ratio =  0.004229253833251898\n",
      "Epoch =  721 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133290873 learning rate ratio =  0.004229253833251915\n",
      "Epoch =  722 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133290923 learning rate ratio =  0.00422925383325193\n",
      "Epoch =  723 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133290995 learning rate ratio =  0.0042292538332519445\n",
      "Epoch =  724 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133291084 learning rate ratio =  0.004229253833251962\n",
      "Epoch =  725 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329116 learning rate ratio =  0.004229253833251978\n",
      "Epoch =  726 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329126 learning rate ratio =  0.004229253833251996\n",
      "Epoch =  727 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329135 learning rate ratio =  0.004229253833252008\n",
      "Epoch =  728 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329137 learning rate ratio =  0.004229253833252015\n",
      "Epoch =  729 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133291484 learning rate ratio =  0.0042292538332520295\n",
      "Epoch =  730 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329155 learning rate ratio =  0.004229253833252039\n",
      "Epoch =  731 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133291617 learning rate ratio =  0.00422925383325205\n",
      "Epoch =  732 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329167 learning rate ratio =  0.004229253833252062\n",
      "Epoch =  733 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329173 learning rate ratio =  0.004229253833252073\n",
      "Epoch =  734 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133291794 learning rate ratio =  0.004229253833252085\n",
      "Epoch =  735 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133291794 learning rate ratio =  0.004229253833252092\n",
      "Epoch =  736 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329189 learning rate ratio =  0.004229253833252106\n",
      "Epoch =  737 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329192 learning rate ratio =  0.004229253833252113\n",
      "Epoch =  738 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133291944 learning rate ratio =  0.004229253833252119\n",
      "Epoch =  739 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329198 learning rate ratio =  0.004229253833252125\n",
      "Epoch =  740 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292016 learning rate ratio =  0.004229253833252132\n",
      "Epoch =  741 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292055 learning rate ratio =  0.004229253833252138\n",
      "Epoch =  742 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329213 learning rate ratio =  0.0042292538332521475\n",
      "Epoch =  743 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329217 learning rate ratio =  0.004229253833252156\n",
      "Epoch =  744 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329223 learning rate ratio =  0.004229253833252165\n",
      "Epoch =  745 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329226 learning rate ratio =  0.0042292538332521735\n",
      "Epoch =  746 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329229 learning rate ratio =  0.004229253833252179\n",
      "Epoch =  747 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329231 learning rate ratio =  0.004229253833252185\n",
      "Epoch =  748 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329233 learning rate ratio =  0.004229253833252189\n",
      "Epoch =  749 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292355 learning rate ratio =  0.004229253833252194\n",
      "Epoch =  750 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329239 learning rate ratio =  0.0042292538332522\n",
      "Epoch =  751 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329245 learning rate ratio =  0.004229253833252211\n",
      "Epoch =  752 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292455 learning rate ratio =  0.004229253833252214\n",
      "Epoch =  753 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329245 learning rate ratio =  0.004229253833252216\n",
      "Epoch =  754 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329247 learning rate ratio =  0.0042292538332522195\n",
      "Epoch =  755 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332925 learning rate ratio =  0.004229253833252224\n",
      "Epoch =  756 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329253 learning rate ratio =  0.004229253833252229\n",
      "Epoch =  757 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292566 learning rate ratio =  0.004229253833252234\n",
      "Epoch =  758 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329255 learning rate ratio =  0.004229253833252235\n",
      "Epoch =  759 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292594 learning rate ratio =  0.00422925383325224\n",
      "Epoch =  760 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292627 learning rate ratio =  0.0042292538332522455\n",
      "Epoch =  761 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292627 learning rate ratio =  0.004229253833252248\n",
      "Epoch =  762 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292644 learning rate ratio =  0.004229253833252251\n",
      "Epoch =  763 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292694 learning rate ratio =  0.004229253833252258\n",
      "Epoch =  764 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329268 learning rate ratio =  0.0042292538332522585\n",
      "Epoch =  765 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329272 learning rate ratio =  0.004229253833252265\n",
      "Epoch =  766 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329273 learning rate ratio =  0.004229253833252268\n",
      "Epoch =  767 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329275 learning rate ratio =  0.00422925383325227\n",
      "Epoch =  768 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329277 learning rate ratio =  0.004229253833252274\n",
      "Epoch =  769 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329275 learning rate ratio =  0.004229253833252273\n",
      "Epoch =  770 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329274 learning rate ratio =  0.004229253833252273\n",
      "Epoch =  771 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329278 learning rate ratio =  0.004229253833252279\n",
      "Epoch =  772 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329282 learning rate ratio =  0.004229253833252283\n",
      "Epoch =  773 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329279 learning rate ratio =  0.00422925383325228\n",
      "Epoch =  774 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329285 learning rate ratio =  0.004229253833252287\n",
      "Epoch =  775 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329285 learning rate ratio =  0.004229253833252287\n",
      "Epoch =  776 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329284 learning rate ratio =  0.004229253833252286\n",
      "Epoch =  777 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329286 learning rate ratio =  0.00422925383325229\n",
      "Epoch =  778 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329286 learning rate ratio =  0.004229253833252289\n",
      "Epoch =  779 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329284 learning rate ratio =  0.004229253833252288\n",
      "Epoch =  780 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329287 learning rate ratio =  0.00422925383325229\n",
      "Epoch =  781 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329289 learning rate ratio =  0.0042292538332522915\n",
      "Epoch =  782 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332929 learning rate ratio =  0.004229253833252293\n",
      "Epoch =  783 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329288 learning rate ratio =  0.004229253833252293\n",
      "Epoch =  784 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292916 learning rate ratio =  0.004229253833252296\n",
      "Epoch =  785 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332929 learning rate ratio =  0.004229253833252295\n",
      "Epoch =  786 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329295 learning rate ratio =  0.0042292538332523\n",
      "Epoch =  787 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329291 learning rate ratio =  0.004229253833252298\n",
      "Epoch =  788 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329295 learning rate ratio =  0.004229253833252302\n",
      "Epoch =  789 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292943 learning rate ratio =  0.004229253833252302\n",
      "Epoch =  790 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292943 learning rate ratio =  0.004229253833252303\n",
      "Epoch =  791 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292927 learning rate ratio =  0.004229253833252301\n",
      "Epoch =  792 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329293 learning rate ratio =  0.004229253833252303\n",
      "Epoch =  793 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329295 learning rate ratio =  0.004229253833252305\n",
      "Epoch =  794 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329296 learning rate ratio =  0.004229253833252306\n",
      "Epoch =  795 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329297 learning rate ratio =  0.004229253833252308\n",
      "Epoch =  796 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329297 learning rate ratio =  0.004229253833252308\n",
      "Epoch =  797 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329297 learning rate ratio =  0.004229253833252309\n",
      "Epoch =  798 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329296 learning rate ratio =  0.004229253833252308\n",
      "Epoch =  799 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329297 learning rate ratio =  0.004229253833252309\n",
      "Epoch =  800 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292993 learning rate ratio =  0.004229253833252311\n",
      "Epoch =  801 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292993 learning rate ratio =  0.004229253833252312\n",
      "Epoch =  802 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133292993 learning rate ratio =  0.004229253833252312\n",
      "Epoch =  803 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293016 learning rate ratio =  0.004229253833252315\n",
      "Epoch =  804 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293005 learning rate ratio =  0.004229253833252315\n",
      "Epoch =  805 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293 learning rate ratio =  0.004229253833252314\n",
      "Epoch =  806 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329301 learning rate ratio =  0.004229253833252315\n",
      "Epoch =  807 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293016 learning rate ratio =  0.004229253833252316\n",
      "Epoch =  808 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293016 learning rate ratio =  0.004229253833252315\n",
      "Epoch =  809 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329304 learning rate ratio =  0.004229253833252317\n",
      "Epoch =  810 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293054 learning rate ratio =  0.0042292538332523175\n",
      "Epoch =  811 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329302 learning rate ratio =  0.004229253833252316\n",
      "Epoch =  812 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293066 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  813 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329303 learning rate ratio =  0.004229253833252317\n",
      "Epoch =  814 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293027 learning rate ratio =  0.004229253833252317\n",
      "Epoch =  815 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329306 learning rate ratio =  0.004229253833252318\n",
      "Epoch =  816 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329302 learning rate ratio =  0.004229253833252315\n",
      "Epoch =  817 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329307 learning rate ratio =  0.004229253833252319\n",
      "Epoch =  818 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293054 learning rate ratio =  0.004229253833252318\n",
      "Epoch =  819 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293066 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  820 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  821 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  822 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293043 learning rate ratio =  0.0042292538332523175\n",
      "Epoch =  823 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293043 learning rate ratio =  0.0042292538332523175\n",
      "Epoch =  824 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  825 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329307 learning rate ratio =  0.004229253833252319\n",
      "Epoch =  826 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329307 learning rate ratio =  0.004229253833252319\n",
      "Epoch =  827 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293077 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  828 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293077 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  829 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  830 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329307 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  831 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  832 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  833 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293066 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  834 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329307 learning rate ratio =  0.004229253833252319\n",
      "Epoch =  835 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293066 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  836 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  837 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  838 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  839 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  840 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  841 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  842 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  843 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  844 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  845 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  846 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  847 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293077 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  848 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  849 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  850 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  851 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  852 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  853 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  854 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  855 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  856 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  857 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  858 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  859 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  860 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  861 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  862 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252319\n",
      "Epoch =  863 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  864 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  865 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  866 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  867 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  868 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  869 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  870 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  871 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  872 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  873 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  874 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  875 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  876 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  877 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  878 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  879 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  880 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  881 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  882 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  883 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  884 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  885 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  886 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  887 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  888 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  889 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  890 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  891 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  892 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  893 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  894 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  895 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  896 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  897 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  898 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  899 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  900 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  901 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  902 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  903 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  904 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  905 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  906 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  907 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  908 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  909 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  910 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  911 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  912 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  913 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  914 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  915 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  916 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  917 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  918 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  919 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  920 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  921 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  922 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  923 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  924 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  925 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  926 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  927 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  928 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  929 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  930 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  931 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  932 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  933 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  934 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  935 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  936 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  937 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  938 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  939 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  940 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  941 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  942 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  943 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  944 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  945 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  946 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  947 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  948 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  949 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293077 learning rate ratio =  0.004229253833252319\n",
      "Epoch =  950 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  951 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  952 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  953 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  954 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  955 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  956 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  957 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  958 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  959 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  960 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  961 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  962 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  963 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  964 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  965 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  966 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  967 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  968 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  969 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  970 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  971 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  972 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  973 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  974 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  975 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  976 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  977 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  978 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  979 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  980 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  981 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  982 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  983 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  984 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  985 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  986 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  987 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  988 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  989 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  990 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  991 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  992 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  993 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  994 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  995 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  996 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  997 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  998 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  999 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1000 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1001 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1002 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1003 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1004 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1005 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1006 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1007 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1008 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1009 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1010 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1011 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1012 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1013 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1014 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1015 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1016 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1017 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1018 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1019 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1020 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1021 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1022 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1023 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1024 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1025 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1026 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1027 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1028 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1029 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1030 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1031 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1032 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1033 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1034 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1035 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1036 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1037 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1038 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1039 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1040 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1041 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1042 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1043 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1044 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1045 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1046 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1047 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1048 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1049 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1050 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1051 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1052 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1053 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1054 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1055 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1056 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1057 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1058 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1059 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1060 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1061 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1062 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1063 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1064 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1065 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  1066 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1067 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1068 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1069 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1070 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1071 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1072 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1073 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1074 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1075 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1076 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1077 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1078 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1079 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1080 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1081 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1082 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1083 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1084 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1085 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1086 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1087 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1088 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1089 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1090 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1091 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1092 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1093 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1094 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1095 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1096 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1097 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1098 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1099 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1100 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  1101 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1102 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1103 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1104 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1105 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1106 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1107 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1108 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1109 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1110 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1111 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1112 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1113 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1114 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1115 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1116 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1117 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1118 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1119 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1120 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1121 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329318 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1122 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1123 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1124 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1125 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1126 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1127 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1128 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1129 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1130 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1131 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1132 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1133 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1134 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1135 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1136 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1137 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1138 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1139 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1140 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1141 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1142 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1143 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1144 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1145 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1146 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1147 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1148 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1149 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1150 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1151 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1152 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1153 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1154 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1155 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1156 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1157 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1158 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1159 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1160 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1161 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1162 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1163 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1164 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1165 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1166 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1167 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1168 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1169 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1170 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1171 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1172 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1173 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1174 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1175 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1176 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1177 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1178 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1179 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1180 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1181 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1182 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1183 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1184 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1185 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293077 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1186 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1187 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1188 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1189 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1190 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1191 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1192 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1193 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1194 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1195 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1196 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1197 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1198 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1199 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1200 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1201 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1202 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1203 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1204 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1205 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1206 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1207 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1208 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1209 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1210 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1211 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1212 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  1213 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1214 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1215 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1216 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1217 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1218 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1219 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1220 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1221 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1222 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1223 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1224 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1225 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1226 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1227 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1228 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1229 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1230 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1231 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1232 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1233 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1234 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1235 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1236 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1237 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1238 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1239 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1240 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1241 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1242 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1243 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1244 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1245 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1246 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1247 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1248 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1249 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1250 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1251 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1252 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1253 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1254 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1255 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1256 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1257 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1258 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1259 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1260 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1261 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1262 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1263 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1264 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1265 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1266 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1267 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1268 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1269 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1270 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1271 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1272 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1273 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1274 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1275 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1276 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1277 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1278 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1279 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1280 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1281 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1282 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1283 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1284 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1285 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1286 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1287 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1288 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1289 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1290 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  1291 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1292 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1293 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1294 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1295 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1296 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1297 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1298 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1299 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1300 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1301 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1302 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1303 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1304 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1305 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1306 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1307 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1308 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1309 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1310 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  1311 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1312 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1313 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1314 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1315 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1316 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1317 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1318 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1319 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1320 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1321 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1322 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1323 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1324 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329318 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1325 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1326 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1327 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1328 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1329 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1330 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1331 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1332 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1333 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1334 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1335 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1336 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1337 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1338 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1339 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1340 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1341 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1342 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1343 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1344 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1345 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1346 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1347 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1348 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1349 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1350 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1351 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1352 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1353 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1354 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1355 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1356 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1357 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1358 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1359 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1360 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1361 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1362 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1363 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1364 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1365 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1366 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1367 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1368 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1369 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1370 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1371 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1372 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1373 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1374 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1375 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1376 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1377 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1378 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1379 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1380 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1381 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1382 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1383 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1384 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1385 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1386 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1387 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1388 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1389 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1390 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1391 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1392 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1393 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1394 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1395 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1396 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1397 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1398 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1399 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1400 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1401 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1402 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1403 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1404 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1405 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1406 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1407 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1408 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1409 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1410 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1411 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1412 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1413 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1414 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1415 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1416 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1417 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1418 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1419 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1420 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1421 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1422 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1423 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1424 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1425 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1426 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1427 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1428 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1429 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1430 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1431 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1432 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1433 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1434 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1435 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1436 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1437 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1438 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1439 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1440 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1441 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1442 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1443 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1444 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1445 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1446 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1447 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1448 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1449 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1450 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1451 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1452 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1453 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1454 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  1455 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1456 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1457 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1458 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1459 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1460 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1461 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1462 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1463 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1464 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1465 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1466 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1467 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1468 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1469 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1470 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1471 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1472 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1473 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1474 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1475 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1476 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1477 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1478 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1479 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1480 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1481 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1482 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1483 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1484 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1485 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1486 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1487 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1488 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1489 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1490 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1491 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1492 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1493 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1494 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1495 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1496 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1497 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1498 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1499 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1500 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1501 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1502 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1503 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1504 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1505 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1506 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1507 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1508 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1509 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1510 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1511 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1512 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1513 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1514 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1515 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1516 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1517 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1518 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1519 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1520 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1521 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1522 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1523 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1524 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1525 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1526 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1527 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1528 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1529 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1530 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1531 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1532 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1533 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1534 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1535 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1536 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1537 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1538 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1539 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1540 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1541 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1542 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1543 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1544 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1545 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1546 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1547 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1548 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1549 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1550 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1551 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1552 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1553 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1554 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1555 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1556 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1557 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1558 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1559 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1560 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1561 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1562 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1563 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1564 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1565 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1566 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1567 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1568 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1569 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1570 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1571 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1572 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1573 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1574 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1575 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1576 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1577 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1578 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1579 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1580 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1581 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1582 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1583 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1584 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1585 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1586 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1587 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1588 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1589 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1590 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1591 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1592 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1593 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1594 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1595 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1596 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1597 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1598 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1599 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1600 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1601 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1602 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1603 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1604 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1605 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1606 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1607 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1608 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1609 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1610 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1611 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1612 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1613 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1614 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1615 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1616 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1617 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1618 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1619 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1620 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1621 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1622 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1623 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1624 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1625 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1626 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1627 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1628 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1629 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1630 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1631 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1632 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1633 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1634 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1635 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1636 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1637 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1638 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1639 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1640 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1641 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1642 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1643 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1644 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1645 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1646 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1647 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1648 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1649 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1650 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1651 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  1652 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1653 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1654 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1655 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1656 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1657 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1658 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1659 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1660 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1661 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1662 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1663 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1664 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1665 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1666 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1667 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1668 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1669 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1670 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1671 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1672 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1673 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1674 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1675 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1676 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1677 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1678 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1679 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1680 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1681 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1682 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1683 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1684 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1685 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1686 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1687 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1688 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1689 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1690 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1691 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1692 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1693 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1694 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1695 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1696 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1697 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1698 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1699 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1700 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1701 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1702 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1703 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  1704 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1705 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1706 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1707 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1708 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1709 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1710 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1711 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1712 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1713 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1714 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  1715 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1716 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  1717 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1718 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1719 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1720 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1721 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1722 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1723 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1724 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1725 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1726 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1727 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1728 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1729 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1730 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1731 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1732 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1733 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1734 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1735 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1736 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1737 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1738 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1739 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1740 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1741 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1742 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1743 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1744 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1745 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1746 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1747 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1748 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1749 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1750 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1751 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1752 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1753 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1754 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1755 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1756 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1757 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1758 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1759 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1760 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1761 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1762 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1763 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1764 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1765 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1766 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1767 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1768 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1769 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1770 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1771 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1772 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1773 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1774 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1775 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1776 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1777 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1778 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1779 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1780 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1781 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1782 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1783 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1784 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1785 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1786 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1787 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1788 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1789 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1790 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1791 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1792 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1793 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1794 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1795 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1796 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1797 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1798 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1799 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1800 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1801 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1802 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1803 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1804 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1805 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1806 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1807 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1808 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1809 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1810 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1811 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1812 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1813 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1814 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1815 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1816 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1817 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1818 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  1819 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1820 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1821 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1822 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1823 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1824 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1825 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1826 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1827 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1828 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1829 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1830 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1831 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1832 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1833 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1834 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1835 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1836 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1837 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1838 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1839 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1840 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1841 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1842 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1843 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1844 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1845 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1846 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1847 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1848 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1849 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1850 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1851 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1852 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1853 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1854 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1855 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1856 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1857 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1858 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1859 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1860 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1861 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1862 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1863 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1864 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1865 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1866 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1867 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1868 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1869 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1870 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1871 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1872 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1873 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1874 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1875 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1876 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1877 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1878 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1879 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1880 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1881 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1882 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1883 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1884 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1885 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1886 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1887 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1888 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1889 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1890 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1891 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1892 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1893 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1894 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1895 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1896 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1897 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1898 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1899 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1900 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1901 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1902 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1903 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1904 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1905 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1906 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1907 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1908 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1909 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1910 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1911 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1912 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1913 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1914 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1915 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  1916 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1917 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1918 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1919 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1920 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1921 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1922 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1923 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1924 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1925 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1926 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1927 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1928 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1929 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1930 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1931 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  1932 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1933 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1934 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1935 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1936 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1937 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1938 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1939 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1940 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1941 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1942 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1943 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1944 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1945 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1946 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1947 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1948 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1949 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1950 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1951 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1952 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1953 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1954 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1955 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1956 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1957 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1958 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1959 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1960 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1961 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1962 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1963 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1964 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1965 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1966 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1967 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1968 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293077 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  1969 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1970 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1971 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1972 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1973 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1974 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1975 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1976 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  1977 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1978 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1979 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1980 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1981 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1982 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1983 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1984 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1985 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1986 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1987 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1988 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  1989 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1990 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  1991 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  1992 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  1993 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1994 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1995 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1996 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  1997 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  1998 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  1999 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2000 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2001 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2002 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2003 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2004 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2005 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2006 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2007 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2008 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2009 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2010 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2011 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2012 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2013 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2014 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2015 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2016 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2017 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2018 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2019 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2020 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2021 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2022 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2023 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2024 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2025 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2026 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2027 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2028 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2029 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2030 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2031 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2032 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2033 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2034 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2035 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2036 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2037 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2038 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2039 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2040 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2041 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2042 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2043 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2044 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2045 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2046 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2047 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2048 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2049 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2050 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2051 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2052 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2053 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2054 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2055 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2056 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2057 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2058 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2059 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2060 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2061 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2062 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2063 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2064 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2065 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2066 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2067 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2068 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2069 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2070 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2071 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2072 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2073 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2074 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2075 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2076 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2077 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2078 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2079 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2080 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2081 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2082 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2083 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2084 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2085 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2086 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2087 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2088 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2089 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2090 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2091 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2092 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2093 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2094 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2095 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2096 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2097 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2098 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2099 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2100 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2101 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2102 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2103 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2104 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2105 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2106 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2107 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2108 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2109 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2110 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2111 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2112 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  2113 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2114 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2115 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2116 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2117 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2118 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2119 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2120 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2121 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2122 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2123 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2124 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2125 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2126 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2127 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2128 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2129 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2130 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2131 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2132 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2133 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2134 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  2135 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2136 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2137 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2138 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2139 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2140 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2141 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2142 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2143 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2144 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2145 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2146 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2147 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2148 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2149 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2150 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2151 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2152 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2153 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2154 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2155 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2156 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2157 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2158 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2159 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2160 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2161 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2162 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2163 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2164 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2165 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2166 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2167 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2168 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2169 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2170 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2171 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2172 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2173 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2174 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2175 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2176 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2177 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2178 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2179 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2180 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2181 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2182 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2183 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2184 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2185 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2186 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2187 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2188 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2189 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2190 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2191 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2192 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2193 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2194 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2195 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2196 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2197 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2198 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2199 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2200 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2201 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2202 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2203 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2204 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2205 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2206 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2207 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2208 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2209 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2210 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2211 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2212 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2213 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2214 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2215 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2216 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2217 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2218 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2219 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2220 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2221 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2222 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2223 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2224 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2225 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2226 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2227 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2228 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2229 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2230 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2231 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2232 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2233 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2234 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2235 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2236 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2237 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2238 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2239 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2240 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2241 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2242 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2243 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2244 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2245 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2246 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2247 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2248 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2249 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2250 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2251 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2252 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2253 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2254 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2255 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  2256 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2257 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2258 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2259 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2260 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2261 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2262 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2263 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2264 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2265 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2266 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2267 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2268 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2269 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2270 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2271 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2272 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2273 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2274 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2275 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2276 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2277 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2278 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2279 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2280 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2281 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2282 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2283 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2284 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2285 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2286 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2287 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2288 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2289 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2290 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2291 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2292 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2293 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2294 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2295 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2296 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2297 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2298 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2299 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2300 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2301 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2302 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2303 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2304 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2305 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2306 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2307 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2308 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2309 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2310 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2311 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2312 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2313 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2314 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2315 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2316 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2317 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2318 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  2319 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2320 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2321 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2322 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2323 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  2324 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  2325 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2326 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2327 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2328 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2329 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2330 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2331 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2332 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2333 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2334 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2335 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2336 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2337 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2338 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2339 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2340 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2341 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2342 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2343 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2344 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2345 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2346 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2347 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2348 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2349 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2350 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2351 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2352 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2353 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2354 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2355 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2356 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2357 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2358 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2359 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2360 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2361 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2362 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2363 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2364 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2365 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2366 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2367 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2368 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2369 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2370 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2371 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2372 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2373 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2374 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2375 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2376 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2377 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2378 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2379 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2380 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2381 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2382 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2383 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2384 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2385 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2386 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2387 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2388 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2389 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2390 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2391 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2392 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2393 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2394 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2395 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2396 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2397 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2398 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2399 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2400 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2401 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2402 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2403 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2404 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2405 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2406 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2407 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2408 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2409 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2410 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2411 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2412 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2413 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2414 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2415 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2416 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2417 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2418 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2419 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2420 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2421 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2422 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2423 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2424 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2425 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2426 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2427 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2428 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2429 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2430 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2431 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2432 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2433 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2434 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2435 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2436 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2437 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2438 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2439 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2440 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2441 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2442 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2443 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2444 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2445 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2446 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2447 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2448 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2449 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2450 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2451 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2452 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2453 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2454 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2455 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2456 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2457 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2458 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2459 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2460 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2461 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2462 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2463 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2464 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2465 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2466 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2467 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2468 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2469 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2470 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2471 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2472 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2473 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2474 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2475 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2476 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2477 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2478 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2479 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2480 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2481 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2482 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2483 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2484 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2485 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2486 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2487 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2488 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2489 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2490 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2491 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2492 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2493 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2494 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2495 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2496 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2497 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2498 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2499 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2500 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2501 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2502 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2503 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2504 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2505 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2506 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2507 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2508 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2509 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2510 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2511 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2512 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2513 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2514 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2515 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2516 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2517 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2518 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2519 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2520 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2521 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2522 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2523 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2524 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2525 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2526 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2527 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2528 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2529 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2530 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2531 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2532 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2533 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2534 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2535 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2536 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2537 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2538 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2539 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2540 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2541 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2542 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2543 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2544 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2545 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2546 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2547 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2548 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2549 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2550 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2551 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2552 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2553 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2554 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2555 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2556 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2557 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2558 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2559 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2560 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2561 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2562 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2563 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2564 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2565 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2566 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2567 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2568 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2569 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2570 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2571 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2572 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2573 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2574 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2575 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2576 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2577 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2578 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2579 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2580 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2581 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2582 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2583 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2584 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2585 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2586 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2587 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2588 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2589 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2590 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2591 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2592 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2593 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2594 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2595 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2596 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2597 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2598 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2599 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2600 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2601 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2602 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2603 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2604 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2605 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2606 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2607 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2608 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2609 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2610 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2611 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2612 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2613 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2614 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2615 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2616 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2617 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2618 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2619 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2620 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2621 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2622 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2623 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2624 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2625 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2626 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2627 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2628 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2629 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2630 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2631 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2632 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2633 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2634 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2635 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2636 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2637 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2638 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2639 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2640 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2641 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2642 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2643 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2644 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2645 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2646 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2647 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2648 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2649 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2650 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2651 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2652 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2653 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2654 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2655 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2656 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2657 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2658 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2659 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2660 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2661 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2662 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2663 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2664 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2665 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2666 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2667 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2668 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2669 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2670 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2671 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2672 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2673 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2674 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2675 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2676 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2677 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  2678 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2679 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2680 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2681 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2682 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2683 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2684 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2685 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2686 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2687 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2688 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2689 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2690 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2691 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2692 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2693 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2694 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  2695 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2696 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2697 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2698 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2699 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2700 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2701 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2702 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2703 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2704 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2705 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2706 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2707 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2708 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2709 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2710 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2711 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2712 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2713 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2714 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2715 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2716 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2717 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2718 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  2719 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2720 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2721 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2722 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2723 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2724 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2725 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  2726 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2727 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2728 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2729 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2730 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2731 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2732 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2733 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2734 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2735 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2736 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2737 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2738 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2739 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2740 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2741 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2742 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2743 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2744 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2745 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2746 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2747 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2748 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2749 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2750 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2751 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2752 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2753 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2754 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2755 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2756 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2757 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2758 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2759 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2760 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2761 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2762 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2763 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2764 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2765 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2766 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2767 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2768 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2769 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2770 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2771 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2772 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2773 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2774 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2775 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2776 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2777 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2778 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2779 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2780 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2781 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2782 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2783 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2784 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2785 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2786 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2787 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2788 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2789 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2790 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2791 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2792 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2793 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2794 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2795 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2796 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2797 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2798 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2799 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2800 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2801 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2802 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2803 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2804 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2805 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2806 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2807 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2808 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2809 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2810 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2811 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2812 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2813 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2814 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2815 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2816 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2817 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2818 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2819 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2820 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2821 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2822 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2823 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2824 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2825 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2826 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2827 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2828 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2829 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2830 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2831 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2832 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2833 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2834 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2835 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2836 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2837 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2838 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2839 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2840 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2841 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2842 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2843 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  2844 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2845 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2846 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2847 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  2848 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2849 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2850 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2851 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2852 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2853 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2854 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2855 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2856 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2857 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2858 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2859 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2860 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2861 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2862 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2863 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2864 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2865 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2866 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2867 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2868 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2869 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2870 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2871 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2872 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2873 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2874 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2875 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2876 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2877 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2878 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2879 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2880 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2881 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2882 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2883 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2884 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2885 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2886 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2887 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2888 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2889 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2890 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2891 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2892 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2893 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2894 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2895 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2896 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2897 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2898 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2899 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2900 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2901 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2902 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2903 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2904 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2905 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2906 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2907 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2908 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2909 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2910 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2911 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2912 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2913 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2914 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2915 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2916 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2917 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2918 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2919 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2920 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2921 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2922 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2923 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2924 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2925 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2926 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2927 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2928 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2929 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2930 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2931 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2932 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2933 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2934 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2935 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2936 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2937 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2938 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2939 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2940 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2941 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  2942 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2943 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2944 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2945 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2946 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2947 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2948 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2949 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2950 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2951 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2952 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2953 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2954 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2955 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2956 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.00422925383325232\n",
      "Epoch =  2957 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2958 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2959 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2960 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2961 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  2962 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2963 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2964 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2965 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2966 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  2967 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  2968 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2969 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2970 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  2971 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2972 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2973 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2974 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2975 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2976 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2977 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2978 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2979 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2980 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2981 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2982 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  2983 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2984 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2985 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2986 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2987 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  2988 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2989 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2990 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  2991 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2992 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2993 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2994 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  2995 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  2996 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2997 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  2998 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  2999 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3000 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3001 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3002 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3003 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3004 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3005 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3006 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3007 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3008 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3009 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3010 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3011 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3012 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3013 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3014 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3015 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3016 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3017 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3018 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3019 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3020 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3021 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3022 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3023 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3024 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3025 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3026 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3027 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3028 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3029 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3030 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3031 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3032 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3033 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3034 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3035 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3036 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3037 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3038 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3039 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3040 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3041 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3042 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3043 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3044 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3045 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3046 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3047 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3048 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3049 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3050 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3051 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3052 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3053 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3054 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3055 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3056 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3057 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3058 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3059 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3060 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3061 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3062 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3063 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3064 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3065 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3066 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3067 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3068 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3069 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3070 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3071 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3072 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3073 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3074 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3075 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3076 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3077 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3078 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3079 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3080 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3081 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3082 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3083 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3084 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3085 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3086 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3087 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3088 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3089 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3090 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3091 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3092 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3093 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3094 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3095 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3096 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3097 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3098 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3099 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3100 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3101 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3102 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3103 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3104 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3105 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3106 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3107 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3108 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3109 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3110 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3111 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3112 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3113 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3114 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3115 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3116 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3117 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3118 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3119 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3120 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3121 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3122 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3123 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3124 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3125 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3126 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3127 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3128 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3129 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3130 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3131 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3132 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3133 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3134 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3135 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3136 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3137 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3138 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3139 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3140 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3141 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3142 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3143 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3144 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3145 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3146 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3147 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3148 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3149 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3150 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3151 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3152 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3153 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3154 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3155 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3156 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3157 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3158 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3159 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3160 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3161 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3162 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3163 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3164 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3165 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3166 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3167 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3168 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3169 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3170 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3171 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3172 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3173 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3174 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3175 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  3176 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3177 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3178 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3179 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3180 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3181 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3182 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3183 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3184 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3185 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3186 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3187 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3188 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3189 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3190 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3191 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3192 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3193 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3194 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3195 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3196 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3197 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3198 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3199 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3200 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3201 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3202 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3203 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3204 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3205 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3206 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3207 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3208 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3209 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3210 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3211 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3212 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3213 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3214 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3215 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3216 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3217 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3218 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3219 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3220 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3221 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3222 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3223 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3224 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3225 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3226 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3227 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3228 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3229 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3230 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3231 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3232 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3233 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3234 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3235 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3236 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3237 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3238 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3239 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3240 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3241 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3242 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3243 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3244 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3245 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3246 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3247 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3248 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3249 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3250 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3251 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3252 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3253 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3254 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3255 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3256 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3257 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3258 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3259 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3260 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3261 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3262 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3263 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3264 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3265 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3266 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3267 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3268 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3269 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3270 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3271 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3272 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  3273 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3274 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3275 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3276 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3277 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3278 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3279 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3280 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3281 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3282 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3283 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3284 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3285 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3286 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3287 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3288 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3289 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3290 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3291 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3292 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3293 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3294 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3295 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3296 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3297 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3298 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3299 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3300 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3301 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3302 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3303 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3304 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3305 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3306 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3307 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3308 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3309 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3310 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3311 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3312 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3313 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3314 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3315 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3316 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3317 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3318 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3319 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3320 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3321 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3322 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3323 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3324 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3325 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3326 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3327 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3328 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3329 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3330 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3331 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3332 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3333 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3334 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3335 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3336 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3337 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3338 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3339 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3340 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3341 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3342 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3343 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3344 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3345 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3346 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3347 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3348 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3349 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3350 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3351 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3352 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3353 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3354 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3355 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3356 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3357 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3358 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3359 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3360 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3361 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3362 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3363 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3364 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3365 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3366 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3367 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3368 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3369 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3370 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3371 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3372 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3373 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3374 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3375 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3376 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3377 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3378 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3379 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3380 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3381 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3382 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3383 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3384 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3385 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3386 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3387 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3388 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3389 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3390 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3391 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3392 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3393 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3394 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3395 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3396 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3397 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3398 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3399 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3400 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3401 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3402 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3403 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3404 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3405 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3406 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3407 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3408 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3409 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3410 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3411 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3412 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3413 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3414 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3415 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3416 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3417 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3418 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3419 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3420 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3421 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3422 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3423 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3424 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3425 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3426 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3427 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3428 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3429 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3430 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3431 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3432 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3433 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3434 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3435 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3436 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3437 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3438 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3439 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3440 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3441 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3442 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252322\n",
      "Epoch =  3443 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3444 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3445 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3446 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3447 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3448 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3449 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3450 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3451 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3452 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3453 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3454 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3455 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3456 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3457 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3458 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3459 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3460 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3461 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3462 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3463 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3464 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3465 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3466 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3467 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3468 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3469 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3470 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3471 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3472 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3473 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3474 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3475 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3476 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3477 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3478 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3479 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3480 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3481 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3482 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3483 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3484 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3485 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3486 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3487 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3488 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3489 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3490 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3491 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3492 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3493 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3494 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3495 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3496 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3497 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3498 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3499 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3500 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3501 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3502 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3503 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3504 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3505 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3506 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3507 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3508 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3509 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3510 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3511 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3512 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3513 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3514 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3515 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3516 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3517 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3518 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3519 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3520 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3521 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3522 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3523 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3524 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3525 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3526 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3527 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3528 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3529 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3530 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3531 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3532 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3533 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3534 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3535 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3536 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3537 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3538 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3539 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3540 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3541 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3542 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3543 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3544 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3545 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3546 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3547 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3548 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3549 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3550 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3551 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3552 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3553 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3554 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3555 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3556 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3557 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3558 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3559 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3560 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3561 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3562 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3563 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3564 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3565 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3566 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3567 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3568 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3569 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3570 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3571 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3572 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3573 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3574 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3575 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3576 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3577 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3578 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3579 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3580 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3581 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3582 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3583 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3584 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3585 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3586 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3587 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3588 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3589 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3590 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3591 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3592 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3593 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3594 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3595 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3596 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3597 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3598 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3599 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3600 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3601 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3602 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3603 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3604 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3605 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3606 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3607 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3608 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3609 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3610 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3611 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3612 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3613 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3614 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3615 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3616 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3617 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3618 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3619 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3620 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3621 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3622 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3623 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3624 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3625 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3626 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3627 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3628 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3629 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3630 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3631 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3632 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3633 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3634 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3635 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3636 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3637 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3638 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3639 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3640 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3641 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3642 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3643 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3644 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3645 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3646 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3647 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3648 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3649 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3650 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3651 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3652 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3653 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3654 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3655 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3656 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3657 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3658 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3659 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3660 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3661 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3662 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3663 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3664 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3665 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3666 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3667 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3668 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3669 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3670 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3671 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3672 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3673 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3674 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3675 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3676 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3677 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3678 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3679 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3680 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3681 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3682 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3683 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3684 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3685 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3686 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3687 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3688 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3689 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3690 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3691 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3692 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3693 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3694 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3695 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3696 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3697 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3698 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3699 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3700 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3701 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3702 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3703 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3704 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3705 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3706 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3707 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3708 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3709 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3710 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3711 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3712 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3713 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3714 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3715 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3716 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3717 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3718 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3719 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3720 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3721 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3722 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3723 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3724 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3725 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3726 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3727 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3728 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3729 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3730 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3731 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3732 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3733 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3734 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3735 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3736 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3737 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3738 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3739 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3740 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3741 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3742 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3743 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3744 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3745 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3746 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3747 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3748 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3749 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3750 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3751 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3752 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3753 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3754 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3755 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3756 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3757 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3758 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3759 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3760 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3761 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3762 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3763 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3764 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3765 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3766 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3767 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3768 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3769 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3770 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3771 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3772 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3773 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3774 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3775 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3776 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3777 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3778 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3779 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3780 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3781 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3782 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3783 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3784 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3785 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3786 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3787 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3788 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252332\n",
      "Epoch =  3789 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3790 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3791 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3792 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252333\n",
      "Epoch =  3793 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3794 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3795 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3796 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3797 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3798 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3799 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3800 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3801 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3802 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252333\n",
      "Epoch =  3803 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3804 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3805 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  3806 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3807 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3808 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3809 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252332\n",
      "Epoch =  3810 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3811 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3812 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3813 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3814 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3815 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3816 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3817 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3818 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3819 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3820 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3821 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3822 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3823 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3824 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3825 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3826 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3827 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3828 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3829 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3830 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3831 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3832 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3833 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3834 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3835 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3836 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3837 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3838 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3839 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3840 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3841 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  3842 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3843 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3844 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3845 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3846 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3847 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3848 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3849 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3850 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3851 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3852 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3853 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3854 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3855 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3856 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3857 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3858 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3859 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3860 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3861 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3862 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3863 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3864 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3865 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3866 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3867 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3868 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3869 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3870 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3871 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3872 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3873 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3874 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3875 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3876 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3877 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3878 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3879 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3880 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3881 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3882 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3883 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3884 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3885 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3886 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3887 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3888 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3889 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3890 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3891 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3892 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3893 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3894 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3895 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3896 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3897 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3898 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3899 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3900 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3901 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3902 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3903 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3904 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3905 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3906 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3907 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3908 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3909 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252332\n",
      "Epoch =  3910 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3911 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3912 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3913 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3914 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3915 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3916 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3917 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3918 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3919 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3920 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3921 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3922 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3923 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3924 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3925 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3926 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  3927 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3928 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3929 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3930 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3931 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3932 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3933 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3934 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3935 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3936 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3937 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3938 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3939 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3940 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3941 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3942 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3943 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3944 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3945 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3946 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3947 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3948 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3949 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3950 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3951 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3952 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3953 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3954 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3955 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3956 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3957 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3958 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3959 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3960 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3961 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3962 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3963 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3964 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3965 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3966 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3967 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3968 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3969 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3970 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3971 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3972 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3973 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3974 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3975 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3976 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3977 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3978 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3979 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3980 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3981 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3982 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3983 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3984 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3985 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3986 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3987 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  3988 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3989 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3990 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  3991 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3992 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  3993 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  3994 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  3995 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3996 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  3997 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  3998 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  3999 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4000 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4001 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4002 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4003 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4004 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4005 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4006 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4007 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4008 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4009 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4010 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4011 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4012 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4013 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4014 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4015 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4016 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4017 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4018 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4019 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4020 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4021 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4022 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4023 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4024 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4025 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4026 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4027 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4028 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4029 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4030 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4031 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4032 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4033 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4034 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4035 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4036 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4037 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4038 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4039 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4040 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4041 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4042 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4043 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4044 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4045 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4046 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4047 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4048 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4049 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4050 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4051 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4052 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4053 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4054 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4055 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4056 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4057 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4058 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4059 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4060 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4061 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4062 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4063 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4064 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4065 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4066 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4067 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4068 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4069 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4070 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4071 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4072 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4073 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4074 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4075 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4076 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4077 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4078 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4079 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4080 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4081 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4082 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4083 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4084 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4085 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4086 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4087 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4088 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4089 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4090 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4091 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4092 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4093 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4094 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4095 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4096 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4097 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4098 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4099 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4100 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4101 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4102 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4103 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4104 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4105 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4106 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4107 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4108 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4109 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4110 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4111 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4112 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4113 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4114 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4115 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4116 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4117 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4118 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4119 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4120 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4121 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4122 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4123 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4124 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4125 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4126 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4127 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4128 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4129 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4130 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4131 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4132 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4133 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4134 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4135 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293177 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4136 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329318 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  4137 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4138 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4139 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4140 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4141 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4142 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4143 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4144 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4145 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4146 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4147 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4148 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4149 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4150 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4151 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4152 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4153 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4154 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4155 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4156 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4157 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4158 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4159 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4160 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4161 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4162 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4163 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4164 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4165 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4166 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4167 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4168 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4169 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4170 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4171 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4172 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329318 learning rate ratio =  0.004229253833252332\n",
      "Epoch =  4173 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4174 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4175 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4176 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  4177 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4178 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4179 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4180 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  4181 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4182 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4183 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4184 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4185 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4186 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4187 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4188 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4189 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4190 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4191 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4192 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4193 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4194 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4195 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4196 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4197 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4198 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4199 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4200 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4201 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4202 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4203 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4204 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4205 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4206 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4207 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4208 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4209 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4210 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4211 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4212 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4213 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4214 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4215 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4216 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4217 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4218 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4219 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4220 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4221 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4222 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4223 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4224 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4225 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4226 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4227 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4228 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4229 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4230 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4231 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4232 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4233 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4234 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4235 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4236 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4237 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4238 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4239 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4240 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4241 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4242 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4243 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4244 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4245 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4246 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4247 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4248 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4249 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4250 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4251 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4252 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4253 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4254 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4255 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4256 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4257 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4258 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4259 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4260 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4261 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4262 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4263 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4264 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4265 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4266 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4267 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4268 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4269 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4270 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4271 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4272 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4273 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4274 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4275 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4276 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4277 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4278 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4279 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4280 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4281 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4282 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4283 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4284 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4285 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4286 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4287 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4288 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4289 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4290 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4291 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4292 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4293 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4294 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4295 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4296 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4297 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4298 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4299 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4300 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4301 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4302 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4303 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4304 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4305 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4306 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4307 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4308 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4309 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4310 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4311 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4312 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4313 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4314 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4315 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4316 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4317 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4318 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4319 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4320 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4321 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4322 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4323 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4324 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4325 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4326 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4327 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4328 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4329 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4330 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4331 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4332 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4333 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4334 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4335 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4336 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4337 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4338 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4339 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4340 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4341 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4342 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4343 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4344 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4345 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4346 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4347 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4348 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4349 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4350 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4351 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4352 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4353 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4354 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4355 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4356 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4357 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4358 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4359 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4360 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4361 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4362 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4363 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4364 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4365 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4366 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4367 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4368 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4369 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4370 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4371 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4372 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4373 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4374 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4375 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4376 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  4377 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  4378 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  4379 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4380 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4381 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252321\n",
      "Epoch =  4382 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4383 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4384 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4385 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4386 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4387 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4388 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4389 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4390 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4391 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4392 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4393 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4394 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4395 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4396 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4397 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4398 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4399 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4400 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4401 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4402 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4403 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4404 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4405 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4406 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4407 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4408 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4409 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4410 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4411 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4412 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4413 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4414 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4415 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4416 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4417 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4418 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4419 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4420 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4421 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4422 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4423 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4424 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4425 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4426 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4427 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4428 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4429 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4430 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4431 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4432 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4433 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4434 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4435 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4436 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4437 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4438 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4439 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4440 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4441 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4442 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4443 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4444 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329318 learning rate ratio =  0.004229253833252333\n",
      "Epoch =  4445 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4446 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4447 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4448 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4449 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4450 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4451 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4452 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4453 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4454 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4455 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329308 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4456 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4457 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4458 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4459 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4460 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4461 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4462 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4463 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4464 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4465 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4466 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4467 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4468 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4469 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4470 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4471 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4472 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4473 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4474 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4475 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4476 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4477 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4478 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4479 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4480 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4481 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4482 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4483 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4484 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4485 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4486 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4487 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4488 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329309 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4489 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4490 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4491 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4492 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4493 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4494 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4495 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4496 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4497 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4498 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4499 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4500 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4501 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4502 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4503 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4504 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4505 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4506 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4507 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4508 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4509 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4510 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4511 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4512 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4513 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4514 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4515 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4516 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4517 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4518 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4519 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4520 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4521 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4522 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4523 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4524 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4525 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4526 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4527 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4528 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4529 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4530 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4531 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4532 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4533 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4534 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4535 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4536 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4537 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4538 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4539 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4540 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4541 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4542 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4543 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4544 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4545 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4546 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4547 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4548 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4549 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4550 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4551 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4552 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4553 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4554 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4555 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4556 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4557 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4558 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4559 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4560 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4561 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4562 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4563 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4564 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4565 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4566 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4567 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4568 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4569 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4570 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4571 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4572 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4573 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4574 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4575 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4576 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4577 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4578 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4579 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4580 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4581 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4582 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4583 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4584 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4585 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4586 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4587 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4588 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4589 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4590 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4591 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4592 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4593 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4594 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4595 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4596 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4597 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4598 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4599 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4600 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4601 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4602 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4603 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4604 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4605 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4606 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4607 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4608 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4609 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4610 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4611 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4612 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4613 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4614 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4615 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4616 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4617 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4618 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4619 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4620 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4621 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4622 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4623 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4624 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4625 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4626 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4627 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4628 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4629 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4630 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4631 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4632 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4633 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4634 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4635 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4636 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4637 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4638 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4639 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4640 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4641 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4642 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4643 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4644 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4645 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4646 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4647 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4648 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4649 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4650 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4651 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4652 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4653 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4654 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4655 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4656 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4657 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4658 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4659 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4660 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4661 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4662 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4663 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4664 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4665 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4666 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4667 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4668 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4669 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4670 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4671 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4672 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4673 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4674 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4675 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4676 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4677 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4678 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4679 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4680 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4681 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4682 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4683 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4684 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4685 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4686 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4687 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4688 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4689 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4690 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329318 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  4691 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4692 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4693 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4694 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4695 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4696 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4697 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4698 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4699 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4700 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4701 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4702 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4703 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4704 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4705 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4706 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4707 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4708 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4709 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4710 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4711 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4712 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4713 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4714 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4715 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4716 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4717 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4718 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4719 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4720 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4721 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4722 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4723 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4724 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4725 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4726 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4727 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4728 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4729 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4730 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4731 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4732 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4733 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4734 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4735 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4736 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4737 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4738 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4739 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4740 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4741 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4742 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293093 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4743 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4744 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4745 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4746 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252331\n",
      "Epoch =  4747 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4748 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4749 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4750 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4751 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4752 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4753 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4754 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4755 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4756 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4757 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4758 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4759 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4760 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4761 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4762 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4763 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4764 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4765 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4766 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4767 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4768 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4769 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4770 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4771 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4772 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4773 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4774 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4775 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4776 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4777 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4778 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4779 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4780 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4781 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4782 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4783 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4784 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4785 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4786 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4787 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4788 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4789 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4790 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4791 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4792 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4793 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4794 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4795 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4796 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4797 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4798 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4799 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4800 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4801 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4802 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4803 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4804 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4805 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4806 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4807 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4808 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4809 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4810 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4811 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4812 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4813 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4814 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4815 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4816 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4817 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4818 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4819 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4820 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4821 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4822 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4823 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4824 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4825 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4826 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4827 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4828 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4829 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4830 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4831 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4832 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4833 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4834 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4835 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4836 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4837 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4838 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4839 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4840 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4841 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4842 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4843 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4844 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4845 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4846 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4847 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4848 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4849 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4850 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4851 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4852 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4853 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4854 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4855 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4856 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4857 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4858 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4859 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4860 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4861 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4862 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4863 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4864 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4865 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4866 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4867 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4868 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4869 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4870 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4871 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4872 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4873 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4874 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4875 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4876 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4877 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4878 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4879 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4880 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4881 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4882 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4883 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4884 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4885 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4886 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4887 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4888 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4889 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4890 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4891 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4892 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4893 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4894 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4895 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4896 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4897 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4898 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4899 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4900 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4901 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4902 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4903 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4904 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4905 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4906 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4907 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4908 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4909 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4910 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4911 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4912 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4913 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4914 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4915 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4916 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4917 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4918 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4919 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4920 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4921 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4922 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4923 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4924 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4925 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4926 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4927 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4928 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4929 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4930 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4931 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4932 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4933 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293154 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4934 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4935 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4936 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4937 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4938 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4939 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4940 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4941 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4942 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4943 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4944 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4945 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4946 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4947 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4948 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4949 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4950 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4951 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4952 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4953 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4954 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4955 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4956 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4957 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.481443861332931 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4958 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4959 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4960 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252329\n",
      "Epoch =  4961 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4962 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4963 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4964 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4965 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4966 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293166 learning rate ratio =  0.0042292538332523305\n",
      "Epoch =  4967 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4968 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293143 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4969 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4970 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4971 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4972 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4973 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4974 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4975 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4976 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4977 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329311 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4978 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4979 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4980 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4981 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4982 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329317 learning rate ratio =  0.00422925383325233\n",
      "Epoch =  4983 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4984 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4985 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4986 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329316 learning rate ratio =  0.004229253833252328\n",
      "Epoch =  4987 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4988 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329312 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4989 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.004229253833252326\n",
      "Epoch =  4990 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4991 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252323\n",
      "Epoch =  4992 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4993 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293116 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4994 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329314 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4995 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252325\n",
      "Epoch =  4996 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329315 learning rate ratio =  0.004229253833252327\n",
      "Epoch =  4997 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293127 learning rate ratio =  0.004229253833252324\n",
      "Epoch =  4998 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.4814438613329313 learning rate ratio =  0.0042292538332523244\n",
      "Epoch =  4999 Batch =  0 Loss =  [8.34795266] Gradient_max =  0.48144386133293104 learning rate ratio =  0.004229253833252321\n"
     ]
    }
   ],
   "source": [
    "SGD_momentum = Solver(simplelogistic, x_train, y_train, lr = 1.7e-2, batch_size = 15, num_epochs = 5000, print_every = 1000)\n",
    "SGD_momentum.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(0,SGD_momentum.num_epochs)\n",
    "plt.plot(epochs, SGD_momentum.loss_history, label = 'Loss', color = 'red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, SGD_momentum.vel_history, label = 'Velocity', color = 'blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7380766223612197"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def training_accuracy(model, x_train,y_train):\n",
    "    y_pred = model.predict(x_train)\n",
    "    y_new = y_train.reshape(y_pred.shape)\n",
    "    return np.sum(y_new == y_pred) / y_new.shape[0]\n",
    "\n",
    "\n",
    "training_accuracy(simplelogistic, x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_accuracy(model, x_test, y_test, mu, sigma):\n",
    "    y_pred = model.predict((x_test - mu) / sigma)\n",
    "    y_new = y_test.reshape(y_pred.shape)\n",
    "    return np.sum(y_new == y_pred) / y_new.shape[0]\n",
    "\n",
    "x_test_new = x_test\n",
    "x_test_new = x_test_new \n",
    "test_accuracy(simplelogistic, x_test_new, y_test, mu, sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5316653635652854\n"
     ]
    }
   ],
   "source": [
    "print ((np.sum(y_train == 1))/y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplelogistic.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8201e9342a4a492ddbd4e81efec90b2ccf0d205cda2cc39ac893f0c43374b5e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
